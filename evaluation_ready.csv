article_name,md_path,image_path_1,image_context_1,item_id,with_context,without_context
2507.08637v1-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/arxiv_articles/processed_articles/2507.08637v1-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/arxiv_articles/processed_articles/2507.08637v1-with-image-refs_artifacts/image_000000_23c6d3101ca79aafafc9409dd2c6dda83fb6cdd6d7ec3e3324772485f94f9e49.png,"## 3.1 WERSA Algorithm

Let queries Q ∈ R n × d , keys K ∈ R n × d , and values V ∈ R n × d , the WERSA mechanism is as follows:

1. Linear Projection: Computes projected queries, keys, and values:

2. Multi-Head Splitting: Divides Q K ′ , ′ , and V ′ into h heads and reshape them to R n × × h d h , where d h = d /h k .
3. Wavelet Decomposition: Performs the wavelet transform (for simple implementation, it has been used the Haar wavelet transform see Appendix A) on Q ′ and K ′ for L levels:

where W generates a sequence of wavelet coefficients at various scales. It is important to state that the method is generic and thus Haar can be substituted with other types of wavelet decomposition such as Daubechies, symlets, etc.

4. Adaptive Wavelet Filtering: One of WERSA's innovations includes adaptive wavelet filtering. The average query representation for a specific head h can be computed as:

A neural network represented by g then maps Q ′ avg ,h to filter coefficients:

where σ denotes the sigmoid function,yielding coefficients in the [0 , 1] range. The coefficients are then applied element-wise to each respective wavelet coefficient acting like a filter. Additionally, each wavelet scale is modulated by a learnable parameter:

where ω i controls the importance of the i -th wavelet scale. Prior wavelet-based transformers [Zhuang et al., 2022] process all wavelet scales equally, which is suboptimal for sequences with heterogeneous frequency components. WERSA's gating mechanism ω i learns to suppress noisy high-frequency scales or enhance low-frequency global patterns, depending on the input.

5. Filtered Wavelet Representation: Multiplies the wavelet coefficients by the corresponding filter coefficients:

6. Unified Multi-Scale Reconstruction: WERSA efficiently reconstructs filtered signals through a single operation:

where W -1 represents the inverse wavelet transform of the filtered coefficients.

7. Random Feature Projection: To mitigate computational complexity, approximations with random features are utilized to approximate the softmax kernel. This is done by using a map ϕ x ( ) :

and specifically, a ReLU-based random feature map is used:

with R randomly drawn from R d h × m . The projection is enhanced with a trainable bandwidth parameter β :

where R can be initialized as orthogonal or Gaussian matrices. This yields the linear attention approximation:

8. Attention Computation: Compute attention with the filtered signals:

where ϵ is a small constant aiming at ensuring numerical stability.

9. Multi-Head Combination: Concatenates the outputs of all heads and project them:

A pictorial representation of the operations flow is depicted in Figure 1.

In order to increase transparency and reproducibility the WERSA pseudocode can be found in Appendix D while the Hugging Face Transformers compatible implementation will be released upon acceptance.

Figure 1: Wavelet Enhanced Random Feature Self-Attention (WERSA) architecture. From left to right: Input Processing (gray/blue), Wavelet Processing (orange), and Attention Mechanism (green).",2507.08637v1-with-image-refs,"The chart divides the model’s workflow into three color-coded stages: initial data splitting, wavelet-based filtering, and an efficient attention module. In the filtering stage, each data stream is broken down into broad and fine-grained details, then a learned gating system removes noisy fine details while emphasizing important broad patterns. These cleaned signals are reconstructed and passed through a fast projection step that approximates traditional attention calculations at a fraction of the computational cost. By adaptively selecting which detail levels to keep, the system concentrates on the most relevant sequence features, improving its ability to handle long-range dependencies and ignore background noise. Overall, the diagram demonstrates how combining multi-scale filtering with efficient approximation techniques yields a powerful, resource-friendly attention mechanism.","The diagram shows how input data is first split into different frequency channels, adaptively filtered to highlight important features, and then reassembled before entering a streamlined attention mechanism. By using wavelet transforms to decompose and selectively emphasize information at multiple scales, the model can capture both fine details and broader patterns more effectively. It then replaces the traditional dot-product attention with a random-feature method that cuts the computational cost from growing rapidly with input size to growing more slowly. This combination preserves the ability to model complex relationships while dramatically speeding up processing, making it well suited for large-scale or real-time applications."
2507.08665v1-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/arxiv_articles/processed_articles/2507.08665v1-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/arxiv_articles/processed_articles/2507.08665v1-with-image-refs_artifacts/image_000001_468b541f690f83514b5020ab075d945df8c5dffd9287913fc2bb0b8612b156a0.png,"## 2.2. Autoformalization

Autoformalization constitutes a specialized machine translation task that transforms natural language statements into formal representations while preserving semantic content and complying with target syntax requirements. Initial investigations explored neural approaches like (Wang et al., 2018; Cunningham et al., 2023), demonstrating the feasibility of this paradigm.

Current research on LLM-based autoformalization primarily follows two dominant approaches: (1) few-shot incontext learning (Wu et al., 2022; Patel et al., 2023; Zhou et al., 2024), and (2) fine-tuning LLMs on NL-FL pairs (Lu et al., 2024a;b; Gao et al., 2025). While the latter has shown promising results with 96 % (pass@128) accuracy in MiniF2F (Zheng et al., 2021), performance drops to just 16 %(pass@128) on the College CoT benchmark, revealing the critical limitation of NL-FL data scarcity.

Aparallel research direction addresses the more challenging task of formal proof generation, where natural language proofs often diverge substantially from their formal counter-

Figure 2. Overview of Our Method. (a) Data Collection . We gather problems from various sources, including online resources and exercise sets, and construct an ontology library of relevant concepts and theorems. Through filtering and data synthesis strategies, we obtain a natural language (NL) corpus. (b) Semantic Parsing . We employ the KELPS model to perform semantic parsing, translating natural language problems into knowledge equations. The initial iteration of data is obtained through annotation. (c) Syntax Validation . The knowledge equations generated in (b) are validated by the AL Parser. Problems that pass validation are then converted into other formal languages via rule-based transformation. (d) Semantic Validation . Data that passes the compiler validation in the previous stage undergoes semantic review by both LLMs and human experts. Finally, the verified data is incorporated into the dataset, which is then used to continuously train the baseline model.

parts. Current approaches include: (1) proof decomposition into draft skeletons with subsequent completion (Jiang et al., 2022; Wang et al., 2023), and (2) direct neural translation of informal proofs (Wang et al., 2024; Shao et al., 2024). The first approach uses language models to complete proof steps within a structured framework, whereas the second aims for complete automated translation.

The alternative paradigm (Wu et al., 2024; Gao et al., 2025) initiates from formal language corpora, employing LLMs for backward translation to natural language. While valuable, this approach remains fundamentally constrained by the scope and completeness of existing formal libraries.",2507.08665v1-with-image-refs,"The diagram presents a four-step pipeline for converting informal math problems into trustworthy formal examples by combining data creation, direct translation, automated checking, and expert validation. It begins by gathering and expanding a language corpus with relevant concepts, then uses a trained model to turn each problem into a precise “knowledge equation.” Next, automated syntax checks weed out formatting errors before deeper semantic reviews by AI tools and human experts catch logical inconsistencies. Only entries passing all checks are added to the dataset, feeding back into training to steadily improve translation accuracy. This multi-layered process overcomes the lack of clean data and ensures the final models learn from rigorously vetted examples.","The chart outlines a four‐stage workflow that transforms raw text into a rigorously validated formal dataset. First, data is gathered from natural‐language corpora, ontologies, and web resources, then filtered and synthesized to create a clean input set. Next, a fine‐tuned semantic model translates each sentence into precise declarations and facts, which are passed through automated syntax parsers and formal compilers to catch formatting or logical errors. Finally, human experts and advanced language models perform a last semantic review, ensuring only accurate, well‐formed knowledge enters the final repository. This layered process of automated checks and expert validation offers a practical blueprint for building high‐integrity knowledge bases."
2507.08716v1-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/arxiv_articles/processed_articles/2507.08716v1-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/arxiv_articles/processed_articles/2507.08716v1-with-image-refs_artifacts/image_000003_4296db732d14ec712519c017680de3b4de17b6abd86a53cb8036b137372cf9cb.png,"## II. THE GREAT-X: ALL-IN-UE MULTIMODAL SIMULATOR

The constructed simulation platform reconstructs the electromagnetic process of the ray tracing channel simulation in the Unreal Engine, thereby enabling the implementation of the ray tracing algorithm based on ray tracing in high-fidelity scenarios. The following mainly introduces the reflection, refraction, and the scattering parts. The following formulas are referenced from SionnaRT [13].

During the propagation of electromagnetic waves, when a plane wave is incident on the planar interface between two different media, due to the difference in medium properties, part of the wave will be reflected at the interface, while the other part will pass through the interface and enter the second medium; that is, transmission or refraction occurs. For analysis, this paper assumes that both media are uniform and non-magnetic dielectrics and adopts the parameter definitions in ITU recommendations [14]. The complex amplitude vector of the incident wave's electric field, E i , can be represented by two arbitrary orthogonal polarization components:

Fig 3 illustrates the reflection and refraction phenomena that occur when electromagnetic waves are incident on a planar interface between two materials with relative dielectric constants of η 1 and η 2 . For ease of analysis, the coordinate system is chosen such that the wave vectors of the incident wave, reflected wave, and transmitted wave are all located in the same incident plane, which is defined as the x z -plane. At the same time, the normal vector ˆ n of the interface is oriented along the negative z axis. Due to the different polarization characteristics, the incident wave needs to be expanded in a set of specific bases, specifically expressed as two mutually orthogonal polarization components E i, ⊥ and E i, ∥ , that is:

Among them, the component perpendicular to the incident plane is called transverse electric (TE) polarization (on the left side of the figure). In contrast, the component parallel to the incident plane is called transverse magnetic (TM) polarization (on the right side of the figure). In the subsequent discussion, it is uniformly agreed that all the transverse vector components point outward from the plane of the figure. Based on the above definition, it can be directly verified that the following relationships must be satisfied:

The incident electric field phasor in terms of its TE and TM components can be expressed as:

The phasors of the reflected wave and the transmitted wave E r and E t are also expressed in a similar manner:

The corresponding unit vectors for the reflected and transmitted waves are defined as follows:

Fig. 3: Reflection and refraction of a plane wave at a plane interface between two materials [13].

Combining all the information, we obtain the following relationship among the incident wave, the reflected wave, and the transmitted wave:

In the phenomenon of scattering, when an electromagnetic wave is incident on a surface, some of the energy is reflected. At the same time, the other part penetrates the surface and enters the medium interior, forming a refracted wave. We usually classify reflection into two types: specular reflection and diffuse reflection. The former has been analyzed in the discussion of reflection and refraction, while this section will focus on the latter, namely diffuse reflection. Unlike specular reflection, when an electromagnetic wave is incident on a surface with diffuse reflection properties, the energy does not concentrate and reflect in a single direction, but is scattered in multiple directions outward. Considering that most actual surfaces will simultaneously produce specular reflection and diffuse reflection, we use S 2 to represent the energy of the diffuse reflection part, where S ∈ [0 , 1] is the scattering coefficient; correspondingly, R 2 represents the energy of the specular reflection part, where R ∈ [0 , 1] is the specular reflection attenuation factor. The relationship between the two is as follows:

Consider the scenario depicted in Fig 4, where at the scattering point q , there exists a plane incident wave with local linear polarization, and its electric field vector is E q i ( ) . Our focus is on the scattered field generated by this incident wave on the scattering direction ˆ k s , through an infinitesimal surface element dA . It should be noted that the surface normal vector ˆ n is in any direction in the global coordinate system, and the ( x, y, z ) coordinate axes indicated by the green dotted lines in the figure are used to assist in the explanation. The incident field vector can be expressed as two orthogonal polarization components perpendicular to the incident wave direction ˆ k i :",2507.08716v1-with-image-refs,"The chart illustrates how a plane electromagnetic wave splits into reflected and refracted components when it crosses a boundary between materials with different dielectric properties, with bending angles set by their contrast. It contrasts two polarization cases—one where the electric field vibrates perpendicular to the incident plane and one where it vibrates parallel—showing that field orientation alters the balance and directions of reflected and transmitted beams. By mapping the electric and magnetic field vectors before and after the interface, the visualization makes it clear that these polarization-dependent effects are essential for accurate ray tracing. Feeding this behavior into the simulator ensures that losses and path deviations at material interfaces are realistically captured, which is critical for designing reliable antennas, optical coatings, and wireless channels. Incorporating these boundary-interaction rules into virtual prototypes cuts down on expensive physical tests by predicting real-world signal behavior more faithfully.","The chart illustrates how an electromagnetic wave striking a boundary between two media splits into a reflected beam and a transmitted beam with angles set by the change in material. It contrasts two polarization states: in the TE case the electric field oscillates perpendicular to the plane of incidence, while in the TM case the magnetic field does. Because each polarization enforces different matching conditions on the fields at the interface, they yield different reflection and transmission strengths. Notably, the TM arrangement can eliminate reflection entirely at a specific incidence angle (the Brewster angle), a principle behind anti-glare coatings and polarized sunglasses. The inclusion of the two media’s impedances (η₁ and η₂) highlights how material properties tune this balance of reflected versus transmitted energy."
2507.08723v1-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/arxiv_articles/processed_articles/2507.08723v1-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/arxiv_articles/processed_articles/2507.08723v1-with-image-refs_artifacts/image_000008_db51379aed090643ae5c5df8c474e036c1b620583c457f1b98fef5b2cd6229a6.png,"## 2. Water-Cherenkov Detectors

The Water-Cherenkov Detectors form the core of the Surface Detector array and have operated reliably since the beginning of Phase I. Each WCD consists of a cylindrical tank filled with 12 000 l of purified water, instrumented with three downward-facing large photomultiplier tubes (LPMTs) mounted at the top. These PMTs detect Cherenkov light from charged particles and provide signals over a wide dynamic range suitable for most shower geometries.

As already mentioned, as part of the AugerPrime upgrade, a fourth, small PMT was added to extend the dynamic range, and the original electronics were replaced by the Upgraded Unified Board. The SPMT allows accurate measurement of high particle densities near the shower axis, while the UUB provides enhanced digitization, timing, and calibration capabilities [3]. This extends the WCD dynamic range by more than an order of magnitude, from a few hundred VEM (vertical equivalent muon) with the large PMTs to nearly 20,000 VEM with the SPMT.

The stability of the WCD array is shown in Fig. 2, which presents the daily rate of high-quality events per active hexagon for the SD-1500, as an example. The indicated energy threshold is chosen to ensure full trigger efficiency. Blue triangles indicate Phase I data, and red circles correspond to Phase II data, while the open gray circles show the transition period between the two. Rates remain

Event rate above 3 EeV 1500m array

Figure 2: Daily rate of high-quality events per active hexagon for the SD-1500. The energy threshold ensures full trigger efficiency. Blue triangles indicate Phase I data, and red circles correspond to Phase II data, while the open gray circles show the transition period.

Figure 3: Evolution of SPMT-related parameters: number of active SPMTs (black), calibration factor 𝛽 (red), station temperature (blue), and SPMT current (magenta), from 2021 to 2025.

stable over time, demonstrating consistent array performance during and after the deployment.

The SPMT, a 1-inch Hamamatsu R8619, cannot be directly calibrated with atmospheric muons due to its small photocathode area. A cross-calibration procedure is used instead, converting the integrated ADC signal to VEM via a linear relation, 𝑆 VEM = 𝛽 𝑄 ADC counts, with a conversion factor 𝛽 determined to better than 2 5 % [6]. .

The long-term evolution of SPMT-related parameters can be seen in Fig. 3. The number of active SPMTs (black) increases until mid-2023 as deployment progresses. The calibration factor 𝛽 (red) shows a seasonal modulation of 8-10 %, consistent with the temperature dependence of SPMT gain. Temperature (blue), taken from sensors on the LPMTs, and SPMT current (magenta) reflect environmental trends and their effect on the hardware. In particular, the current stabilizes in mid-2023 following the activation of automatic high-voltage regulation.

The UUB digitizes waveforms from all detectors and provides unified acquisition across the upgraded Surface Detector [7]. It also maintains backward compatibility with Phase I triggers to support hybrid operation. Monitoring of voltage rails, temperature, and acquisition rates enables early issue detection and long-term stability tracking.

Figure 4: Daily averages of SSD MIP charge (red), PMT high voltage (blue), anode current (magenta), and the number of operational SSDs (black), grouped in 50-day intervals.",2507.08723v1-with-image-refs,"The chart tracks four performance metrics—signal charge per particle, PMT supply voltage, detector current, and number of active units—across an 18-month period, showing a rapid deployment phase that levels off by mid-2023. Once at full scale, signal readings stay within a 5% band, voltage holds to better than 1% of its set-point, and current fluctuates only a few microamps, illustrating the effectiveness of automatic high-voltage regulation and cross-calibration. Small, regular oscillations in both signal charge and current line up with seasonal temperature shifts, confirming an environmental influence on detector gain. Introducing finer thermal compensation or adaptive gain control would help tighten these residual variations and boost long-term stability.","After a rapid build-out that raised the active detector count from about 500 to over 1,350 by early 2023, the array has held steady at full capacity ever since. Over the ensuing two years, the average single-particle signal has drifted upward by roughly 5%, and the PMT current has shown a similar small increase, suggesting mild ageing or gain shift. Meanwhile, the high-voltage supply has remained rock-solid within ±1 V of its target, confirming excellent power stability. To keep measurement precision high, instituting a biannual gain calibration and routine current-leakage inspection is recommended to counteract the observed slow drifts."
2507.08731v1-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/arxiv_articles/processed_articles/2507.08731v1-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/arxiv_articles/processed_articles/2507.08731v1-with-image-refs_artifacts/image_000006_6ff42391dda2dae179fb8fdebcce05e776a8320e9103ed5189152c3cc9d3bb50.png,"## 4.1. Spectral measurements

In this section, we present the results of our measurements. The values for the pEW and FWHM covering the first 40 days after the explosion are presented in Figure 6. The colour bars on the right side indicate the SN phase. The analysis across the full-time interval (0 - 40 days) does not reveal a distinct separation into two clusters; instead, it shows a continuum where SNe IIb tend to dominate at higher values.

We examined the spectral features across smaller time intervals to investigate whether the observed continuum is intrinsic or a consequence of the broad time range considered (0 - 40 days). The results for the earliest two intervals (0 - 10 and 10 - 20 days) are presented in Figure 7. Within these intervals, the pEW and FWHM parameters reveal clear trends: SNe IIb consistently exhibit stronger H 𝛼 and He I features throughout the entire early time range. Notably, an intermediate region is observed around 25 Å in the pEW of both H 𝛼 and He I, as well as around 125 Å for H 𝛼 and 200 Å for He I in the FWHM. This overlap between the two populations further supports the continuum observed in the full-time interval (0 - 40 days). However, we note that the overlap between the groups becomes more significant as time passes. Specifically, the later the analysed time range, the greater the overlap, as the SN II values grow stronger as time passes (see Figures A.3 and A.4 in the Appendix A). This highlights the important role that the epoch of the SN will have in our analysis and the development of our method. A summary of the pEW and FWHMvalues for each SN type at each time interval is presented in Appendix B (see Table B.4).

We also find that, although SNe 87A-like are spectroscopically similar at all phases to SNe II, they exhibit notably higher values, especially in the H 𝛼 absorption feature (Figures 6 and 7). Despite this, we excluded them from further analyses due to their low numbers.

To evaluate the statistical significance of our results, we applied the Kolmogorov-Smirnov test (KS test). This nonparametric method computes the maximum difference between the Cumulative Distribution Function (CDF) of two data sets under the null hypothesis that both groups are drawn from populations with identical distributions. The KS test results for the comparison of the pEW and FWHM of H 𝛼 and He I between SNe II and IIb are summarised in Table 2. The Table includes the KS statistic (greatest difference between the two CDFs) and the corresponding p-value (significance of the difference obtained) for each time interval. A p-value &lt; 0 05 indicates the rejection . of the null hypothesis, suggesting a statistically significant difference between the two groups.

From Table 2, we observe that most distributions have pvalues significantly below the 0.05 threshold, indicating a mean-

Fig. 6. Left panel: pEW of the H 𝛼 absorption profile versus the pEW of the He I 𝜆 5876 line measured across the full-time interval (0 - 40 days). The markers are established by SN type: SNe IIb are shown as blue triangles, SNe II as red circles, and SNe 87A-like as green squares. The colour bars on the right side indicate the SN phase. Right panel: Same as the left panel, but for the FWHM measurements.

Table 2. The KS-statistic run for our measurements. Separately for the pEW and FWHM results for the lines of H 𝛼 and He I.

|            |       | pEW          | pEW       | pEW          | pEW       | FWHM         | FWHM      | FWHM         | FWHM      |
|------------|-------|--------------|-----------|--------------|-----------|--------------|-----------|--------------|-----------|
| Time range | # Sp. | He I         | He I      | H 𝛼          | H 𝛼       | He I         | He I      | H 𝛼          | H 𝛼       |
|            |       | KS-statistic | p-value   | KS-statistic | p-value   | KS-statistic | p-value   | KS-statistic | p-value   |
| 0-10d      | 220   | 0.349        | 0.0002    | 0.281        | 0.005     | 0.303        | 0.002     | 0.205        | 0.085     |
| 10-20d     | 278   | 0.482        | 2.012e-08 | 0.439        | 5.374e-07 | 0.386        | 1.609e-05 | 0.445        | 3.048e-07 |
| 20-30d     | 233   | 0.348        | 1.494e-04 | 0.264        | 8.490e-03 | 0.340        | 2.366e-04 | 0.275        | 5.434e-03 |
| 30-40d     | 135   | 0.205        | 0.227     | 0.141        | 0.667     | 0.195        | 0.273     | 0.259        | 0.063     |
| 0-40d      | 866   | 0.732        | 7.914e-51 | 0.675        | 1.530e-42 | 0.635        | 3.131e-37 | 0.624        | 7.170e-36 |

The table shows the two values returned by the KS-test: the KS-statistic and the p-value. Values that are not statistically significant are shown in italics.

ingful separation between the SN II and IIb. The two samples exhibit statistically significant differences in the earliest 30 days post-explosion, with the most clear separation occurring within a 10 to 20-day interval. These findings support our hypothesis that SNe II and IIb display differences in their early spectral evolution, reinforcing the possibility of identifying them based on pEW and FWHM values.

The obtained KS test values suggest that while early-time observations can effectively differentiate between the two types based on pEW values, this distinction becomes less clear as SNe evolve. Interestingly, the greatest difference between SN II and IIb is observed during the 10 - 20-day interval rather than the earlier range (0 - 10 days). This is likely because both types are characterised by a blue featureless spectrum without prominent features soon after the explosion. Even as H lines emerge, they may still be weak, lacking sufficient signatures to differentiate between SNe II and IIb. Consistent with this, we find no significant difference in the H 𝛼 FWHMdistribution during the first 10 days post-explosion.

From30dayspost-explosion, classifying SNe II and IIb based on the analysed spectral features becomes increasingly challenging due to the significant overlap between the two populations. For the 30 - 40-day interval, the p-values exceed the significance threshold across all analysed distributions, indicating that the differences between the pEW and FWHM of the two SN types decrease as their spectra evolve. Although the reduced number of spectra may influence the results, the physical convergence of spectral features also contributes. Around 30 days after explosion, the H 𝛼 line in SNe II typically strengthens, reaching intensities comparable to those observed in SNe IIb. At the same time, the He I line is often blended with the emerging Na I feature, particularly for SNe II (Gutiérrez et al. 2017). Nevertheless, the distinction between the two types remains clear at this stage when considering additional spectral features beyond H 𝛼 and He I. Indeed, by ∼ 30 days post-explosion, the H 𝛼 P-Cygni profile in SNe IIb begins to exhibit distinct characteristics of this subtype. Notably, the He I 𝜆 6678 feature, located near the peak of H 𝛼 becomes more prominent, indicating an interference in the emission of the H feature. Additionally, He I lines emerge at

Fig. 7. pEW and FWHM values of H 𝛼 line and He I 𝜆 5876 for 0 - 10 days and 10 - 20 days. The markers are established by SN type: SNe IIb are shown as blue triangles, SNe II as red circles, and SNe 87A-like as green squares. The colour bars on the right side indicate the SN phase. The pEW measurements are in the top panels, and the FWHM measurements are in the bottom panels. In the left panels, we show the measurements within 0 - 10 days and in the right panels, between 10 - 20 days.

this phase, providing robust criteria for distinguishing between SNe II and IIb.",2507.08731v1-with-image-refs,"The chart shows that the strength and width of the hydrogen and helium lines form a continuous spread rather than two separate clusters, with SNe IIb generally displaying stronger and broader features and SNe II showing weaker, narrower lines. A band of overlap around moderate values highlights how these two types become harder to distinguish outside a specific time window. Indeed, the clearest separation appears about 10 to 20 days after the explosion, while measurements taken very early or later tend to blend together. A few rare 87A-like events stand out with exceptionally strong hydrogen absorption, underscoring their unique behaviour despite their low numbers. Overall, this implies that timing is crucial: using these line measurements to classify supernovae is most reliable in that early sweet spot before their spectra converge.","The chart shows that Type IIb supernovae exhibit much stronger helium absorption and broader spectral lines than Type II explosions during the first 40 days after the blast, reflecting their helium-rich outer layers and faster-expanding gas. Regular Type II events remain clustered at low helium strength and narrower lines, consistent with their hydrogen-dominated envelopes. The small sample of 87A-like explosions falls in between, showing moderate helium and hydrogen line properties. The color gradient from red (earliest) to green (around 40 days) reveals that both helium and hydrogen features steadily grow in strength and width over the first month before leveling off. This clear separation in helium behavior offers a straightforward spectroscopic way to distinguish supernova types early on."
2507.08745v1-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/arxiv_articles/processed_articles/2507.08745v1-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/arxiv_articles/processed_articles/2507.08745v1-with-image-refs_artifacts/image_000003_ae9a6f20ef308ce874db6661c9a78a626e7dd78d52c1fe78f4c240bde76dbea7.png,"## 4.4 Results on Real-World Data

Data sets. We used four real-world data sets in our experiments. Some statistics about them are presented in Table 1. The Mammals dataset contains information about which mammal species inhabit which areas of the world on one side, and climate information on the other side [12]. The Dialect data contains features of spoken dialects of Finnish over different geographical regions [6, 7]. The 20Newsgroups data 4 is a corpus of 1000 posts from 20 different newsgroups, stemmed and with rare terms removed. The Abstracts data 5 is another corpus, this time of project abstracts. Terms are stemmed and rare ones are removed.

4 https://archive.ics.uci.edu/dataset/113/twenty+newsgroups 5 https://archive.ics.uci.edu/dataset/134/nsf+research+award+abstracts+1990+

2003

Table 1: Properties of real-world data sets

| Data         | Rows   | Columns   | Density ( % )   | Note           |
|--------------|--------|-----------|-----------------|----------------|
| Mammals      | 54 013 | 4 802     | -               | Numerical data |
| Dialect      | 1 334  | 506       | 16 . 14         | Numerical data |
| 20Newsgroups | 5 163  | 19 997    | 0 . 89          | Numerical data |
| Abstracts    | 4 894  | 12 841    | 0 . 90          | Numerical data |

Figure 5: Top row: Relative reconstruction error on the y -axis and number of tiles on the x -axis. (a) Tiles are redescriptions from the Mammals data. (b) Tiles are frequent itemsets from the Dialect data. (c) Tiles are rank-1 Boolean matrices for BMF from the Dialect data. Bottom row: Time (in seconds) on y -axis and number of tiles on x axis. (d)-(f) as (a)-(c).

Redescription mining. We used redescriptions mined using the fast redescription mining algorithm Fier [14]. These results contained 161 redescriptions, some of them very similar to each other. The results are shown in Fig. 5(a) and (d). The results show that HaPSi is able to find very fast a good set of approximately 50 redescriptions. It should be noted that this data can be fully covered using all 161 tiles, so all methods converge towards the end. Notice also that Greedy is very slow.

Tiling databases. We mine frequent itemsets from the Dialect dataset with minimum frequency of 15 % . This gave us 28 535 frequent itemsets. We used all of these as tiles, and searched for a set of maximum 1000 tiles. The results are in Fig. 5(b) and (e). We see that HaPSi is again very good up to about 50 itemsets, after which it has converged. It is again significantly faster than Greedy and significantly better than Naïve .

Boolean matrix factorization. We created the rank-1 Boolean matrices using the association matrix technique used by the Asso algorithm and the restarted random walks technique [19]. These rank-1 matrices can cover 0 s in the data, unlike in the previous cases. The results using the Dialect data are shown in Fig. 5(c) and (f). The results are similar to the other real-world use cases, except that in this case the Naïve algorithm increases its error consistently. But again HaPSi is competitive with Greedy , especially when we consider the significant advance it has on running time.

We also ran experiments with tiles found only by the association matrix technique using the two large corpus matrices. This created approximately 5000 tiles for 20Newsgroups and 12 000 for Abstracts . HaPSi took 850 s for Abstracts , while Greedy took 101 h . For 20Newsgroups HaPSi took 665 s and Greedy 74 h . This shows that HaPSi can find results efficiently with input sizes where Greedy is unable to produce the results in a reasonable amount of time.",2507.08745v1-with-image-refs,"The chart compares how three algorithms—Naïve, HaPSi, and Greedy—trade off reconstruction error and computation time as more tiles are added across three real-world pattern-mining scenarios. In every case, HaPSi drives the error down almost as quickly as Greedy, reaching near-optimal accuracy by around fifty tiles, while the Naïve approach lags behind. Beyond this point, Greedy only marginally improves in error but its running time explodes, whereas HaPSi’s running time grows gently and remains practical. In the Boolean factorization setting the Naïve method’s error even increases with more tiles, but HaPSi stays consistently low. Overall, HaPSi offers the best balance of speed and accuracy, scaling to large datasets where Greedy becomes impractically slow.","The chart contrasts three solution methods—naïve (yellow), HaPSi (blue), and greedy (red)—across six performance curves, revealing that the naïve approach almost always lags behind or even worsens over time, while HaPSi and greedy both drive rapid, sustained improvements. In the top row (plots a–c), which track a loss‐like metric, greedy and HaPSi sharply reduce the value in the first few dozen iterations and then slowly refine toward a low plateau, whereas naïve falls much more gradually or actually creeps upward. In the bottom row (plots d–f), which measure reward‐type gains, greedy consistently accumulates the highest total, HaPSi achieves moderate but steady growth, and naïve offers little to no increase in most scenarios. The persistent spacing among the three curves underscores that the extra computation in HaPSi and greedy yields clear dividends over the simplistic baseline. Overall, greedy typically reaches the best final performance, while HaPSi strikes a balance between speed of convergence and smoothness of improvement."
2507.08747v1-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/arxiv_articles/processed_articles/2507.08747v1-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/arxiv_articles/processed_articles/2507.08747v1-with-image-refs_artifacts/image_000008_96611d5e31f451e5446bc6eda0e6cef728a387d83ea47a0f31ceca02be89bb6f.png,"## 3.3 Thunderstorms

The largest source of detector instabilities is lightning activity. Both in Phase I and Phase II, a spike in the T3 rate and a subsequent drop in the T3 purity are observed during and in the wake of thunderstorms, as shown in Figure 4. During these periods, the SD stations exhibit an elevated rate of T2 triggers. More importantly, T2 triggers from neighboring stations are highly correlated

Figure 4: a A large number of 3-fold T3s is recorded during periods with thunderstorms, as indicated by the arrows. b The corresponding 3-fold T3 purity rate measures a drop coincident with the weather period.

Figure 5: An erroneously triggered WCD trace (black) displays electronic noise in the form of a 10 MHz oscillation around the baseline (orange) between two sub-threshold muon pulses. The blue trace is the output of a trace-cleaning algorithm using the black trace as input. The effect of the electronic noise is mitigated.

in time, as they are likely caused by the same electric discharge. Consequently, the CDAS identifies many more T3 events than during nominal operation. Since the communication between the CDAS and SD stations is bandwidth-limited, only one to two T3 requests can be issued to the SD array every second. This represents a bottleneck that results in the buildup of a large T3 queue during thunderstorms. The CDAS naively iterates through the T3 queue and requests data from a station often hours after the event happened. 3 Such stale events block the acquisition of new, real air shower events and drop the T3 purity and SD uptime considerably. The estimated effect on the duty cycle of the UB array is ∼ 2 %. The UUB, with its higher resolution, is more susceptible to lightning-induced triggering. The drop in the SD duty cycle for Phase II reads 5%. The algorithm presented in Section 5 partially fixes this issue.",2507.08747v1-with-image-refs,"The chart illustrates the number of basic detector triggers recorded over several days, with two pronounced peaks that coincide with periods of thunderstorm activity. Outside these storm windows, the trigger rate stays low and steady, but during lightning events it soars by more than an order of magnitude, overwhelming the normal data flow. These surges are caused by electrical noise from lightning, which makes neighboring stations fire in unison as if they saw real air showers. The resulting backlog of spurious events clogs the communication pipeline, delaying genuine cosmic-ray data requests and dropping the purity of collected events. Introducing real-time noise filtering or flagging during storms would help prioritize valid signals and maintain high detector uptime.","The chart tracks event‐level trigger counts over several days and shows a steady low‐level background punctuated by two dramatic spikes peaking at about 4,000 and 7,000 triggers. Each of these surge events aligns with periods of thunderstorm activity, indicating that storms drive a sudden and large increase in detected events. Outside of these bursts, trigger rates remain stable and modest, underlining how distinct the storm‐related enhancements are. Recognizing and filtering out these weather‐induced spikes can improve the reliability of background measurements and help isolate genuine signals of interest."
2507.08748v1-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/arxiv_articles/processed_articles/2507.08748v1-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/arxiv_articles/processed_articles/2507.08748v1-with-image-refs_artifacts/image_000006_94bcae196ac044a7e56f3bee2a6dfa0d40721f308fe3476fe7e23a1e5f809a4f.png,"## 4. Data quality monitoring

IceCube data-taking proceeds in periods called runs which have a typical duration of 8 hours. The condition of the detector during one run and resulting data quality are verified manually afterwards, with help of an array of monitored quantities and knowledge of past problems. This allows to define the set of runs usable for a selfconsistent archival data set. For FRA however, a heuristic is required that can assure data quality with a reasonable degree of confidence with lower latency. This has already been implemented to accommodate the GFU event selection [4]. This heuristic's starting point are the rates of intermediate event selection stages, measured in time intervals of typically 600 seconds. With this rate as 𝑋 and the exponentially weighted averages ⟨ 𝑋 ⟩ and ⟨ 𝑋 2 ⟩ , a Z-score is calculated for the deviation of 𝑋 -⟨ 𝑋 ⟩ relative to the standard deviation.

Summing this Z-score for several rates as well as the ratio between them results in the final instability score, which can be compared to a threshold. We present here

Figure 4: ROC curves comparing possible thresholds to be set on the three instability scores. The x-axis is a ""false positive"" rejection rate during good runs, while the y-axis is the ""true positive"" rejection during bad runs. The threshold of 10 is indicated by filled circles on the respective curves.

instability scores that are analogously derived for a cascade selection like DNN Cascades, and a DeepCore-focused selection like GRECO or ELOWEN. These might eventually prove to take complementary roles to the original, ensuring the quality of the specific event selections. √

In the new definitions, the Z-scores are also weighted proportionally to Δ 𝑡 in accordance with the expected variance. This suppresses the statistical fluctuations present for the GFU instability score during shorter-than-typical bins. Figure 4 shows that both new scores remain sensitive to

(a) Sky map of randomized events within a 1000second interval including one DNN Cascades event.

(b) Corresponding test statistic map in a point source analysis combining the three event selections.

Figure 5

conditions flagged by the run monitoring, and a common threshold or multi-variate cut can be optimized for a particular analysis.",2507.08748v1-with-image-refs,"The chart compares three different data quality scores used to monitor the IceCube detector by plotting the trade-off between mistakenly rejecting healthy runs and correctly flagging problematic ones. At a threshold of 10 on each score, fewer than 1% of good runs are excluded while around 50% of the bad runs are caught. The muon-based score shows a slightly higher rejection of bad runs at low false-alarm rates, whereas the DeepCore score is modestly less sensitive. The close alignment of these curves means a single threshold can effectively govern multiple event selections, enabling automated, low-latency flagging of detector issues.","The chart compares three methods for filtering data by showing how many valid observations they mistakenly discard versus how many problematic ones they successfully catch. When set to only reject about 0.1% of good data, the cascade and muon approaches still flag just over half of the bad observations, while the deepcore approach captures a bit less. If we allow up to a 10% false-reject rate, all three rise to around 90% detection of bad data, and they converge toward 100% detection only when false rejects also approach 100%. This highlights that modest increases in the allowed discard rate can dramatically improve problem detection. For cases where preserving valid data is paramount, the cascade or muon methods offer the most effective balance."
2507.08753v1-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/arxiv_articles/processed_articles/2507.08753v1-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/arxiv_articles/processed_articles/2507.08753v1-with-image-refs_artifacts/image_000007_a3135b8346d49dc006d95f67eacb9f391e913a730dc410800cf6ebb06f62f43c.png,"## 3. Method

This is a template-based analysis where templates based on four different galactic emission models are being tested. The Fermi 𝜋 0 template [9], the KRA 5 𝛾 and KRA 50 𝛾 templates [10] and the CRINGE template [11]. The results of this analysis will include the measurement of the model normalization for each of the four model hypotheses, the rejection of the null hypothesis under each model assumption, and the rejection of the global null hypothesis by the most significant model hypothesis after trial correction. We use an unbinned maximum likelihood method that utilizes the reconstructed direction, energy, and angular uncertainty of each neutrino candidate in the ICEMAN dataset.

The signal considered is the excess of neutrinos along the galactic plane over the only declination-dependent background fluxes of atmospheric muons, atmospheric neutrinos, and diffuse

Figure 2: Per-flavor all-sky effective area in the ICEMAN sample.

astrophysical neutrinos.

To parameterize this excess prediction based on each model, templates of their respective prediction are folded with the effective area of each dataset. After normalizing, this creates a spatial probability density function (PDF) for each dataset and template combination. To account for the respective angular uncertainty of each event, each spatial PDF is smeared with a set of 2D Gaussian kernels corresponding to different angular uncertainties. They are then evaluated based on the uncertainty of the respective event. Figure 4 shows an example of the unsmeared PDF as well as an example of the smeared PDF for each sub-dataset.

To build the energy PDF, the MC is weighted with a single power law with a spectral index of -2.7 is used for the Fermi 𝜋 0 template. For the other three templates, the sky-averaged spectrum predicted by each template is used. The weighted MC is then binned in reconstructed energy. This way an energy PDF in reconstructed energy based on the predicted energy spectra is obtained. All nominal all-sky fluxes are shown in Figure 5.

To parameterize the declination-dependent backgrounds, a PDF of the density of neutrinos per declination is used. Since this background PDF is derived from data that contains partial contamination from the signal, a signal subtraction method is applied. The observed PDF from data ¯ , 𝐷 is characterized as the sum of the true isotropic background 𝐵 , and the average density of the signal per declination band ¯ 𝑆 𝛿 ( 𝑖 ) :

Using this, the likelihood of observing 𝑛 𝑠 neutrinos is defined as shown in equation 2:

Figure 3: Sketch showing which dataset overlapping events are kept in. The event numbers exclusive to the datasets and in all overlapping zones are shown. The colors of the shaded regions show which dataset the overlap will stay in.

Then, the test statistic (TS) is defined as the likelihood ratio of fitting 𝑛 𝑠 neutrinos over zero neutrinos,

Using this method, we can derive the sensitivity and discovery potential of this analysis using pseudo-experiments. Here, a number of Monte-Carlo events corresponding to an injected flux are

Table 1: Sensitivity and discovery potentials for different models. For Fermi 𝜋 0 , the per-flavor flux at 100 TeV is reported. For the other three models, sensitivity and discovery potential are reported in units of model flux.

| Quantity            |   Fermi 𝜋 0 [10 - 12 TeV cm - 2 s - 1 |   KRA 5 𝛾 |   KRA 50 𝛾 |   CRINGE |
|---------------------|---------------------------------------|-----------|------------|----------|
| Sensitivity         |                                  4.68 |      0.13 |        0.1 |     0.13 |
| Discovery Potential |                                 18.7  |      0.49 |        0.4 |     0.53 |

sampled from the spatial and energy distribution predicted by a chosen model. Data, with the right ascension coordinate randomized, is added to these signal events to account for the isotropic backgrounds. Table 1 shows the necessary flux for 50% of pseudo experiments to exceed the TS corresponding to a 5 𝜎 discovery for the four different templates. Since the flux measured in [3] is above the flux necessary to reach the 5 𝜎 threshold for the Fermi 𝜋 0 model, a 5 𝜎 pre-trial significance is a likely outcome of this analysis. In a set of pseudo-experiments, injecting the best fit flux of the previous measurement, we obtain a median local significance of 5.5 𝜎 for the Fermi 𝜋 0 model, as shown in Table 2.

Figure 4: Spatial signal PDF for the Fermi 𝜋 0 template. Shown in a) acceptance weighted with ESTES without smearing. In b) acceptance weighted with ESTES and smeared with a Gaussian kernel of 0.3 degrees width. In c) the template is acceptance weighted with DNN Cascades and smeared with a Gaussian kernel of 7 degrees width. In d) the template is acceptance weighted with the Northern Tracks and smeared with a Gaussian of 0.3 degree width. These smearing widths are chosen based on the median angular uncertainty of the signal events in the respective datasets.

Engergy [Gev]

Figure 5: Nominal per-flavor flux prediction for all four templates used in this analysis.",2507.08753v1-with-image-refs,"The chart compares how four different galactic emission models predict the per-flavor neutrino flux across a wide energy range. At a few TeV up to tens of TeV, all models cluster around similar flux levels, but above ~100 TeV they diverge sharply: the Fermi π⁰ curve drops off steeply, the CRINGE and KRA5 templates sit in the middle, and the KRA50 scenario maintains a pronounced high-energy tail extending toward the PeV scale. This spread means that measurements in the highest-energy bins will be most sensitive to telling these models apart. Focusing observational efforts above 100 TeV will therefore be key to confirming or ruling out the presence of extra high-energy neutrinos predicted by the more optimistic templates.","The chart compares how four theoretical approaches predict the intensity of neutrinos across a wide energy range. The simplest scenario (red line) remains far below the others, implying that relying on traditional mechanisms would miss most of the potential signal. In contrast, the more detailed models (green, blue, and orange curves) forecast substantially stronger fluxes, with the orange prediction staying highest at the very top of the spectrum. This suggests that detectors optimized for energies around 100 TeV to 1 PeV have the best chance to catch these elusive particles. Focusing experimental sensitivity where these models diverge will provide the most decisive test of competing theories."
2507.08765v1-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/arxiv_articles/processed_articles/2507.08765v1-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/arxiv_articles/processed_articles/2507.08765v1-with-image-refs_artifacts/image_000000_7e924c9d86139a0f8f27c9686b10e4ee2f00c2e1d13bdea744f7c4eafdb1336f.png,"## I. INTRODUCTION

I N computer vision, Segment Anything Model (SAM) [1] has emerged as a dominant solution for a wide range of segmentation tasks. Extensive independent evaluations confirm that SAM could achieve high-quality segmentation masks through natural interactions, i.e. , clicks, bounding boxes, or text prompts. SAM's core objective is zero-shot segmentation, enabling segmentation of arbitrary objects without taskspecific training. Building on SAM's significant success, as Table I shows, numerous variants like Med-SAM [2] and SAMHQ [3] have been developed. They employ novel architectures and fine-tuning strategies to enhance efficacy in specific domains or boost segmentation performance further. SAM and its

Fenglei Fan (hitfanfenglei@gmail.com) is the corresponding author.

Juntong Fan and Feng-Lei Fan are with Frontier of Artificial Networks (FAN) Lab, Department of Data Science, City University of Hong Kong, Hong Kong, China SAR.

Zhiwei Hao and Jianqiang Shen are with the Data Storage Product Line, Huawei Technologies Co., Ltd. (haozhiwei@huawei.com; shenjianqiang@huawei.com)

Shang-Ling Jui is with Lagrange Mathematics and Computing Research Center (jui.shangling@huawei.com)

Yi Zhang is with the School of Cyber Science and Engineering, Sichuan University, Chengdu 610207, China, and also with the Key Laboratory of Data Protection and Intelligent Management, Ministry of Education, Sichuan University, Chengdu 610207, China (e-mail: yzhang@scu.edu.cn).

Jing-Xiao Liao is with Department of Industrial and Systems Engineering, The Hong Kong Polytechnic University, Hong Kong, SAR of China.

variants have been widely applied in diverse scenarios such as healthcare and intelligent manufacturing. However, SAM and many variants are notoriously large-scale, hindering efficient deployment for users with limited computational resources. For instance, MobileSAMv2 [4], a compact SAM variant, still comprises 641 million parameters and requires 2.37 GB of storage. Deploying such large models presents significant challenges in resource-constrained settings, such as autonomous vehicles and smartphones. Consequently, effectively compressing SAM and its variants becomes an increasingly pressing practical need.

TABLE I

A SUMMARY OF SEVERAL MAINSTREAM SAM VARIANTS

| Model                  | Pre-training Data   | File Size   | Parameters   | linear params(%)   |
|------------------------|---------------------|-------------|--------------|--------------------|
| SAM-B [1]              | SA-1B               | 357MB       | 94M          | 94.84%             |
| SAM-L [1]              | SA-1B               | 1.16GB      | 312M         | 97.96%             |
| SAM-H [1]              | SA-1B               | 2.38GB      | 641M         | 98.76%             |
| SAM-HQ-Tiny [3]        | SA-1B + HQSeg-47K   | 40.5MB      | 11M          | 86.12%             |
| SAM-HQ-B [3]           | SA-1B + HQSeg-47K   | 361MB       | 95M          | 93.92%             |
| SAM-HQ-L [3]           | SA-1B + HQSeg-47K   | 1.16GB      | 314M         | 97.58%             |
| SAM-HQ-H [3]           | SA-1B + HQSeg-47K   | 2.39GB      | 643M         | 98.53%             |
| SAM2-Tiny [5]          | SA-1B               | 148MB       | 39M          | 97.58%             |
| SAM2-Small [5]         | SA-1B               | 175MB       | 46M          | 97.91%             |
| SAM2-Base [5]          | SA-1B               | 308MB       | 81M          | 98.64%             |
| SAM2-Large [5]         | SA-1B               | 856MB       | 224M         | 99.36%             |
| MobileSAM [4]          | 1%SA-1B             | 38.8MB      | 10M          | 88.56%             |
| MobileSAMv2(ViT-H) [4] | 1%SA-1B             | 2.37GB      | 641M         | 98.76%             |
| EdgeSAM [6]            | 1%SA-1B             | 37.0MB      | 10M          | 41.19%             |
| EdgeSAM-RPN [6]        | 3%SA-1B             | 37.0MB      | 10M          | 41.19%             |
| EfficientSAM-Ti [7]    | SA-1B+IN            | 39.0MB      | 10M          | 90.72%             |
| EfficientSAM-S [7]     | SA-1B+IN            | 100MB       | 26M          | 95.41%             |
| TinySAM [8]            | SA-1B               | 38.8MB      | 10M          | 88.56%             |
| MedSAM [9]             | FLARE               | 358MB       | 94M          | 95.90%             |

We think that an ideal compression solution for SAMs should exhibit four key characteristics: versatility across model types, agility in model deployment, faithfulness to the original model, and compactness in model size. (i) The philosophy of SAMs lies in the universality, compression methods shall not be restricted to specific model architectures either. This universality enhances compression efficiency and facilitates the transfer of diverse SAM variants into massive users. (ii) The growing adoption of 'model factory' paradigms in cloud computing necessitates parsimonious model handling. Compression must therefore be sufficiently rapid to enable efficient batch processing, which can significantly boost the overall compression throughput. (iii) Compressed models shall closely preserve the original SAM's capabilities. Given the substantial costs involved in training foundational SAMs, a large performance loss is unacceptable. Consequently, compression should not require model retraining. While model open-sourcing is common, accessing the data engine, e.g. , as with Med-SAM, is often prohibited, and datasets prepared specifically for compression are typically limited. Retraining risks cumbersome procedures, catastrophic forgetting [10] (undermining SAM's core capabilities), and the potential introduction of unintended biases. (iv) Provided faithfulness is maintained, the compression solution shall achieve a high compression ratio to enable viable deployment in extremely

resource-constrained environments.

Four principal model compression techniques are commonly employed: pruning, quantization, knowledge distillation, and low-rank decomposition. The compression ratios by pruning and quantization methods are hard to scale. Studies have shown that under the constraint of model fidelity, pruning can typically achieve only a compression ratio of 2 to 4 times [11]. The theoretical lower bound for quantization is 1-bit representation, but subject to the severe perturbation to model parameters and the difficulty of fine-tuning. To the best of our knowledge, achieving a reliable and performant lower-than-INT4 quantization has been a challenging task for industry and academia so far. Therefore, low-bit quantization remains impractical for models like SAM. Moreover, lowrank decomposition approximates weight matrices via lowrank representations but faces critical limitations. First, the method necessitates extensive tuning for convergence due to the non-convex optimization landscape. Second, low-rank decomposition struggles with standard convolution kernels which are small, limiting its applicability as a universal model compression technique [12]. As for knowledge distillation, it involves extensive training; therefore, we exclude it from our discussion.

Fig. 1. The basic principle of the Hyper-Compression, where x ∈ R N denotes the target high-dimensional point. Some transformation T and the initial point x 0 establish the existence of a scalar k ∈ N + for an arbitrary x such that x can be approximately represented as T k ( x 0 ) . Thus, we can compress x into k . This figure is reproduced based on Figure 2 in [13]. .

Earlier, our group proposed Hyper-Compression, which is a novel model compression technique that leverages trajectory density to transform the high-dimensional parameter compression into the representation of a low-dimensional dynamic system. As illustrated in Figure 1, there exists a dynamic system such that the one-dimensional trajectory induced by some initial point can fill the high-dimensional space. Here, 'filling the high-dimensional space' means that the trajectory can always arrive in the arbitrarily small neighborhood of any point. Thus, Hyper-Compression establishes a deterministic mapping between the one-dimensional trajectory parameter k (composition times or trajectory length) and the highdimensional target vector x ∈ R N : x ≈ T k ( x 0 ) . This potentially enables the compression of x into the scalar k : x → k , while decompressing x from k follows the compo- sition of transformation: T k ( x 0 ) ≈ x . As Table II shows, Hyper-Compression enjoys a novel mechanism for model compression that is fundamentally different from quantization, pruning, and low-rank decomposition.",2507.08765v1-with-image-refs,"The diagram depicts a high-dimensional parameter space as a simple box and shows how iteratively applying a basic transformation from an initial point can wander through every corner of that space. By counting only the number of steps k it takes to land near a desired target configuration, the full complex vector of model parameters can be encoded in a single integer. Decompression simply repeats the same transformation k times to faithfully reconstruct the original parameter set. This technique bypasses heavyweight compression schemes by exploiting dynamic trajectories rather than trimming or quantizing weights. In practical terms, it means massive segmentation models like SAM can be shrunk to tiny storage footprints without retraining or performance loss.","The chart shows an initial guess in blue being updated step by step, landing on the green markers that move toward the red target point. The purple boundary frames the area of interest, and the dotted lines hint at how each update pushes the guess closer to the goal. Comparing early and later updates makes it clear that more steps yield a tighter approximation of the desired value. This pattern confirms that repeating the same update rule can reliably refine an estimate to reach a specific target."
2507.08771v1-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/arxiv_articles/processed_articles/2507.08771v1-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/arxiv_articles/processed_articles/2507.08771v1-with-image-refs_artifacts/image_000003_ff9df14cc5c6595ac9125e1d9738fd8503be8034b376904a6bf9e0045f292161.png,"## 4.1.1 Overall Results

To demonstrate the rationality of our architecture, we conduct experiments by comparing BlockFFN with multiple sparsely-activated architectures: Vanilla TopK MoE, DeepSeekMoE (DSMoE) (Dai et al., 2024), GRIN (Liu et al., 2024c), and ReMoE (Wang et al., 2024b) (see Appendix C). To ensure fairness, we keep consistent settings for attention layers and MoE experts (i.e., the number and intermediate dimension of experts) throughout baselines and BlockFFN. Besides, all settings (within each scale) have close parameter numbers, training token numbers, and token-level sparsity. We involve four parameter scales: Small (0.1B), Medium (0.5B), Large (0.8B), and XLarge (1.2B). See Appendix D for model settings.

We adopt two comparison metrics: perplexity (PPL) on validation datasets and evaluation scores on benchmarks. Benchmarks include two groups: commonsense reasoning (C.R.) and reading comprehension (R.C.). See Appendix E for details about data and benchmarks.

The PPL and evaluation scores are shown in Table 2 and 3, respectively. The training curves of the 'XLarge' settings are drawn in Figure 1b. We can draw the following observations:

(1) Performance : Under close parameter numbers, all the settings (except for Small ReMoE and BlockFFN) cannot match the 'Dense' setting, due to the performance compromise of sparsification. However, under close TLS values (i.e., identical average FLOPs for each token), BlockFFN outperforms other MoE baselines in terms of validation PPL, train loss, and scores on downstream tasks , showing less performance compromise.

Figure 3: For each token in vocabulary, we calculate its frequencies and average ratios of activated experts, which show a bimodal distribution of expert allocation.

Figure 4: The layer-wise distributions of average activation magnitudes on the 'Small' settings. While BlockFFN uses CLS-aware objectives, ReMoE adopts L1 regularization.

(2) Sparsity : Under close TLS values, BlockFFN always has considerably higher CLS values than other baselines . Attributed to CLS-oriented training objectives, this property makes BlockFFN more friendly for acceleration.",2507.08771v1-with-image-refs,"The chart shows that both BlockFFN variants steadily amplify their activation strengths from the first to the last layer, far surpassing the modest growth seen in the competing model. Removing RMSNorm in BlockFFN triggers the most dramatic rise, with activation magnitudes nearly tripling by the final layer, while keeping RMSNorm yields a smoother increase that still outperforms alternatives. In contrast, ReMoE’s L1-based regularization keeps activations relatively low, suggesting a more uniform but weaker participation of its sub-networks. This demonstrates that tailoring the loss around the classification token inherently drives stronger, more concentrated learning in deeper layers, which can speed up convergence and improve inference efficiency. Applying a block-level normalization like RMSNorm offers a practical balance, tempering extreme activation growth for greater stability without losing the focused benefits of the classification-driven objective.","The chart tracks how the average signal strength grows across successive processing layers in three model variants. Without a normalization step (blue line), activations swell rapidly, nearly tripling by the final layer and risking unstable training. Adding RMS normalization (red line) tames this growth substantially but still allows a steady climb. In contrast, the sparse mixture‐of‐experts approach guided by an L1 objective (green line) holds activations consistently low, demonstrating a tightly controlled flow of information. These differences highlight that both normalization and targeted sparsity are powerful levers for keeping neural activations in a healthy range and promoting more stable learning."
2507.08776v1-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/arxiv_articles/processed_articles/2507.08776v1-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/arxiv_articles/processed_articles/2507.08776v1-with-image-refs_artifacts/image_000003_400fb125b741a72f6db786d8b5c16a6d271a2451b7ee8fa05f806bc438fcda25.png,"## 4.2 Main Results

Figure 2 and Figure 4 show the main evaluation results on the RealEstate10K and DL3DV datasets. In the plots, the x-axis represents the data size of the scene representation: Storage CLiFTs for our method, decoder input tokens for LVSM, and splats for MVSplat and DepthSplat. The y-axis reports PSNR; due to space constraints, the LPIPS and SSIM plots are deferred to the Appendix, where they show consistent trends.

None of the baselines support controlling the data size and require a separately trained model for each data point. Many baseline points are missing from the plots because we either use publicly available checkpoints (for MVSplat and DepthSplat) or train a new model (§4.1). In contrast, our method trains a single model per dataset and supports fine-grained control over both the storage data size and the render data size via the numbers of Storage ( N s ) and Render ( N r ) CLiFT tokens.

The plots show that CLiFT achieves comparable PSNR with approximately 5-7 × less data size than MVSplat and DepthSplat, and about 1.8 × less than LVSM, highlighting the effectiveness of our compressed tokens and overall scene representation. CLiFT also attains the highest overall PSNR with significantly lower data usage. Qualitative results in Figure 4 support these findings: our method preserves sharp appearance details that are closer to the ground truth and maintains high visual fidelity even under strong compression, with only minor loss in high-frequency content.

Figure 3: Ablation studies on our individual components, in particular, latent K-means and neural condensation. The plots compare three variants of our system by dropping latent K-means and neural condensation one by one from the system, while varying the data size. Specifically, the x-axis is the size of the scene representation. The y-axis is rendering quality (PSNR, LPIPS, and SSIM), rendering speed (FPS), or rendering cost (FLOPs), measured on an NVIDIA RTX A6000 GPU.

Table 1: We fix the number of storage CLiFTs to represent a scene ( N s =4096), then vary the number of render CLiFTS (how many tokens to use for rendering) on-the-fly and measure rendering quality (PSNR), rendering speed (PSNR), and rendering cost (theoretical number as GFLOPs).

| Metrics   |   Render CLiFTs | Render CLiFTs   | Render CLiFTs   | Render CLiFTs   | Render CLiFTs   |
|-----------|-----------------|-----------------|-----------------|-----------------|-----------------|
| Metrics   |         4096    | 3072            | 2048            | 1024            | 512             |
| PSNR      |           26.72 | 26.71 (-0.01)   | 26.56 (-0.16)   | 25.75 (-0.97)   | 23.89 (-2.83)   |
| GFLOPs    |           70.6  | 63.3 (-10%)     | 56.1 (-21%)     | 48.9 (-31%)     | 45.23 (-36%)    |
| FPS       |           54.3  | 53.89 (-1%)     | 66.44 (+22%)    | 80.77 (+49%)    | 90.15 (+66%)    |",2507.08776v1-with-image-refs,"The chart shows that combining latent K-means clustering with neural condensation delivers the highest rendering fidelity per data token, reaching nearly 30 dB PSNR, 0.10 LPIPS, and 0.92 SSIM at the 4096-token setting. Removing K-means leads to the largest drop in quality (about 0.5–1 dB PSNR) and higher perceptual error, while dropping the condenser also hurts fidelity, though to a smaller extent. Beyond roughly 1024 tokens, PSNR gains taper off, indicating that most scene detail can be captured with a moderate token budget. Rendering speed falls smoothly from around 110 FPS at 128 tokens to about 55 FPS at 4096 tokens, and theoretical cost rises linearly from ~12 GFLOPs to ~72 GFLOPs, illustrating a clear trade-off between quality and performance. These results confirm that each component is essential for building a compact, high-quality scene representation that can be tuned on-the-fly without retraining.","The chart shows that the full method (blue line) consistently delivers the highest reconstruction quality—measured by PSNR and SSIM—and the lowest perceptual error (LPIPS) across every storage/render budget, with the largest improvements at lower token counts. Quality smoothly increases as more CLiFTs are allocated, illustrating a clear trade-off between memory footprint and image fidelity. Removing the condenser or the K-means step causes noticeable drops (up to 2 dB PSNR and higher LPIPS) at the same budgets, confirming their importance. Meanwhile, rendering speed gracefully tapers from over 100 FPS to about 60 FPS as model size grows, yet remains roughly twice as fast as the LVSM-ED baseline at maximum capacity. The computational workload (GFLOPs) rises predictably with budget but stays below or in line with the competing method, highlighting an efficient balance of quality, speed, and resource use."
2507.08784v1-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/arxiv_articles/processed_articles/2507.08784v1-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/arxiv_articles/processed_articles/2507.08784v1-with-image-refs_artifacts/image_000005_94edd31b0621700b447074ce346f0cef4c4f83d6d4ecee89edf2a8fe26e48ad2.png,"## D. Memory Analysis

Figure 5 reports the peak GPU memory of GreedyLore during pre-training of LLaMA models. For models with up to 350 M parameters, we use a per-GPU batch size of 128. For the 1B-parameter model, we reduce the batch size to 64 due to memory constraints. We do not employ gradient accumulation here, even though it enables larger effective batch sizes.

The memory overhead of GreedyLore comprises two components. First, the error-feedback mechanism requires additional storage proportional to the total number of model parameters. However, activation tensors dominate memory usage during pre-training, rendering this overhead negligible. Second, storing the projection matrix U ∈ R min( m,n ) × min( m,n ) incurs storage of size min( m,n ) 2 by selecting either a leftor right-sided projection (see Appendix D). Consequently, the overall memory overhead of GreedyLore remains negligible, as confirmed by our profiling results.

Fig. 5: Peak memory of pre-training of LLaMA models.",2507.08784v1-with-image-refs,"The chart shows that peak GPU memory usage rises from roughly 12 GB for a 60 million-parameter model to about 50 GB for a 1 billion-parameter model as model size increases. Across all sizes, GreedyLore—with or without the error-feedback mechanism—matches the baseline AdamW memory footprint almost exactly, with only a 1–2 GB uptick when error feedback is enabled. This small increase confirms that the extra storage for error feedback and the projection matrix remains negligible compared to activation tensors. In practice, you can adopt GreedyLore for large-scale pre-training without worrying about any significant GPU memory penalty.","The chart compares peak GPU memory use of three training approaches—AdamW, GreedyLore without an extra feature (EF), and GreedyLore with EF—across four LLaMA model sizes. For the smallest 60 M–parameter model, GreedyLore without EF uses slightly less memory than AdamW, while adding EF nudges it just above the baseline. As models grow to 350 M and 1 B parameters, both GreedyLore variants begin to exceed AdamW’s footprint, with the EF-enabled version showing the largest overhead. This pattern highlights that the extra feature introduces a moderate but compounding memory cost at scale. In practice, teams working with very large models should plan for this additional GPU memory requirement or consider sticking with the lighter-weight version when resources are limited."
"""Did_I_buy_that_just-with-image-refs","/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/""Did_I_buy_that_just-with-image-refs.md","/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/""Did_I_buy_that_just-with-image-refs_artifacts/image_000006_baa0d407f0fbba5355dafa833b7d7f1e3e331f706135b2da22e050080103059e.png","## 4 Results

- *Descriptive Statistics: Sample*

After the exclusion of outliers (reported in section 3.5), a total of n=290 participants (n=118 male, n=171 female, n=1 non-specified) were analyzed. Their mean age was 33.27 years (SD = 10.34, Range: 18-71). Table 1 summarizes demographic and other characteristics of the participants. The sample is non-representative and follows the known pattern of Prolific’s panel to be younger, more female and more educated than the general population (Prolific, 2024). Net household incomes in the sample were distributed around the average net household incomes in Germany (€3174), Austria (€3205), and Switzerland (€7132) (Eurostat, 2024). Compared to the population in these countries, the sample also contains a higher share of people with another country of birth (41 % in the sample vs. 17.3 % in Germany and 19.3 % in Austria, Statistisches Bundesamt, 2023). 32 % of the sample (94 out of 290) were enrolled as students at the time of the study.

| Variable                                | n =290   | % sample share   |
|-----------------------------------------|----------|------------------|
| Country of residence                    |          |                  |
| Germany                                 | 253      | 87 %             |
| Austria                                 | 27       | 9 %              |
| Switzerland                             | 10       | 3 %              |
| Country of birth                        |          |                  |
| Germany/Austria/Switzerland             | 172      | 59 %             |
| Other                                   | 118      | 41%              |
| Highest education                       |          |                  |
| Secondary school or lower               | 19       | 7 %              |
| Highschool or trade school              | 79       | 27 %             |
| University degree                       | 184      | 63 %             |
| PhD                                     | 8        | 3%               |
| Net household income                    |          |                  |
| Less than €1000                         | 26       | 9 %              |
| €1000 to €1999                          | 51       | 18 %             |
| €2000 to €2999                          | 70       | 24 %             |
| €3000 to €5000                          | 82       | 28 %             |
| More than €5000                         | 48       | 17 %             |
| Unspecified                             | 13       | 4 %              |
| Familiarity with online (food) shopping |          |                  |
| No experience with online shopping      | 1        | 0 %              |
| Shops non-food products online          | 77       | 27 %             |
| Tried online food shopping once         | 138      | 48 %             |
| Regularly shops food online             | 74       | 26 %             |
| Dietary preferences                     |          |                  |
| No restrictions                         | 195      | 67 %             |
| Flexitarian (i.e. mostly vegetarian)    | 54       | 19 %             |
| Vegetarian                              | 27       | 9%               |
| Vegan                                   | 9        | 3%               |
| Other                                   | 5        | 2%               |
| Allergies/Intolerances                  |          |                  |
| No allergies                            | 240      | 83 %             |
| At least one allergy /food intolerance  | 50       | 17 %             |

Table 1: Characteristics of the sample (n = 290)

- *Descriptive Statistics: Shopping Behavior*

Table 2 contains summary statistics for indicators of shopping behavior. Older participants tended to take longer during the shopping task (Correlation between age and shopping time: *r* = *.* 31 *, p &lt; .* 05). Beverages were excluded from relevant metrics, as there was high variance in the spending in this category, and it was not of relevance to any of our analyses. Even after this adjustment, there was a large range for money spent. However, the median amount of spent (excl. beverages) was reasonably close to the German average of € 70 for a 2-person household (Statistisches Bundesamt, 2021).

| Variable                                          |     M |    SD |   Median | Range          |
|---------------------------------------------------|-------|-------|----------|----------------|
| Total shopping time (in mins)                     | 13.65 |  8.59 |    11.41 | 1.18 – 53.12   |
| Number of individual items chosen                 | 32.7  | 15.02 |    30    | 5.00 – 91.00   |
| Avg. time spent deliberating each item (in secs)  | 26.25 | 14.63 |    23.41 | 6.08 – 116.43  |
| Money spent in € (excl. beverages)                | 91.72 | 52.09 |    81.86 | 10.42 – 394.69 |
| Avg. price per chosen item in € (excl. beverages) |  2.15 |  0.61 |     2.09 | 0.81 – 7.68    |

Table 2: Summary statistics of shopping behavior. Avg, time spent deliberating each item is calculated as ""Total shopping time"" divided by the ""Number of individual items chosen"".

To estimate the validity of participants’ demonstrated shopping behavior, we asked them how regularly they buy products of the top-level categories in their daily lives. Figure 4 shows the average values for each of the top-level categories. Participants’ behavior was considered consistent if their choices in the shopping task included products from categories that they reported buying ”at least somewhat regularly” or more often (5 or higher on the 7-point Likert scale). Conversely, their shopping was also considered consistent if they reported buying from a category ”now and then” or rarer (4 or lower on the 7-point Likert scale), and then did not include any products of that category in their selection.

Figure 4: Mean values of habituality for each top-level category. Error bars represent standard deviations. Base: n=290.

On average (standard deviation), their behavior was consistent in 6.90 (1.50) out of 9 categories. Outliers (n=6) with inconsistent behavior in most (6 or more) categories were further inspected in terms of instruction compliance, but their behavior in the shopping task or the survey was not noticeable in any other way. Therefore, we did not exclude them from the analyses. It is conceivable that the one-time nature of our assessment failed to accurately capture the dietary habits of these participants. This is a limitation that dietary recall studies also acknowledge (Jonnalagadda et al., 2000). In general, fulfilling the shopping task seems to have been at least somewhat representative of participants’ usual purchasing habits.

- *Accuracy of Self-Reports*

In shopping self-reports, two types of errors were possible: Underreporting (false negative) errors refer to participants choosing products of this category during the shopping task, but then not indicating this in the self-reports, whereas overreporting (false positive) errors represent reported product choices from a category they did not actually choose any products from. This was possible for all three types of recall: TBR at top-level, IBR at sub-level, and CR.

Self-reports contained, in total, an average (standard deviation) of 3.81 (2.20) errors for a total of 29 self-report categories. The range of errors was between 0 and 13; the median was 4. Figure 5 shows how errors were distributed in the sample, and that 6 participants gave error-free self-reports. Conversely, 98 % of the sample had made at least one error, and most of the participants (86 %) had made more than one. None of the participants made use of the option ”I don’t remember” for any of the categories.

Figure 6 shows all categories included in the self-reports, and how many errors occurred for each of them. Further, it differentiates occurrences of underreporting and overreporting per category.

Figure 5: Frequencies of absolute error counts. Base: n=290.

*Figure 6: Frequencies of error counts, by category and error type. The dashed horizontal lines represent the separation of self-report types, from top to bottom: TBR, IBR, CR. Base: n=290.*

Generally, underreporting was more common than overreporting. Underreporting errors mostly occurred in cued recall (CR). In Figure 6, it is especially apparent how CR was the most common source of error. Participants reported their purchases more accurately when they were presented as clickable options (TBR and IBR) compared to having no specific cue to help them to remember.

Noticeably, top-level-categories, meat and sausage products, were more often overreported than underreported. A striking number of participants (n=95) overreported sausage-type products. Product categories that the sample reported buying frequently, such as vegetables (as can be seen in Figure 4), were less often falsely reported (see Figure 6). However, while this is inherently plausible, implications should be interpreted with caution. If fewer participants choose to shop for a category, there is also a lower chance for it to be reported inaccurately.

The measure of sensitivity *d′* reflects participants’ ability to differentiate signals from one another and is routinely used to compare performances in recall or detection tasks between subjects or conditions (Macmillan &amp; Creelman, 2005). Its calculation is detailed in section 3.5. The total average (standard deviation) self-reporting sensitivity *d′* was 2.18 (0.63). Table 3 contains more error measures for the three types of self-reports. Paired t-tests confirmed that the average accuracy was significantly different between self-report types ( *p &lt; .* 05, all significant), with participants showing the highest accuracy in text-based recognition (TBR, 89 %), followed by image-based recognition (IBR, 78 %) and cued recall (CR, 44 %).

|                          | Text-based recognition (TBR / “top-level”)   | Text-based recognition (TBR / “top-level”)   | Image-based recognition (IBR / “sub-level”)   | Image-based recognition (IBR / “sub-level”)   | Cued recall (CR / “other”)   | Cued recall (CR / “other”)   |
|--------------------------|----------------------------------------------|----------------------------------------------|-----------------------------------------------|-----------------------------------------------|------------------------------|------------------------------|
|                          | M                                            | SD                                           | M                                             | SD                                            | M                            | SD                           |
| Absolute error count     | 1.00                                         | 0.95                                         | 1.48                                          | 1.45                                          | 1.33                         | 0.95                         |
| Relative accuracy (in %) | 88.89                                        | 10.58                                        | 77.60                                         | 7.24                                          | 44.37                        | 31.75                        |
| Sensitivity d’           | 1.91                                         | 0.62                                         | 2.15                                          | 0.59                                          | Cannot be computed           | Cannot be computed           |

Table 3: Accuracy metrics for different types of shopping self-reports. Sensitivity d' cannot be computed for CR as there were only three instances (""other fruits/vegetables/types of meat"").

- *Exploratory Regression*

In addition to analyzing the occurrence of errors in shopping self-reports, we conducted a linear regression to explore which factors predicted self-report accuracy (expressed as sensitivity *d′* ). Table 4 shows the items that were used to capture the measured variables and summarizes their descriptive statistics. A more detailed documentation of how the questionnaire was administered is contained in the Appendix. The results of the linear model are shown in Table 5. Data from n=17 participants were not included in the model, as they had missing data points for survey questions. Two variables went into the regression model as aggregated indices with Cronbach’s ɑ = 0.83 (health-related identity) and ɑ = 0.82 (UTAUT effort subscale).

The regression model was overall significant at *p &lt; .* 05, explaining 8.18 % of the total variance in sensitivity (Adjusted *R* 2). The model revealed the following variables as significant ( *p &lt; .* 05) predictors of sensitivity *d′* : perceived mental load, average time spent deliberating each item, and dietary preferences (participants with a flexitarian, vegetarian or vegan lifestyle reported more accurately than omnivores). All these effects withstood robustness checks which are documented in Table 7, found in the Appendix.

Self-reported degree of deliberation (”While shopping in the online supermarket, I very consciously thought about my choice of products.”), gender (male participants reported more accurately than female participants) and household size (participants with a household size of 2 or more reported more accurately than participants in a single-household) emerged as borderline significant at *p &lt; .* 10.

| Variable                                                                                                                                                |     M |    SD |   Median | Scale Range   |
|---------------------------------------------------------------------------------------------------------------------------------------------------------|-------|-------|----------|---------------|
| Social desirability: sum score                                                                                                                          |  2.47 |  1.17 |     2.5  | 0 – 5         |
| UTAUT effort 1: ”I found it easy to learn how to use the online supermarket.”                                                                           |  6.34 |  0.94 |     7    | 1 – 7         |
| UTAUT effort 2: ”My interaction with the online supermarket was clear and understandable.”                                                              |  6.23 |  0.93 |     6    | 1 – 7         |
| UTAUT effort 3: ”I found the online supermarket easy to use.”                                                                                           |  6.02 |  1.22 |     6    | 1 – 7         |
| UTAUT_effort (aggregated)                                                                                                                               |  6.2  |  0.89 |     6.33 | 1 – 7         |
| TLX mental Load: ”How did you feel about the mental demands of the shopping task?”                                                                      | 38.9  | 26.57 |    35    | 0 – 100       |
| TLX effort: ”In your opinion, how much effort was required for the shopping task?”                                                                      | 37.64 | 25.87 |    34.5  | 0 – 100       |
| Attitude towards online food shopping: ”How do you feel about doing at least some of your grocery shopping online?”                                     |  3.75 |  0.96 |     4    | 1 – 5         |
| Self-reported degree of deliberation: ”While shopping in the online supermarket just now, I have thought very consciously about my choice of products.” |  3.94 |  0.82 |     4    | 1 – 5         |
| Health identity 1: ”I often think about my health.”                                                                                                     |  5.46 |  1.16 |     6    | 1 – 7         |
| Health identity 2: ”My health is an important part of my identity.”                                                                                     |  5.16 |  1.36 |     5    | 1 – 7         |
| Health identity 3: ”Being a healthy person is an important part of how I see myself.”                                                                   |  5.12 |  1.38 |     5    | 1 – 7         |
| Health_identity (aggregated)                                                                                                                            |  5.25 |  1.12 |     5.33 | 1 – 7         |
| Environmental identity: ”I act in an environmentally conscious manner even if it involves considerable costs and effort.”                               |  4.24 |  1.46 |     4    | 1 – 7         |
| Household size                                                                                                                                          |  2.21 |  1.07 |     2    | 1 – 10        |

*Table 4: Descriptive statistics of measured variables. Base: n=290.*

| Variable                                                                    | Estimate  (Std. Error)   | p-value   |
|-----------------------------------------------------------------------------|--------------------------|-----------|
| (Intercept)                                                                 | 2.32 (0.41)              | <0.000    |
| Social desirability: sum score                                              | -0.02 (0.03)             | 0.462     |
| Cognitive Factors                                                           |                          |           |
| UTAUT_effort                                                                | -0.01 (0.04)             | 0.839     |
| TLX_mentalLoad                                                              | -0.01 (0.00)             | 0.003     |
| TLX_effort                                                                  | 0.00 (0.00)              | 0.130     |
| Experience with online food shopping (vs. no experience)                    | 0.03 (0.09)              | 0.749     |
| Positive Attitude towards online grocery shopping (vs. neutral to negative) | 0.00 (0.09)              | 0.980     |
| Conscious Food Choice Deliberation                                          |                          |           |
| Self-reported degree of deliberation                                        | 0.09 (0.05)              | 0.079     |
| Avg. time spent deliberating each item                                      | 0.01 (0.00)              | 0.012     |
| Money spent (excl. beverages)                                               | 0.00 (0.00)              | 0.863     |
| No dietary restrictions (vs. flexitarian, vegetarian or vegan)              | -0.17 (0.09)             | 0.046     |
| Allergies (vs. no allergies)                                                | -0.13 (0.10)             | 0.193     |
| Health-related identity                                                     | -0.03 (0.04)             | 0.382     |
| Strong environmental identity (vs. neutral to negative)                     | -0.06 (0.08)             | 0.425     |
| Sociodemographic Characteristics                                            |                          |           |
| Education                                                                   | 0.02 (0.03)              | 0.544     |
| Income                                                                      | 0.00 (0.02)              | 0.868     |
| Female gender (vs. male)                                                    | -0.15 (0.08)             | 0.068     |
| Age                                                                         | -0.01 (0.00)             | 0.181     |
| Single household (vs. family)                                               | -0.18 (0.10)             | 0.067     |
| Born in Germany / Austria / Switzerland  (vs. another country)              | -0.10 (0.08)             | 0.217     |

Table 5: Results of linear model with a sample of n=273, after excluding participants with missing values. Dependent variable: Sensitivity d' of shopping self-reports. Multiple R²: 0.146, Adjusted R²: 0,0818, F(19,253)=2.28, p &lt; .05.

- *Additional Analyses: Share of Organic Products*

As discussed in section 3.2, there is a striking discrepancy between shoppers self-reported shares of organic purchases and their actual, noticeably lower market shares. Participants in our study were asked to estimate the organic share in their shopping baskets. On average (standard deviation), their estimations were off by 14.88 (13.88) percentage points, with the absolute error ranging between 0 and 65 percentage points. More participants overestimated (n=125) than underestimated (n=102) their organic share. Participants who overestimated vs. underestimated their organic share were compared in terms of their general self-report accuracy in a post-hoc t-test, but no significant difference was found between groups.

From the sample that gave an estimation of their organic share, for about half of them (n=120, or 51.72 %), their estimation was off by more than 10 percentage points. A few participants estimated their share perfectly (N=7), but most of them (N=5) had an organic product share of 0 %. The distribution of estimation errors is shown in Figure 7.

In line with expectations, participants who scored high (i.e., higher than 5 on the 7-point Likert scale) on environmental identity chose a significantly higher share of organic products ( *p &lt; .* 05). This corresponded with a higher degree of error in their estimations that was borderline significant ( *p &lt; .* 10).

To improve our understanding of factors influencing organic estimation accuracy, we again employed a linear model, with participants’ absolute estimation error as dependent variable. 68 participants were not included in the model due to missing values in survey items or because they ticked ”I don’t know” upon being asked for an estimation of their organic share. Table 6 contains the results for this model.

Figure 7: Histogram of the degree of deviation of participants' estimation from their observed organic share. Note: A deviation of exactly 0 only occurred for n=7 participants. Base: n=236 who gave an estimate of their share of organic products.

The model was overall significant at *p &lt; .* 05, explaining 16.32 % of the total variance (Adjusted *R* 2) in the absolute deviation between participants' estimation and their observed organic share. Perceived mental load in the NASA-TLX emerged as a significant ( *p &lt; .* 05) predictor in the model, just as it did for the prediction of overall self-report accuracy. Among indicators for conscious food choice deliberation, self-reported degree of deliberation and health-related identity emerged as a significant ( *p &lt; .* 05) predictors for higher absolute estimation errors, while average deliberation time per item was borderline significant (p &lt; .10) for predicting lower estimation errors.

Age was also revealed as a significant predictor, i.e., younger participants were better at estimating their organic share. Another significant ( *p &lt; .* 05)  predictor was the country of birth, i.e., people who were born in a different country from the one they are living in more accurately estimated their organic share.  However, these effects should be interpreted with caution, as age also significantly positively correlated with the chosen share of organic products (Age: r = .16, p &lt; .05). Similarly, non-immigrants had a significantly higher organic share than people with a different country of birth. A higher organic share results in an increased potential for estimation errors, which may have contributed to this observation.

In addition, the regression model revealed the presence of allergies as a significant predictor, i.e., participants without allergies estimated their share more accurately. A post-hoc t-test revealed a significant  ( *p &lt; .* 05)  distinction in estimation error between individuals with allergies and those without. However, this is based off of only n=38 of participants who gave an estimation and also reported to have food allergies (vs. n=198 without allergies).

| Variable                                                                     | Estimate  (Std. Error)   | p-value   |
|------------------------------------------------------------------------------|--------------------------|-----------|
| (Intercept)                                                                  | 6.41 (10.07)             | 0.525     |
| Social desirability sum score                                                | -0.24 (0.83)             | 0.771     |
| Cognitive Factors                                                            |                          |           |
| UTAUT_effort                                                                 | -0.88 (1.03)             | 0.398     |
| TLX_mentalLoad                                                               | 0.16 (0.05)              | 0.001     |
| TLX_effort                                                                   | -0.07 (0.05)             | 0.132     |
| Experience with online food shopping (vs. no experience)                     | 1.93 (2.19)              | 0.378     |
| Positive Attitude towards online grocery shopping  (vs. neutral to negative) | 0.89 (2.16)              | 0.682     |
| Conscious Food Choice Deliberation                                           |                          |           |
| Self-reported degree of deliberation                                         | 2.56 (1.22)              | 0.037     |
| Avg. time spent deliberating each item                                       | -0.12 (0.07)             | 0.090     |
| Money spent (excl. beverages)                                                | -0.01 (0.02)             | 0.420     |
| No dietary restrictions (vs. flexitarian, vegetarian or vegan)               | -1.49 (2.09)             | 0.478     |
| Allergies (vs. no allergies)                                                 | 9.56 (2.38)              | <0.001    |
| Health-related identity                                                      | 2.32 (0.90)              | 0.010     |
| Strong environmental identity (vs. neutral to negative)                      | -0.07 (1.95)             | 0.972     |
| Sociodemographic Characteristics                                             |                          |           |
| Education                                                                    | -0.11 (0.67)             | 0.874     |
| Income                                                                       | -0.74 (0.58)             | 0.205     |
| Female Gender (vs. male)                                                     | -0.09 (1.96)             | 0.963     |
| Age                                                                          | -0.19 (0.09)             | 0.038     |
| Single-household (vs. family)                                                | -0.17 (2.38)             | 0.945     |
| Born in Germany / Austria / Switzerland  (vs. another country)               | 4.66 (2.08)              | 0.026     |

Table 6: Results of linear model with a reduced sample of n=222, after excluding participants with missing values. Dependent variable: Absolute deviation between the estimated and the observed share of chosen organic products. Multiple R²:0.2352, Adjusted R²:0.1632, F(19,202)=3.268, p &lt; .05.","""Did_I_buy_that_just-with-image-refs","The chart reveals that participants’ self-estimates of their organic product share spanned a wide range, from underestimations of about 35 percentage points to overestimations exceeding 60 points, with most errors clustering within ±20 points. A noticeable skew toward positive values shows that shoppers more often believe they buy a higher share of organic items than they actually do. Nearly half of the sample misjudged their organic share by more than 10 percentage points, highlighting that simple recall questions may substantially misrepresent true behavior. These findings suggest that researchers and retailers should pair self-reported organic purchase data with objective basket or transaction records to obtain a more accurate picture of consumer behavior.","The chart reveals that self-reported shares of organic products tend to be overstated more often than understated, as shown by the heavier right-hand tail beyond zero. Most deviations cluster within a 10-percentage-point window, indicating modest misreporting is common, but a small group exaggerates their organic purchases by 40+ points. This bias suggests that people believe they consume more organic food than they actually do, which could inflate survey estimates. Researchers and policymakers should adjust for this upward reporting bias or use complementary measures to get more accurate consumption data."
1_UNDERSTANDING_AND_-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/1_UNDERSTANDING_AND_-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/1_UNDERSTANDING_AND_-with-image-refs_artifacts/image_000006_b261b90770fd59ea5cb50519db71ed7c81f8da27b61eddaee730475e1d6fead3.png,"## Key Insights from Household Habit Shifting:

The Time of Use (ToU) tariff is presented as an opportunity for savings, contingent on the consumer's ability to shift electricity usage to off-peak hours when rates are lower. However, a critical analysis of the simulation results reveals that this structure may disproportionately penalize vulnerable ""Stay at home"" households, for whom shifting energy consumption is not a simple choice but a significant burden that can compromise well-being. The simulation compares bills for two family types, demonstrating the severe financial consequences of not being able to shift consumption patterns. While the ""Out of home family"" is less affected, the ""Stay at home family"" faces a different reality. With their original habits, this family faces a significant financial penalty, with their ToU bill simulated at RM 536.96 a cost increase of RM 97.11 compared to the General Tariff.

Tariff Bill Comparison

Figure 7: Simulated Monthly Bills: Original vs. Shifted Habits for Domestic Users

Description: This bar chart directly compares the estimated monthly bill under the Time of Use (ToU) tariff for each domestic family type, showing both their 'Original' consumption habits and their 'Shifted' habits (where usage is moved to offpeak times).

The simulation suggests that only by actively shifting habits can this family mitigate the penalty, lowering their bill to RM 423.82 to achieve a modest saving of just RM 16.03. The stark difference between a nearly RM 100 penalty and a RM 16 saving represents the immense financial pressure placed on these households to alter essential daily routines. This inadvertently highlights a penalty on essential lifestyles, as the higher initial ToU bill is a direct consequence of their need to use electricity at home during the day. The premise that households with higher peak-hour consumption have the ""most to gain' is therefore flawed, as it overlooks that these are often the households with the least flexibility. For them, ""shifting habits"" is not an optimization but a compromise on essential needs. Ultimately, the model's requirement of ""active management and shifting of energy-intensive activities' places an undue expectation on caregivers, the elderly, or work-from-home professionals, turning a supposed benefit into a source of potential increase in efforts.

Weekday Hourly (kWh/day)

Out of home family (Shifted)

Weekday Hourly (kWh/day)

Figure 8: Visualizing Original vs. Shifted Hourly Consumption Profiles for Domestic Families",1_UNDERSTANDING_AND_-with-image-refs,"The chart compares weekday electricity use by hour for two family types before and after shifting consumption under a Time of Use tariff. The out-of-home family can move routine tasks into early-morning off-peak hours and noticeably reduce their peak-hour draw, yielding modest savings. By contrast, the stay-at-home family still records high mid-day and early-evening demand even after shifting, because many essential activities simply can’t be moved without disrupting care or work-from-home routines. This limited flexibility explains why they face a large bill penalty under the new tariff unless they make major compromises to their daily lives. The data highlights that a uniform Time of Use scheme may unfairly burden those with little capacity to shift energy‐intensive tasks, pointing to a need for tailored support or alternative pricing for such households.","The chart compares two typical weekday energy profiles—one household empty during the day and one with someone at home—under the current late-afternoon peak window and a proposed shift to 4–10 pm. Moving the peak period later means that out-of-home families would see a large chunk of their dinner-time cooking and laundry load hit the high-rate hours, so their bills could climb unless they run major appliances before 4 pm. Stay-at-home households already concentrate much of their usage in the morning and early afternoon, so they avoid most of the extra peak exposure but still incur heavy evening demand in the new window. In both cases, scheduling dishwashers, washing machines or cooking tasks before 4 pm or after 10 pm offers a clear way to reduce peak-period charges."
Algorithmic_Fairness-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/Algorithmic_Fairness-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/Algorithmic_Fairness-with-image-refs_artifacts/image_000000_1f1ddc421b65e44fabb784895bf0b683a2dc6cf19413b38289689f98a5ec1383.png,"## Legal and Operational Requirements

The legal and operational requirements affected the fairness perceptions of the participants because they limited the opportunities to implement a shift schedule they perceived as fair. Legal requirements reduced

the flexibility to adapt shift schedules to the healthcare workersÕ preferences because of mandatory rest periods between shifts or the number of allowed shift type rotations that had to be satisfied: Ò The labor law, [...], I do not find it fair. [...] It could have been scheduled differently, but you are not allowed to do so. For example, due to the rest periods Ó (I10). Similarly, personnel shortages and staff turnovers were recognized as a source of perceived unfairness since a shift schedule satisfying operational requirements could consider fewer preferences in this case. The addition of this theme marked the most notable enhancement compared to the OJT. On the one hand, in times of personnel shortages in the entire healthcare sector (OECD, 2023), it  is  not  surprising  that  the  resulting  inflexibility  to  adjust  shift  schedules  is  recognized  by  healthcare workers. On the other hand, legal requirements for shift schedules are mainly perceived as unfair. We argue that  legal  requirements  may  proactively  avert  situations  that  are  perceived  as  unfair,  such  as  ignoring contractual agreements. Still, employees may only notice instances where legal requirements impede their fairness perceptions and subconsciously ignore others where these requirements prevent unfair situations.

Figure 1. Thematic Map",Algorithmic_Fairness-with-image-refs,"The chart illustrates how rigid legal and operational demands around shift scheduling filter through four pillars of workplace justice—distributive, procedural, informational and interpersonal—and ultimately curb healthcare workers’ sense of fairness. Under distributive fairness, mandatory rest requirements and limited rotation options upset the balance between individual need and equal treatment, while staffing shortages shrink opportunities for meaningful participation and decision control. Procedural rules like adjustability, timeliness and consistency break down when flexibility is sacrificed to comply with regulations, and limited disclosure or one-way communications further damage informational fairness. Respectful interpersonal interactions also suffer when workers feel constrained by unyielding legal mandates. To restore fairness perceptions, organizations should focus on enhancing schedule flexibility, clear two-way communications and granting more decision-making autonomy within the bounds of necessary legal safeguards.","The chart presents a clear model for building fairness into business and legal operations, anchored by overarching legal and operational requirements. It distinguishes four interlinked fairness types—fair resource allocation, transparent information sharing, consistent and adaptable procedures, and respectful interpersonal treatment—that together shape people’s sense of justice. Distributive fairness balances individual needs with equal treatment, informational fairness relies on privacy protections and truthful disclosures, procedural fairness is ensured by rules like timeliness, consistency, adjustability, and accuracy, and interpersonal fairness is grounded in respect. Channels for participation and voice, including representativeness and decision control, reinforce procedural trust and legitimacy. Overall, this framework provides actionable steps to design systems that not only meet legal standards but also resonate with users’ expectations for fairness and respect."
Analyzing_Income_Ine-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/Analyzing_Income_Ine-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/Analyzing_Income_Ine-with-image-refs_artifacts/image_000002_fb1cf71533bf0d6714a279468b4c1f08f6a304b645afc7097bc6e8450fb24c37.png,"## 4. Inequality of Income Across Italian Regions

Inequality across the Italian regions between 2004 and 2020. The  analysis  of net  income inequality (S80/S20)  across  Italian  regions  from  2004  to  2020  reveals  significant  disparities  in  how  income distribution has evolved over time. While some regions have successfully reduced inequality, others have experienced  notable  increases,  particularly  in  the  South,  exacerbating  the  existing  economic  divide between northern and southern Italy. There is a slight fall in income inequality in Piemonte and Valle d'Aosta with absolute falls of -0.1 and a relative fall of -2.08% and -2.44%, respectively. This indicates a fairly stable economic scenario in the two regions with minimal deviation of the richest and the poorer part of the population. Trentino-Alto Adige and Emilia-Romagna also record slight falls in the S80/S20 ratio with a fall of -0.2 each. The falls imply that economic interventions or territorial dynamics stabilized the income distribution. Marche and Basilicata record the greatest falls in the S80/S20 ratio with falls of -9.76%  and  -12.24%,  respectively.  The  improvements  might  be  linked  with  social  interventions, economic  diversification,  or  demographic  factors  that  contributed  toward  expanding  the  wealth distribution  base.  But  a  few  of  the  areas  registered  notable  increases  in  income  disparities.  Liguria registered an 11.54% improvement in S80/S20 ratio that signifies a widening of the wealth and poverty gulf. The 4.08%, 9.76%, and 7.14% respective percentage variation rates increases also take place in the areas of Lombardia, Veneto, and of the state of Lazio. The direction of the increases in the latter areas signifies  that  although  they  rank  among  the  richest  in  Italy,  the  economic  development  may  not necessarily be equally distributed. The same direction of the increases also follows in the states of the Friuli-Venezia Giulia and of the states of the state of the Umbria and of the state of the Molise, although the increases of the latter states are smaller in scale. The strongest trends follow in the southern half of Italy with the top- and lower-income brackets' gulf widening enormously. The highest 20.97% growth of the S80/S20 ratio has the state of the Campania with an estimate of 7.5 in 2020. This dramatic leap marks the entrenched economic deprivations of the state of the highest employment rates and limited economic opportunity. The large increases of 17.65% and 17.31% also follow in the states of the state of the Puglia and of the state of the Sardegna and the smaller increases of 4.92% and 4.35% also follow in the states of the state of the Calabria and of the state of the Sicily, respectively. The widening disparities of the latter four states reveal underlying structural factors such as lower-quality labor markets and lower rates  of  industrialization  and  entrenched  socio-economic  deprivations  that  disproportionately  hit  the

lower-income  deciles.  The  evidence  also  highlights  the  widely-documented  north-south  economic cleavage of Italy. The north and center of Italy has experienced stable or slightly expanding disparities and southern Italy has experienced expanding disparities. Underpinning explanations of the trends will be the varied patterns of the development of the industry, employment prospects, government investment and social policies. Policy interventions against the disparities will be necessitated including investment in the education system, employment generation and development of the infrastructure with the intention of catalyzing the economic development of the South (Figure 2). Not adopting such interventions will further widen the disparities of southern Italy and entrench economic disparities at the expense of social mobility and economic prospects of future generations (Kocurová and Hampel, 2020; Daniele, 2021; Guzzardi et al., 2024; Rossi et al., 2024; Sbardella et al., 2021).

Figure 2. Inequality across the Italian regions between 2004 and 2020.

2020

Inequality across the Italian macro-regions between 2004 and 2020. The net income disparities statistics of Italy's macro-regions during the period 2004-2020 demonstrate a widening and permanent economic distance between the South and the North. The income disparities have increased throughout the entire macro-region set except that the relative growth has been highest in the Mezzogiorno and the South

specifically. The income disparities growth has been slight in the North. The general S80/S20 ratio of the macro-region of the north has gone up from 4.7 in 2004 to 4.9 in 2020 with a 4.3% increase. Within this macro-region, the increment has been slightly higher at 6.1% with the ratio going up from 4.9 to 5.2. This signifies that economic growth in the industrious areas has been good and that wealth has been slightly  unevenly  distributed.  The  macro-region  of  the  North-East  with  the  territories  specializing  in powerful small and middle-size firms had a slight increment of 2.3%, and the ratio had gone up to 4.85.2. This signifies that income disparities in this macro-region had been higher and had not changed that dramatically compared with the macro-region of the North-West. The income disparities had gone up in the center of Italy from 4.8 to 5.2 with an 8.3% increment. This signifies that economic disparities had been widening in this macro-region that has the major cities of Rome and Florence among them. The economic disparities may be explained by the workforce transformation in the employment markets and the rural-urban  disparities and  economic  reforms  that  had  benefited  the  higher-income  classes disproportionately. The highest and the most disquieting tendency has been the widening disparities of the Mezzogiorno macro-region with a general increment of the ratio of 6.0 in 2004 to 6.5 in 2020 with an 8.3% increment. In the Mezzogiorno, the South (Sud) posted the steepest growth at 6.4 and 5.7 and a 12.3% relative growth-the highest relative growth among the macro-areas. This reflects an widening gap between the richest and the poorest households in the South and possibly the effect of underlying economic  structural  issues  and  the  influence  of  high  rates  of  unemployment  and  limited  economic prospects. The islands (Sicily and Sardinia) also posted a notable growth in the S80/S20 ratio with the latter advancing at 7.0 and an associated 6.1% growth. This further reflects economic hardships in the two regions with heavy dependence on seasonal activities such as agriculture and the tourism sector that leads to income volatility and economic disparities. Nationally, Italy's S80/S20 macro area also posted an  increase  at  5.9  and  5.6  and  reflects  a  5.4%  growth  in  income  disparities  during  the  16  years  of observation. This development emphasizes the necessity of the efforts of the country toward equitable economic development. The north has been able to keep the problem of inequality at bay at least to a certain limit, the South and the islands posted widening disparities that reinforce Italy's old economic dualism. The statistics further imply that unless the government intervenes with specific interventions such as investment in the South's infrastructure and education and the provision of employment-these disparities between the two areas will be hard to stem and widen further in the future leading to further social and economic cleavages (Figure 3). The widening disparities in the center and north also imply the necessity of economic development interventions that encourage inclusive economic development and that the wealth that ensues will be dispersed among a larger population and not restricted among the higher-income earners (Sbardella et al., 2021; Culotta,  2021; Aresu et al., 2023).

Figure 3. Inequality across the Italian macro-regions between 2004 and 2020.

2020",Analyzing_Income_Ine-with-image-refs,"The visualization tracks the widening gap between the richest and poorest households across Italy’s macro-regions from 2004 to 2020. Northern areas have contained this gap with only slight 4–6% increases, while the Centre saw a more pronounced rise of about 8%. In stark contrast, the Mezzogiorno—particularly the South—jumped by over 12% and the Islands climbed around 6%, pushing their inequality well above the national trend. Italy overall experienced a moderate 5% uptick, nested between these regional extremes. These diverging paths underscore deep structural challenges in the South and suggest that targeted investments in infrastructure, education, and job creation will be critical to narrowing this persistent divide.","The visualization tracks a key indicator across Italy’s macroregions from 2010 to 2020, revealing that the North and Northwest consistently register the lowest levels (around 4–5 units) while the Mezzogiorno, Sud and especially the Islands remain at the highest end (peaking near 8 units) with more pronounced mid-decade spikes. The Centro region and the national average sit in between these extremes, rising steadily but without dramatic swings. All areas show a modest upward trend over the decade, suggesting a general national increase in this measure. Persistent gaps between North and South point to the need for focused strategies in the Mezzogiorno and Islands to bring their performance closer to Northern benchmarks."
Artificial_Intellige-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/Artificial_Intellige-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/Artificial_Intellige-with-image-refs_artifacts/image_000000_eca8b8d652b31d34c335984eb670c6e71ec6ecb28785906aa95e52054744f065.png,"## 3.2 AI in Account-Based and Customer-Based marketing

Account-Based Marketing  (ABM) is a strategic  marketing strategy that targets specific high-value accounts instead of a general audience. It is a personalized marketing effort based on the needs and interests of each account to ensure increased engagement  and  conversion  rates.  AI  supports  ABM  by analyzing data automatically, facilitating predictive targeting, and personalizing content. Machine  learning techniques  assist  in  detecting  the  best  potential  accounts, whereas AI-powered automation supports real-time interactions  in  the  form  of  chatbots,  dynamic  content,  and tailored messages.

AI-powered ABM has been implemented effectively by some firms through the utilization of AI-driven analytics platforms that  enable  them  to  discover  crucial  decision-makers  at targeted accounts and build highly personalized campaigns to enhance conversion rates. ABM uses AI to boost efficiency, enhance  lead  scoring  precision,  and  give  insights  about customers.  Nonetheless,  issues  like  data  privacy  issues, integration challenges, and excessive costs of implementation need to be tackled for successful implementation.

Fig 1 - AI-Driven Account-Based Marketing (ABM) Workflow

Customer-based marketing (CBM)  is  an  approach  that emphasizes individual  customer  behavior,  preferences,  and engagement patterns to provide highly individualized marketing experiences. In contrast to Account-Based Marketing (ABM), which personalizes campaigns for individual high-value accounts, CBM  aims to serve individual  consumers  in  multiple  segments.  AI-powered CBM  leverages  sophisticated  data  analytics  and  machine learning to forecast customer behavior, automate individualized  recommendations,  and  optimize  customer journeys.

For instance, online retail websites like Amazon utilize AI to browse  through history and past purchases to suggest relevant  products,  whereas  streaming  services  like  Netflix utilize AI-based  recommendation  algorithms  to  provide personalized  content  to  each  user.  Similarly,  ride-sharing companies  like  Uber  and  Lyft  utilize  AI  to  implement dynamic pricing models depending on demand, user location, and past patterns of rides. By analyzing customers' interactions  in  real-time,  AI  enhances  CBM  by  offering customized promotions, optimizing email marketing campaigns, and facilitating more efficient customer interactions.",Artificial_Intellige-with-image-refs,"The chart shows how integrating customer CRM records with AI-powered intent signals can automatically surface the most promising target accounts. Once these accounts are identified, the system dynamically serves tailored ads and tracks engagement to inform the sales team’s outreach. Captured performance metrics are then funneled back into the CRM to sharpen future targeting and messaging. This continuous feedback cycle not only accelerates campaign optimization but also aligns marketing and sales around data-driven insights. Companies that adopt this loop can minimize wasted spend, boost conversion rates, and quickly adapt to shifting account behaviors.","The chart illustrates a continuous feedback loop that starts by combining CRM records with AI-driven intent signals to pinpoint the highest-value accounts. Those prioritized targets are fed into ad platforms, where engagement data is captured and passed along to the sales team for personalized outreach. As sales interactions unfold, performance metrics are analyzed and recycled back into the CRM, sharpening both account scoring and ad targeting over time. This cycle ensures marketing and sales stay tightly aligned, focusing resources where they’re most likely to convert. By constantly learning from each stage, the process drives steadily improving engagement rates and maximizes overall ROI."
Automated_Video_Anal-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/Automated_Video_Anal-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/Automated_Video_Anal-with-image-refs_artifacts/image_000003_6fdfe755f8701b05ee3f5c863a82743eedc6b98d59d9c8b63b72014486c42528.png,"## Search Query

TS=(video* OR clip* OR spots OR livestream* OR ""live-streaming"" OR trailer*) AND (SO=('Journal of Marketing' OR 'Journal of Marketing Research' OR 'Journal of Consumer Research' OR 'Journal of Consumer Psychology' OR 'Journal of the Academy of Marketing Science' OR 'Marketing Science' OR 'Management Science' OR 'International Journal of Research in Marketing' OR 'Journal of Retailing')) AND (PY=(2000-2024))

Note. TS = topic, which means title, abstract, keywords plus, and author keywords of publications are searched for these words.

Study Selection. We identified studies in the selected journals by searching the WoS database using the search query outlined in Table 1, which initially returned 255 studies. These 255 studies were screened in two stages based on the inclusion and exclusion criteria outlined in Table 2 (see the flow diagram in Figure 3).

In the first stage, the author manually screened the identified studies' titles, abstracts, and keywords, excluding 211 papers. These were removed for reasons such as being conceptual studies, being unrelated to marketing, and not conducting any video analysis but examining contexts like the video game industry, movies, or video streaming platforms, among others. These exclusions ensured that only studies relevant to the research objectives advanced to the second stage of the screening.

In the second stage, the same author reviewed the full manuscripts of the remaining 44 studies, informed by the inclusion and exclusion criteria. An additional 19 studies were excluded in which humans conducted the video analysis manually or the study lacked sufficient methodological details. This process resulted in a final set of 25 publications for further analysis.

Table 2. Inclusion and Exclusion Criteria

| Criterion          | Inclusion                                                                                                                                                                                                                                                                   | Exclusion                                                                                                                                                                                                                                                                                                                                              |
|--------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Study Type         | Empirical Studies                                                                                                                                                                                                                                                           | Conceptual or Non-empirical Studies                                                                                                                                                                                                                                                                                                                    |
| Method             | Quantitative Research                                                                                                                                                                                                                                                       | Qualitative Research                                                                                                                                                                                                                                                                                                                                   |
| Language           | English                                                                                                                                                                                                                                                                     | Non-English                                                                                                                                                                                                                                                                                                                                            |
| Publication        | Published Work                                                                                                                                                                                                                                                              | Unpublished Work (e.g., working papers, preprints)                                                                                                                                                                                                                                                                                                     |
| Publication Titles | International Journal of Research in Marketing, Journal of Consumer Psychology, Journal of Consumer Research, Journal of Marketing, Journal of Marketing Research, Journal of Retailing, Journal of the Academy of Marketing Science, Management Science, Marketing Science | All other journals, except the list of journals on the left                                                                                                                                                                                                                                                                                            |
| Article Type       | Full-text Articles, Early Access, Online First                                                                                                                                                                                                                              | Abstracts, Proceedings/Conference Articles                                                                                                                                                                                                                                                                                                             |
| Context            | Marketing                                                                                                                                                                                                                                                                   | Non-Marketing                                                                                                                                                                                                                                                                                                                                          |
| Date               | 2000-2024                                                                                                                                                                                                                                                                   | Any study published before 2000 or in 2025                                                                                                                                                                                                                                                                                                             |
| Relevance          | /                                                                                                                                                                                                                                                                           | • Studies that do not contain video analysis (e.g., research context is the video game industry) • Studies that involve video analysis related to non- marketing contexts (e.g., security surveillance) • Studies which focus on non-automated video analysis or manual coding of video features • Studies that lack sufficient methodological details |

Figure 3. Flow Diagram of the Screening Procedure

Data Extraction and Synthesis. We extracted and synthesized the relevant data to address our research questions: (1) Which information have scholars automatically extracted from video data, and (2) which methods, software, tools, and techniques have they employed? We manually screened each manuscript, concentrating on the methodological and empirical sections. Where papers did not include sufficient details about the video analysis, we consulted the supplementary Web Appendix for the necessary information.

We structured the information on the automated video analysis in the papers into three categories, following Schwenzow et al. (2021), Li et al. (2019), Wang et al. (2024), and Zhou

et al. (2021): (1) video data characteristics , (2) technical video analysis approach , and (3) extracted video features . For each category, we defined subcategories to allow for a more indepth interpretation of the information in the papers. Figure 4 shows this framework for automated video analytics, including the categories, subcategories, and examples of the information retrieved from the 25 studies.

Figure 4. Framework for Automated Video Analytics: Key Dimensions and Approaches

For instance, video data characteristics include the subcategories of the modality of video data (i.e., audio, visual, text; Grewal et al., 2022), the extraction level (i.e., video, scene, or frame-level), and the frame rate when applicable. The modality of video data refers to the different ways information is communicated, such as through numerical indicators (e.g., number of likes or star ratings), text, audio, or visual content (Grewal et al., 2022). The extraction level specifically refers to the granularity at which features are extracted from a video, encompassing frame-level (individual frames), segment-level (series of frames), and video-level (entire sequence) analyses (Xu et al., 2024). The frame rate is the number of individual frames or images displayed per second in a video, typically measured in fps (Tekalp, 2015).

The technical video analysis approach category covers six subcategories. The first is the video analysis task : what should be analyzed in a video, e.g., object detection or audio classification. The second subcategory is the algorithm family , which refers to a broad category of algorithms grouped based on their underlying principles, methodologies, or intended tasks (Goodfellow et al., 2016). For example, neural networks and traditional CV are different algorithm families, each with distinct approaches to problem-solving. The third subcategory is the algorithm type , which specifies a subset within an algorithm family detailing the architecture, structure, or mechanism used to solve a task (Vaswani et al., 2017). For example, CNNs, recurrent neural networks (RNNs), and transformer-based models are types within the neural network family. Fourth, the specific models or algorithms subcategory refers to implementations of an algorithm type designed for a specific task or domain (Simonyan &amp; Zisserman, 2014). Examples include VGG (Visual Geometry Group; a CNN for image classification), OpenCV (a library for CV tasks), and GPT (Generative Pre-trained Transformer; a transformer-based model for natural language processing). The fifth subcategory is the type of solution , meaning if the software or tool is open-source (e.g., OpenCV) or proprietary (e.g., Google Vision, Microsoft Azure). The last subcategory, model training , categorizes if the model was pre-trained or was customized for the use case in the respective paper. In video analytics, model training is the process of optimizing a machine learning model's parameters using labeled or unlabeled video data to enable it to recognize patterns, extract features, and perform tasks such as object detection or action recognition, either through pre-trained or custom approaches (Goodfellow et al., 2016; Karpathy et al., 2014).

We separate extracted video features into the subcategories of content features (e.g., facial emotions, body language) and structural features (e.g., color brightness, scene cuts, duration) (Schwenzow et al., 2021; Lang et al., 1993). Structural features are lower-level technical aspects of videos that shape the video's presentation (Schwenzow et al., 2021).

While less interesting, structural features are objective and therefore important for analysis control. In contrast, content features focus on what is presented in the video, including visual elements like faces and objects, as well as viewer perceptions, such as emotions, providing higher-level insights (Schwenzow et al., 2021).

From this systematic literature review, we developed a coding scheme to systematically classify video analysis methods in the reviewed papers, reducing complexity and standardizing diverse information. For example, we categorized tasks involving detecting hands, objects, or people under 'object detection.' If researchers used ChatGPT for text analysis, we classified it as a transformer-based algorithm within the neural network family. This structured approach allows for more precise interpretation and synthesis of data across key dimensions, helping us answer the first two research questions. The detailed coding scheme is available in Web Appendix A.",Automated_Video_Anal-with-image-refs,"The diagram illustrates a rigorous two‐stage screening process that winnowed an initial pool of 255 studies down to just 25 publications meeting the criteria for automated video analysis in top marketing journals. In the first stage, 211 papers were excluded for reasons such as being conceptual, non‐marketing, or lacking automated video analysis in relevant contexts. The second stage removed another 19 studies—primarily because they relied on manual coding or did not report sufficient methodological details—resulting in a final set of 25 empirical papers. This steep drop‐off highlights that fewer than 10% of identified studies actually employ automated video analytics, signaling a clear gap in the marketing literature. To build on these findings, future research should prioritize the development and transparent reporting of automated video‐analysis methods to strengthen empirical insights in marketing.","The chart shows how an initial pool of 255 articles from Web of Science was narrowed down to just 25 studies, an exclusion rate of over 90%. Most papers were culled during title and abstract screening because they didn’t examine video content for marketing purposes. Further full-text review removed studies relying on manual coding or lacking clear methodological details. This steep drop-off reveals a significant gap in rigorously designed, data-driven video marketing research. Enhancing methodological clarity and broadening the focus could help build a stronger foundation in this emerging field."
Confessions_of_a_Gre-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/Confessions_of_a_Gre-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/Confessions_of_a_Gre-with-image-refs_artifacts/image_000000_2f1139537d93c2f65c31e3372103c1e4c692f528ef0b9ccfca4252a6b7fc1ac6.png,"## Conceptual Framework

To investigate the complex pathways through which religiosity influences green purchase intentions, this  study  proposes a  conceptual framework that integrates motivational and social influences. The model positions conspicuous virtue signaling, both self-oriented and other-oriented, as key mediators linking intrinsic and extrinsic religiosity to sustainable consumer behavior

Fig. 1 Conceptual framework",Confessions_of_a_Gre-with-image-refs,"The chart illustrates that people’s spiritual convictions influence their willingness to buy green products both directly and indirectly through two types of virtue signaling. Those with a strong personal faith tendency tend to showcase their environmental values either for internal moral satisfaction or to gain social approval, while those motivated by external religious aspects rely more on outward signals. Both self-oriented and other-oriented signaling significantly boost green purchase intentions, alongside a direct positive effect of religiosity itself. This suggests that sustainability initiatives will have greater impact if they tap into individuals’ private convictions and also provide opportunities for public displays of eco-friendly commitment.","The chart reveals that personal religious conviction and the social or practical aspects of faith shape environmental values in different ways, which then influence green buying choices. Deeply held beliefs boost both self-focused and altruistic motivations for protecting nature, while socially driven religious practice mainly fuels self-focused reasons for caring about the environment. Both types of environmental concern—whether aimed at personal well-being or the welfare of others—lead to stronger intentions to purchase eco-friendly products. This suggests that messages appealing to moral duty or communal benefit can be especially effective at encouraging sustainable consumer behavior."
Data-Driven_Innovati-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/Data-Driven_Innovati-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/Data-Driven_Innovati-with-image-refs_artifacts/image_000003_e3dad1a0685b22e9d0df826d87150d2c12f572f48c2d547de04986ba042db88a.png,"## 5.1. Existing Frameworks

Schemes have been proposed by governmental initiatives to categorise the risks posed by AI-centric artefacts. The AI Act xxiv agreed upon by the EU Parliament and EU Council classifies the risk associated with AI systems as prohibited, high, limited, and minimal. This classification can also be understood in accordance with Maslow's Hierarchy of Needs (Mcleod, 2007), as depicted in Figure 5.1. Prohibited are the livelihood and security-threatening systems, e.g., social scoring, biometric categorisation, and manipulative systems, e.g., artificially inferring emotions for profitable and malicious intentions.

Maslow's Needs Classification

Al Act Risk Classification

Figure 5.1 : The EU AI Act classification of risk associated with AI systems understood in accordance with Maslow's Hierarchy of Needs.

| Physiological Needs air; water; food, shelter; clothing, sleep_                                                         | Prohibited Risk behaviour manipulation, biometric categorisation, social scoring; emotion inference                          |
|-------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------|
| Safetyand Security health, security, employment; property-_                                                             | High Risk Public infrastructure and services, education, safety; employment; law, immigration, and justice                   |
| Love and belonging friendship, intimacy; family, connection _ Self-esteem respect; recognition, individuality, freedom_ | Limited risk General purpose Al systems requiring less transparency obligations, e.g., chatbots, deepfakes, content creation |
| Self-actualization morality, creativity; purpose, inner potential___                                                    | Minimal risk Unregulated systems such as Al video games, product recommendations, grammar check, photo editors               |

Under the high-risk category, the AI Act includes public infrastructure and services such as education, employment, law, migration, and justice etc. For the developers of high-risk AI, the AI Act mandates a risk management system, data governance, technical documentation, record keeping, usage instructions, human oversight, performance, and quality control. For systems with limited risk, e.g., chatbots, content creators,  etc.,  the  AI  Act  mandates  technical  documentation,  copyright  compliance,  and  usage instructions, while being transparent with the users that they are interacting with AI. The systems with minimal risk include self-development, creativity-enhancing, entertainment tools, etc.

In  literature,  Alfrink et  al. (2024,  p.  55)  classify  child  protection,  public  housing,  health,  social protection, security, taxation, etc., under public AI and built environment and mobility solutions like electric vehicle charging, parking systems, etc., under urban AI. Fernández-Llorca and Gómez (2023, p.  32)  review  state-of-the-art  autonomous  vehicles  in  accordance  with  the  recommendations  for trustworthy AI (see Table 2.1). They find that 'Technical Robustness &amp; Safety' is high ly mature, while other recommendations have low and medium levels of maturity.",Data-Driven_Innovati-with-image-refs,"The chart aligns the EU’s four AI risk categories with Maslow’s hierarchy of human needs, showing that any AI infringing on basic physiological needs (like food, shelter or health) is outright prohibited. Systems that underpin safety and security—such as public services, education or justice—are marked high risk and must follow strict risk-management, data-governance and human-oversight requirements. AI tools focused on social interaction and esteem—think chatbots or content generators—fall into a limited-risk zone, so developers need clear documentation, copyright compliance and user transparency. Finally, creative and self-development applications (video games, photo editors, recommendation engines) sit in the minimal-risk category, allowing innovators to move forward with only light regulatory requirements.","The chart aligns Maslow’s hierarchy of human needs with the EU AI Act’s four-tier risk classification, showing that AI applications touching our most essential survival functions are banned outright. Systems that impinge on safety and legal rights—such as healthcare, infrastructure or immigration tools—are labeled high risk and subject to strict oversight. General-purpose models enabling chatbots, deepfakes or content creation fall into a limited-risk category with moderate transparency duties. Everyday utilities like video games, product recommendations or photo editing face minimal regulation since they operate at lower-impact levels. This mapping underscores how regulatory scrutiny intensifies as AI applications target more fundamental needs, guiding organizations to prioritize compliance efforts accordingly. It also reveals a gap around creative and self-actualizing AI functions, suggesting that emerging technologies in this space may require new regulatory attention."
Decentralized_Distru-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/Decentralized_Distru-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/Decentralized_Distru-with-image-refs_artifacts/image_000002_9a8bd8ef481eb799a2143b091186a8de750a6d830122d03c3388c0f0869dc26a.png,"## Results

Perceived Payment Method Risk . As predicted, a one-way ANOVA revealed a significant main  effect  of  the  payment  method  ( F (1,  595)  =  235, p &lt;  .001).  The  results  confirmed  that cryptocurrencies  were  perceived  as  a  significantly  riskier  payment  method  compared  to  more traditional payment methods (M Cryptocurrencies = 4.13, SD Cryptocurrencies = 1.74 vs. M CreditCards = 2.20, SD CreditCards = 1.32, p &lt; .001).

Consumer  Sentiment . We  observed  significantly  more  negative  aspect  sentiment  for cryptocurrencies compared to credit cards as a payment method (see Figure 2:A; M Cryptocurrencies -.24, SD Cryptocurrencies = .60 vs. M CreditCards = .29, SD CreditCards = .54, p &lt; .001). Specifically, our study revealed that consumers have concerns regarding the security and usability of cryptocurrencies as a payment method (see Figure 2:B; M Cryptocurrencies = -.15, SD Cryptocurrencies = .64 vs. M CreditCards = .37, SD CreditCards =  .49,  p  &lt;  .001).  They  are  particularly  dissatisfied  with  the  payment  options (M Cryptocurrencies = -.15, SD Cryptocurrencies = .63 vs. M CreditCards = .24, SD CreditCards = .53, p &lt; .001) and worried about the associated purchase currencies and fees (M Cryptocurrencies = -.36, SD Cryptocurrencies = .54 vs. M CreditCards = .12, SD CreditCards = .63, p &lt; .001).

Mediation .  To  test  whether  perceived  payment  method  risk  mediates  the  relationship between the  payment  method  and  consumers'  firm  attributions  (i.e.,  consumer  sentiment),  we applied  PROCESS  Model  4  (Hayes,  2022)  using  10,000  bootstrapped  samples  (credit  cards dummy-coded  as  0  and  cryptocurrencies  as  1).  As  predicted,  the  negative  direct  effect  of cryptocurrencies as a payment method on consumers' firm attributions (b Direct = -.216, 95% CI Direct : [-.295; -.138]) was significantly mediated via first enhancing the perceived payment method risk (b PaymentMethod = 1.945, t(594) = 15.4, p &lt; .001), which in turn reduced consumers' firm attributions (b PaymentMethodRisk =  -.147,  t(593)  =  -13.4, p &lt;  .001).  The  indirect  effect  through  the  perceived

payment method risk was significant with the 95% confidence interval excluding zero (b Indirect = -.286, 95% CI Indirect : [-.342; -.237]).

Topic

Figure 2: Consumers express negative sentiment towards cryptocurrencies (vs. credit cards)",Decentralized_Distru-with-image-refs,"The chart reveals that consumers feel positively about credit cards across security, payment options, and fees, but express negative views toward cryptocurrencies on all three dimensions. Sentiment for cryptocurrencies dips most sharply around purchase costs and fees, signaling that pricing transparency and fee structure is a central concern. Negative perceptions also extend to the security and usability of cryptocurrency payments, suggesting users find these systems less intuitive or trustworthy. This pattern shows that perceived risk and complexity are significant barriers to broader cryptocurrency adoption. Companies looking to boost consumer confidence will need to simplify crypto payment processes, bolster security assurances, and clarify fee structures.","The chart compares average sentiment toward credit cards and cryptocurrencies across security & usability, payment options, and purchase & fees. Credit cards receive uniformly positive feedback, peaking on security & usability and remaining moderately strong on payment options and fees. In contrast, cryptocurrencies earn negative scores in every category, with the biggest downside rooted in purchase fees and complexity. The most pronounced divergence occurs around purchase & fees, highlighting cost concerns and purchase complexity as central hurdles for crypto adoption. To close this gap, cryptocurrency services should focus on clearer pricing models and a smoother checkout experience to earn more positive sentiment."
Deucalion__A_dataset-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/Deucalion__A_dataset-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/Deucalion__A_dataset-with-image-refs_artifacts/image_000000_9bf6e8c0f52945b21ae1f693d071d8a792da05daf565da766a71fc554866e834.png,"***Table 1:*** *flood classes*

| Deucalion’s classes as of June 2025   | Deucalion’s classes as of June 2025   |
|---------------------------------------|---------------------------------------|
| 1. flooded                            | 8. people                             |
| 2. vehicle                            | 9. pets                               |
| 3. destroyed trees                    | 10. wet surfaces                      |
| 4. vegetation                         | 11. sea                               |
| 5. other damage                       | 12. rain                              |
| 6. rocks and mud                      | 13. other meteo                       |
| 7. ruins                              | 14. river                             |
| 15. pool                              |                                       |",Deucalion__A_dataset-with-image-refs,"The chart depicts an end-to-end workflow for building a flood-image dataset by aggregating raw visuals from Kaggle repositories, other curated sources, and social media (including frames extracted from videos). In a centralized labeling environment, each image is tagged with one of fifteen flood-relevant classes—ranging from flooded areas, vehicles, destroyed trees and vegetation to debris, people, pets, rivers, rain and other meteorological features—before undergoing systematic quality checks. Once verified, the annotated images pass through digitization and feature-extraction steps that convert them into structured, model-ready data. By combining diverse inputs with rigorous annotation and validation, this pipeline ensures a rich and reliable foundation for training flood-detection and damage-assessment models.","The chart outlines a streamlined workflow for assembling and preparing image data from multiple sources—public datasets, social media, and other repositories—and even extracting individual frames from video content. These raw images are funneled into a centralized labeling environment where they’re tagged into predefined categories and subjected to quality checks to ensure annotation accuracy. Once labeled, the images undergo digitization and feature extraction, transforming visual information into structured numeric data. This end-to-end process delivers a high-quality, consistent dataset that’s ready for use in machine learning, analytics, or any application requiring reliable image-derived features."
Forbidden_Fruit_and_-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/Forbidden_Fruit_and_-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/Forbidden_Fruit_and_-with-image-refs_artifacts/image_000000_deb33a6a82013c3f69c68588de0019ff6829c4720a3e8d28efd17d72e9083996.png,"**4.0 Model Construction**

The present framework proposes a dual-stage theoretical model to explain how migrants initiate and then escalate engagement in an addictive consumption behavior after moving from a prohibitive to a permissive environment. We label the stages **Stage 1: The Forbidden Fruit Effect** (behavior initiation, transitioning from 0→1) and **Stage 2: The Rebound Effect** (behavior escalation, from 1→n). In Stage 1, drawing on **psychological reactance theory** (Brehm, 1966) and **commodity theory** (Brock, 1968), we argue that stringent prohibition in the country of origin imbues the forbidden behavior with heightened allure, leading to first-time adoption once the individual is in a freer context. In Stage 2, informed by **rebound effect theory** (an economic concept of over-consumption after efficiency gains or constraints removal) and coping dynamics from psychology, we explain how initial adoption can spiral into excessive or compulsive consumption. **Figure 1** illustrates the conceptual model, highlighting key constructs in each stage and their relationships.

*Figure 1: A dual-stage conceptual model of addictive consumption initiation and escalation among migrants. Stage 1 (Forbidden Fruit Effect) depicts how strong* ***perceived prohibition*** *in the origin context fosters* ***symbolic desire*** *and a* ***latent craving*** *for the forbidden behavior, culminating in* ***first use*** *upon migration. Stage 2 (Rebound Effect) shows how the* ***constraint release*** *of migration triggers an* ***opportunity shock*** *(initial surge in consumption), which—especially under* ***stress reinforcement*** *(using the behavior to cope with post-migration stress)—leads to* ***compulsive escalation*** *of the behavior.*",Forbidden_Fruit_and_-with-image-refs,"The chart shows that migrants’ sense of prohibition at home builds a strong symbolic attraction to a forbidden behavior, making first-time use almost inevitable once they arrive in a permissive setting. This initial surge in use then interacts with post-migration stress and coping efforts, driving consumption into a compulsive escalation. By framing the journey in two stages—initial forbidden‐fruit initiation and later rebound escalation—the model highlights how lifting constraints without support can inadvertently trigger deepening addiction. Effective interventions should combine early awareness about the risks of sudden freedom with accessible stress-management and coping resources. Addressing both the lure of the forbidden and the need for healthy coping can help prevent the slide from experimentation to compulsive use.","The chart illustrates how harsh bans can make an item more attractive, turning symbolic appeal into an underlying craving that leads to first use. When those bans lift, sudden freedom triggers an opportunity shock that, combined with stress and coping challenges, strengthens the habit and drives it toward compulsive overuse. This two-phase model shows that prohibition alone may backfire if people lack practical tools for managing stress. Introducing moderate restrictions alongside coping support and gradual exposure can prevent initial curiosity from spiraling into harmful addiction."
Moderating_Tamil_Con-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/Moderating_Tamil_Con-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/Moderating_Tamil_Con-with-image-refs_artifacts/image_000011_593471f7983fe787471624165cd2e5752ca64e7bae7c368f79a4575010c4c759.png,"## /gid00024/gid00035/gid00052/gid00001/gid00031/gid00042/gid00001/gid00052/gid00042/gid00048/gid00001/gid00047/gid00035/gid00036/gid00041/gid00038/gid00001/gid00047/gid00035/gid00032/gid00001/gid00046/gid00042/gid00030/gid00036/gid00028/gid00039/gid00001/gid00040/gid00032/gid00031/gid00036/gid00028/gid00001/gid00043/gid00039/gid00028/gid00047 /gid00033/gid00042/gid00045/gid00040/gid00001/gid00045/gid00032/gid00040/gid00042/gid00049/gid00032/gid00031/gid00001/gid00052/gid00042/gid00048/gid00045/gid00001/gid00030/gid00042/gid00041/gid00047/gid00032/gid00041/gid00047/gid00169

- Figure 3. Reasons participants identified as to why social media platforms removed or restricted their content (number of survey participants who stated that they have experienced content removals or restrictions, n= 83). Source CDT's online survey (NovemberDecember 2024).

takedowns and actions perceived to suppress the reach of their posts were taken to 'silence' their voices online. These suspicions were pervasive and were repeated in one-on-one interviews with Tamil creators, digital rights advocates, and content moderators.

One longstanding Tamil computing expert and former moderator noted the following when asked about his own experience facing moderation:

'It doesn't matter if it's in Tamil or English, when you are talking against the government the same thing happens. It's not the language, it's the bias. The post will be reported as spam and taken down as violating the community guidelines [...] Community guidelines are so vague. It's not like someone is sitting there and responding to us to understand what exactly went wrong. They'll just say 'You violated the community guidelines' so we read all the guidelines. We don't understand, what have we violated? I think it's for their own legal safety.' (Former moderator, October 2024, India)

Another interview participant suspected politically-driven moderation because they posted about Tamils fleeing persecution from Sri Lanka during the civil war. Multiple interview participants pointed to research and human rights advocacy that has long argued that over-moderation in the Sri Lankan context stifles social movements and 'obstructs

accountability' (Amarasingam &amp; Nandakumar, 2021; Ethirajan, 2021). One interview participant said that even when people posted images of shopfronts containing the very common name of 'Prabhakaran,' which is also the name of the slain leader of the L TTE, their posts would be taken down. One interview participant said that symbols that seemed to be associated with the Tamil nationalist movement were detected and taken down on social media, such as the gloriosa lily. 11 They were not notified whether their post was taken down due to a state request for takedown or proactive moderation, although Sri Lankan and Western media reporting suggest frequent requests for takedowns coming from the government (Constine, 2018; Tamil Guardian, 2024; Mallwarachi, 2024):

'There was a point during COVID, when there were no mass events. There were a few online activations for November 27th and people were sharing their respect online. This was a period where people weren't posting the [LTTE] flag or actual pictures of members or uniforms, but were still having content being taken down. The gloriosa lily, I had a friend post a picture with her friend holding lilies saying 'We remember and resist' and the post was taken down. It was shocking. How can a flower be violent? How did they find this?' (Sri Lankan Tamil journalist and digital rights advocate, January 2025, United Kingdom)

A few interview participants also suspected that they experienced opaque moderation, more commonly known as 'shadowbanning,' when they used certain terms in Tamil or another language. Shadowbanning is a colloquial term for a practice that encompasses a broad range of undisclosed content moderation actions, such as hiding a user's posts from other users, removing a user's handle or posts from search, or ranking users' content so low in a recommendation system that it is less discoverable (Nicholas, 2022). Prior research showed that, without clear disclosure from the platform about the circumstances under which it might moderate a user's content without informing them, users develop their own rationale for why their posts face a reduced reach and find ways to circumvent this type of intervention (Savolainen, 2022; Nicholas, 2022). A Tamil political commentator we interviewed emphasized that some of their political content gets less engagement:

11 The gloriosa lily is seen as a symbol and the national flower of the Tamil Nationalist movement, which has become synonymous with the Liberation Tigers of Tamil Eelam (LTTE) because it contains all the colours contained in the Tamil Eelam national flag.

'Especially when I write a lot about the RSS 12 , it goes to the lower feed. I have 54,000 followers but my reach is very limited. Certain posts where you use the word Gaza or Palestine or RSS or BJP, you know you'll only get…I can close my eyes and say in 10 minutes I'll get three likes. That's for sure. That's how I know it's been moved to the lower feed.' (Tamil journalist and digital rights advocate, November 2024, India)

We found that platforms do rely on government input to shape content moderation and often did not disclose this to users. Based on our interviews with former and current platform representatives, platforms often rely on government guidance to shape content moderation in Tamil in the following ways.

First, Indian and Western social media platforms often comply with state requests for takedown or user information with little notice to users. A moderator described that:

'[Platform] was pretty clear on if a request to takedown a specific content is going to come from a government of a country that states that it is legally obliged to do so, then perhaps [platform] was obliged to remove those content.' (Moderator on social media platform, December 2024, India)

Second, Indian and Sri Lankan laws related to intermediaries are often interpreted by Indian and Western social media platforms in overbroad and subjective ways resulting in takedown of Tamil content. Laws like the IT Act in India and the Online Safety Act, No. 9 in Sri Lanka require online services hosting user-generated content in the respective jurisdictions to take down illegal or harmful content, with differing degrees of specificity. These requirements have been interpreted in different ways by online services, and often are burdensome to smaller, local intermediaries (Kumar et. al, 2022). This has resulted in Indian platforms, and even Western ones, directly incorporating them into their community guidelines and guidance to moderators, as a head of a policy team at an Indian social media company indicated:

'The general principle was what came down from the IT Law… You cannot make comments that would create public disharmony

12 The Rashtriya Swayamsevak Singh or RSS is a Hindu nationalist organisation and the parent organization of the Bharatiya Janata Party (BJP), which is a political party, the party which is currently in power federally in India (Britannica, n.d.).

or discord which means you can't make comments about gods… For instance, Indian law does not allow you to denigrate the national flag, so you can't post content that would denigrate the national flag. Our community standards were largely a subset of that… If something happens on a platform and the police would reach out. If you posted it, you started a riot, the police will ask us for your details. And we'll provide your details…Similarly, there have been various other instances where they have asked for companies to take down things or keep an eye out for things…we'll be a little more proactive about it.' (Head of policy, Indian social media company, October 2024, India)

These laws result in vast takedowns of content, resulting in distrust in platform moderation that causes users to circumvent appeals processes and even content moderation processes altogether. A moderator of an online forum said that disclosure to users was essential to maintaining trust and navigating a tricky balance between compliance and user trust:

'I may be wrong, but if I'm removing content based on a government's request, and I keep on doing that, the users won't feel trusted that this is a safe space to come and talk. So if I am removing content, I would say to them, 'Because the government said I need to remove it.' If it's the second time, I should at least have a substantive policy to say, 'Hey, look this is some sort of government rule. These are the policies that you need to make sure you don't violate.' You also don't want the government to come back to you again and again and say hey remove this, and this.' (Moderator on a small online forum, November 2024, India)

/gid00001

One employee at a U.S.-based social media company said that distrust in platform moderation results in part from some social media users' perception that companies were aligning themselves with government actors, something that has long been documented by academics, civil society, and media (Sombatpoonsiri &amp; Mahapatra, 2024; Horowitz &amp; Purnell, 2020; Mirza, 2023).

Finally, Western social media platforms often increased their investments into content moderation capacity only after pressure from government actors. In interviews with Trust &amp; Safety staff, there was a sense that teams do not listen to anyone except for the government. For instance, in the aftermath of the Easter Sunday attacks in 2019, where the Sri Lankan government restricted access to social media platforms

under a state of emergency measure, one former policy employee noted that their company did pull out 'breakglass measures' to moderate Tamil content:

'There are breakglass measures, for example, in the aftermath of the Easter Sunday attacks, the prominent narrative was that the company was blindsided because it didn't have classifiers in Tamil or Sinhala and there was all this hate speech going unchecked. Which was definitely the case. But at some point, internally, the company did deploy something called 'Green Lantern,' which I think is a hate speech classifier, and which was able to contain hate speech in those local languages. There was a lot of secrecy around it because I think companies don't want it to be known externally that they have these capabilities. So there's a little bit of ambiguity on whether or not there is an actual resource gap, or whether it was the company's unwillingness or it doesn't suit their strategic needs to deploy resources where they might have them.' (Policy Lead at a U.S.-based social media company, December 2024, India)

In 2018, a year before those breakglass measures were used, companies were accused of failing to moderate instances of incitement of violence amongst Sri Lankan users which led to widespread riots in Sri Lanka. In response to those riots, the Sri Lankan government blocked access to social media services under an emergency order. In 2020, after an investigation commissioned by Meta, one of the companies whose services that order blocked, the company stated that it 'recogni[zed] and apologi[zed] for the very real human rights impacts that resulted' (Brustein, 2020). The company committed to hiring more staff who spoke Sinhalese and Tamil, and to using detection technology in Sinhalese to protect vulnerable groups including Muslims and Tamils (Facebook Sri Lanka Human Rights Impact Assessment, 2020). 13

Ultimately, interview participants argued that the real danger is in the growing self-censorship that threatens freedom of speech and the political environment in the region.

'You have to think a lot before you write and speak. People will automatically self-censor.  If the platforms don't want to censor them, they'll censor themselves. If one platform is not good, you

13 Meta also conducted a human rights impact assessment for India but never published it, despite calls for disclosure from international civil society (Access Now, 2022).

can find another platform. Or you can have your own website or your own server. The platforms taking you down is actually a matter of inconvenience. The real danger is the democratic climate of the country. We are putting ourselves [at] risk. More than the platforms themselves, the real risk is the climate, the democratic climate of the country that you're operating in.' (Former moderator, October 2024, India)",Moderating_Tamil_Con-with-image-refs,"The chart shows that about two-thirds of participants believe their posts were removed “to silence my opinion.” A smaller group attributed takedowns to having “violated community standards” (12) or being removed “for political reasons” (11). Very few cited platform bias, language barriers, or other causes, suggesting most see moderation as active suppression rather than a mistake or rule enforcement. This widespread view highlights a deep distrust in platform policies among Tamil creators. Platforms could address this by offering clear, localized explanations and transparent appeal processes whenever content is restricted.","The chart shows that more than half of respondents believe their content was removed to silence their opinion, while only about 15% acknowledge they may have violated rules and roughly 13% see political motives. Very few point to bias against them or language barriers as the cause. This gap highlights a trust issue in how moderation decisions are perceived. By providing clearer, case-specific explanations for removals and simplifying policy language, the platform can reduce confusion and strengthen user confidence in its processes."
Quiet_Quitting_-_Per-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/Quiet_Quitting_-_Per-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/Quiet_Quitting_-_Per-with-image-refs_artifacts/image_000003_fbcf3e540f920ab536876d862d894717998bf10f0cc9e5a6dbcf27c271ea9bfb.png,"## 3.4. Quiet Quitting Across Generational Groups

Quiet quitting is not just restricted to younger generations, such as Generation Y and Z. Recent studies (Alisha Johar et al., 2023; Hamouche et al., 2023; Serenko, 2024) show that workers from other previous generations can also manifest quiet quitting behaviors. Therefore, quiet quitting is not only prevalent among iGen workers and Millennials, but also similarly preeminent among

workers from other (older) generational groups, who are dissatisfied with ineffective management within organizations (Mahand &amp; Caldwell, 2023).

This fact has been corroborated by the data available in the Gallup Report (2023), regarding employee engagement with the organization's goals and mission. The Gallup Report tracks employee engagement in thousands of organizations around the world, measuring employees' perspectives on the most crucial elements of workplace culture. In 2023, the report highlights that the majority of workers worldwide fall into the quiet quitting category, with 59% of workers ""not engaged"". Figure 3 shows the percentage of ""not engaged"" workers for the different regions of the world, following the Gallup Report (2023).

Figure 3 - Quiet Quitting by Region, worldwide, according to the Gallup Report (2023)

Looking at Figure 3 reveals a worrying trend of disengagement in the workplace among workers in different parts of the globe. Most regions have a significant proportion of ""not engaged"" workers, with Europe leading the way (72%), followed by Southeast Asia (68%). This data is relevant to understanding the phenomenon of quiet quitting, as worker disengagement can have a negative impact on the work environment and, consequently, lead to high costs in lost productivity. In this regard, Johnson (2023) introduces the concept of a ""safety zone"" to explain why workers may choose to do only the minimum necessary in the workplace, i.e. quiet quitting. This zone is influenced by various factors (e.g. organizational culture, leadership, reward, and recognition

policies). In addition, the author incorporates an economic approach to analyze workers' behavior within the ""safety zone"". According to this approach, workers respond to incentives and opportunity costs in their work environment. If the benefits of committing more at work (e.g. recognition, promotions or salary increases) are not perceived as significant in relation to the costs (e.g. stress, additional effort, lack of recognition), workers may choose to remain in the ""safety zone"" and do only the bare minimum.

Based on these conclusions regarding Johnson's ""safety zone"" (2023), the authors of this article have drawn up a chart, in every way like a Control Chart, which illustrates the concept of the ""safety zone"" in the workplace, where the profile of workers is mirrored in relation to what is expected as a result of their productivity (See Figure 4). The upper threshold refers to the performance that the individual or group can achieve. The lower threshold refers to the performance deemed acceptable by the supervisor. The irregular line in the middle represents actual performance over time, reflecting the real productivity of an individual or group.

Figure 4 - Productivity variation and the ""safety zone"" in the workplace

The elements in Figure 4 help to visually illustrate the variation in productivity over time, highlighting the established performance limits and the actual performance of workers within the ""safety zone"".

The ""safety zone"" represents a state in the work environment in which workers feel comfortable performing the minimum necessary to keep their jobs, without pushing themselves beyond it.

Within this ""zone"", workers avoid attracting attention, either positively or negatively, and tend to avoid additional risks and efforts, opting to fulfill only the minimum expectations in order to avoid problems or conflicts.

Figure 4 also shows worker profiles, which illustrate how workers inside and outside the ""safety zone"" can position themselves in relation to the established performance limits and how their attitudes and behaviors can influence their productivity and engagement at work. Therefore, while ""Middle Workers"" and ""Accommodators"" fall within the safety zone, ""High Performers"",

""Careerists"" and ""Wage Criminals"" tend to operate outside these limits, for different reasons related to their behaviors and goals in the workplace.

Thus, ""High Performers"" are workers who constantly seek to exceed the established upper limit of productivity, actively seeking success and recognition through exceptional performance. These workers show a high level of engagement with their tasks and responsibilities and consistently exceed expectations. They are intrinsically motivated, proactively demonstrate autonomy and initiative at work and are resilient in the face of challenges and setbacks. As well as excelling individually, 'High Performers' are also able to collaborate effectively with teammates and lead projects or initiatives when necessary. They value constructive feedback and always look for opportunities to learn and develop. They are recognized as leaders and role models within the organization, in line with the conclusions of the studies by Hajra and Jayalakshmi (2024) and Pandey and Chauhan (2021).

'Careerists' are workers who also tend to operate above the safety zone, as they are focused on their professional career, working hard, and setting clear goals to achieve professional success. These workers show high professional ambition, with high self-confidence in their abilities. They are proactive in seeking opportunities for growth and development in order to progress professionally, and they value networking and building professional relationships. Although they are highly dedicated to their careers, these workers also value work-life balance, corroborating the studies by Fan and Sheng (2023) and Järlström et al. (2020)

'Middle workers' maintain consistent productivity that does not exceed the upper limit but does not fall below the lower limit either.  These workers try to avoid standing out to their superiors and colleagues, staying within a range of performance considered acceptable (in the ""safety zone""), to avoid problems and drawing attention to themselves. They are reliable and competent workers but may not actively seek opportunities for growth or promotion, as they value stability and work-life balance, corroborating the conclusions of the study by Farivar et al. (2023).

'Accommodators"" are workers who adopt a passive, adaptable and conformist stance in the workplace, following instructions and established norms without questioning or challenging the status quo. They prefer to stay within the established boundaries (""safety zone"") and avoid situations that could result in conflict or friction in the workplace, opting to maintain harmony and stability, even if this means not expressing their opinions assertively or disagreeing with decisions. These workers tend to avoid drawing attention to themselves, preferring to remain relatively ""invisible"" in the workplace, carrying out their tasks discreetly and without seeking recognition or prominence. They value work-life balance, seeking to maintain a clear separation between their professional responsibilities and their personal needs, avoiding overloading themselves with excess work, according to studies by Man et al. (2020) e Raval (2021).

'Wage Criminals' are workers who operate below the safety zone, violating workplace rules and regulations and failing to meet minimum expected performance standards. They represent a challenge, acting in a harmful and dishonest way, as they tend to perform only the minimum necessary to fulfill their responsibilities at work, showing disinterest, lack of motivation towards their tasks and the work environment in general, resulting in low engagement and productivity. They avoid taking on complementary responsibilities and extra tasks that may require additional effort, as they tend to do as little as possible to avoid work overload. This approach can lead to conflicts with coworkers or superiors, especially as it has a negative impact on the work environment and the team's productivity. Although they try to go unnoticed, these workers run the

risk of being identified as uncommitted and ineffective in their roles, which can result in dismissal, corroborating the studies by Fan and Sheng (2023) e Järlström et al. (2020).

The ""safety zone"" can be seen as a state of equilibrium for workers, where they try to avoid situations of stress or emotional exhaustion, opting for a more passive approach to work. However, by remaining in this zone, workers may display disengagement behaviors, such as quiet quitting, in which they disconnect emotionally and reduce their productivity. Therefore, understanding the dynamics of the ""safety zone"" is essential for organizations wishing to promote a healthy work environment and encourage workers to leave this comfort zone and become more meaningfully involved in their activities, promoting a more productive and satisfying work environment for everyone involved. It is therefore important to understand what organizational strategies should be adopted with workers to prevent the practice of quiet quitting.",Quiet_Quitting_-_Per-with-image-refs,"The visualization maps an individual’s productivity over time against defined upper and lower performance thresholds, illustrating a “safety zone” where many employees settle into doing the minimum required. Fluctuations within that zone represent “middle workers” and “accommodators” who remain engaged enough to meet expectations but avoid extra effort, while fewer consistently push above the upper limit as “high performers” or “careerists,” and some dip below the lower limit as “wage criminals.” This pattern shows that a large share of the workforce quietly quits by staying within a comfort zone rather than striving for higher outcomes, risking stagnation in engagement and productivity. Organizations can use these insights to tailor incentives, feedback and recognition strategies that nudge more employees out of the safety zone toward higher engagement. Tracking real performance against clear thresholds helps leaders identify who needs support, challenge, or reward to foster a more dynamic and committed workforce.","The chart shows that employee productivity fluctuates over time but remains trapped within a “security zone” bounded by upper and lower limits. Within this zone, individuals fall into categories from low-engagement “wage criminals” and “accommodators” up to “middle workers,” while those breaching the top limit are labeled “careerists” and “high performers.” The dashed line represents average performance, yet the solid productivity line repeatedly spikes above and dips below it, illustrating how people self-regulate to stay in their comfort band. This pattern indicates that, absent deliberate challenges or incentives, most employees will plateau in moderate engagement levels. By raising stretch goals, offering targeted feedback, or reshaping rewards, managers can nudge more people toward sustained high performance."
The_Limited_Role_of_-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/The_Limited_Role_of_-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/The_Limited_Role_of_-with-image-refs_artifacts/image_000001_9a95880ca9def823f29d381e103af1cd16f95f716fdbfc00f23aab00d30b5d2b.png,"## Cheating

In all rounds, participants reported, on average, more than the six expected correct predictions in the AFTER version of the task (see Figure 2), showing that some of them cheated. It is not possible to determine precisely the proportion of participants who cheated due to variations in the degree of cheating. However, the difference between the expected and observed proportion of participants reporting a given number of correct predictions allows for a conservative estimate of 36.5% of participants who cheated when they first played the AFTER version of the task. The true proportion is likely higher, given that this estimate assumes a minimum number of cheaters and maximum possible cheating consistent with the observed data. In reality, it is likely that a higher number of participants cheated somewhat less. A similar conservative estimate for participants who played the AFTER version of the task is appreciably higher for later rounds, where participants had to pay to play the AFTER version of the task, and it ranged from 69.6% in the third round to the maximum of 85.8% in the fifth round.

1 Even though we pre-registered that the personality measures will be standardized, we opted to keep the natural scale at the end. Given that standardization and centering differ only in scaling, this difference in analysis can influence only the reported regression coefficients, but not their significance.

Figure 2. The distribution of correct predictions per round . The figure shows the distribution of the number of reported correct predictions in the baseline measure of cheating separately for the two loss conditions and for participants who played the cheating-enabling (AFTER) version of the task in later rounds after payment. Observed means and their 95% confidence intervals are also displayed. The crosses show the binomial distribution expected if all participants reported their predictions honestly.",The_Limited_Role_of_-with-image-refs,"The chart shows that when participants simply played the prediction task (baseline), their average score hovered right around the six-correct mark you’d expect by chance, with only a slight uptick suggesting minimal dishonesty. Once the game was set up so they could cheat after paying, however, the entire distribution of reported scores shifted dramatically to the right, with a large spike at the maximum of twelve correct predictions and mean scores climbing above ten. This pattern grew stronger in later rounds—when stakes were higher, up to 85.8% of participants likely reported inflated scores. The data make it clear that introducing a cost to play while still allowing post-payment cheating greatly amplifies dishonest behavior. To curb this, designers should reconsider how financial incentives and reporting transparency interact in decision-making tasks.","The chart shows that in the initial, unpaid blocks participants’ self‐reported predictions cluster modestly above chance (around 6–8 correct out of 12) with very similar profiles under both high‐loss and low‐loss framings. The moment any payment scheme kicks in—whether via the BDM mechanism or an auction format—nearly everyone reports the maximum of 12 correct, collapsing all meaningful variation. That surge to perfect scores under incentive implies strong inflation of self‐reports rather than genuine performance gains. Moreover, the virtually identical spike at 12 across all paid blocks suggests that neither payment design deters overstatement. Taken together, these patterns warn that relying on participants’ payment‐linked declarations can produce highly misleading measures of actual ability."
VISTA__Verifiable_In-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/VISTA__Verifiable_In-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/VISTA__Verifiable_In-with-image-refs_artifacts/image_000000_00f9d2b044adfd2119124914e6bac76ab63804a1d016cf1e7a25d75b64e72564.png,"## 4. Agent Lifecycle Management in VISTA

VISTA formalizes agent behavior as a lifecycle embedded within an information architecture. This lifecycle is not just a technical protocol-it is a structural commitment to reflexivity, traceability, and adaptive governance. Inspired by biological and legal constructs, the lifecycle enforces checkpoints that ensure agent actions remain verifiable and aligned with institutional norms:

- Genesis - The agent is instantiated and issued a decentralized identifier (DID) and verifiable credentials (VCs), binding its existence to a cryptographically accountable identity.
- Active - The agent performs tasks within a trusted execution environment, maintaining real-time logs and enforcing logic constraints informed by organizational policy.
- Escalation - When decisions deviate from policy or present ambiguity, control is automatically routed to human or institutional oversight-a critical design for reflexivity in IA contexts.
- Sunset - Upon task completion or breach, the agent is revoked or decommissioned. Sunset records are preserved as part of the permanent audit trail, enabling institutional memory.

This lifecycle acts as a temporal skeleton within the broader information architecture-embedding feedback, escalation, and ethical alignment into the substrate of agent behavior.

Figure 1 illustrates this process: a user request is routed through the agent's lifecycle checkpoints

Figure 1 : VISTA Agent Lifecycle and Request Flow.

This diagram illustrates the processing of a user request across the modular VISTA layers. Each stage reflects a checkpoint in the agent's lifecycle-beginning with identity verification (Genesis), through secure execution and policy compliance (Active), potential escalation to human oversight (Escalation), and ultimately resolution or shutdown (Sunset). VISTA ensures cryptographic traceability and ethical accountability at every stage.",VISTA__Verifiable_In-with-image-refs,"The diagram lays out a modular workflow that ties every agent action to a verifiable identity before execution, ensuring that requests only proceed when both user and AI are cryptographically authenticated. By running sensitive logic inside a secure enclave and immediately checking outcomes against an ethical policy layer, the process embeds real-time compliance checks that can halt or escalate ambiguous decisions. Automated logging to a public ledger creates an immutable audit trail, feeding back into legacy systems and reinforcing accountability without interrupting operational flow. The built-in escalation checkpoint ensures any policy deviations trigger human review, striking a balance between autonomy and oversight that any organization can adopt to manage AI risk. This end-to-end lifecycle underscores how layered governance—from identity to execution, policy to audit, and interoperability—translates into transparent, traceable, and ethically aligned decision-making.","The chart maps a streamlined, multi-layered pipeline that vets each user request through identity verification, secure AI execution, ethical policy checks, public auditing, and legacy system updates before returning a response. Only authenticated users trigger agents running inside a secure enclave, and every action must pass a policy compliance check or be escalated for human review. Approved actions are immutably recorded on a public ledger to ensure transparency and auditability, while failed verifications or unapproved requests automatically halt the process. This design minimizes security and compliance risks by embedding stops and human interventions at every critical checkpoint."
A_MODULAR_SOFTWARE_F-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/A_MODULAR_SOFTWARE_F-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/A_MODULAR_SOFTWARE_F-with-image-refs_artifacts/image_000008_c7e3824421c6b0de1bf5cc907773351f9bebd499b208f52b8b68b7dbac03f588.png,"## 5.1 Single-Agent Experiments

This section presents the results obtained in the single-agent experimental settings, as introduced in section 3.3. Each Self-Modeling Agent configuration combines one policy module (BasePolicy, HybridPolicy, RLPolicy, or AdvancedPolicy) with one SelfModel variant (Simple or Advanced). The primary goal is to evaluate how different combinations influence agent behavior, meta-cognitive dynamics, and adaptive performance.

The evolution of key internal variables-confidence, fatigue, and agent mode-over time for each configuration is analyzed below. The corresponding scientific metrics extracted from the experiment logs are summarized in Table 2.

Dummy Policy + Simple/Advanced SelfModel DummyPolicy configurations serve as a baseline, with no learningdriven policy adaptation. The confidence trajectories remain relatively flat, while fatigue gradually increases. The AdvancedSelfModel introduces additional predictive monitoring (predicted confidence and mode), which yields slightly more stable confidence profiles and marginally improved switching dynamics. However, in the absence of active policy learning, the impact of self-modeling remains limited (see Figure 2 and Figure 3).

Hybrid Policy + Simple/Advanced SelfModel HybridPolicy configurations (Section 3.3) balance PPO-driven policy actions with stochastic exploration. This policy enables more dynamic confidence profiles and stronger fatigue modulation. The AdvancedSelfModel further enhances stability and predictive self-consistency, resulting in more adaptive mode switching between exploration and exploitation phases (see Figure 4 and Figure 5).

RL Policy + Simple/Advanced SelfModel Configurations with RLPolicy (Section 3.3) demonstrate the most sophisticated adaptive behavior. RLPolicy agents exhibit confidence trajectories tightly coupled to task performance, with pronounced and interpretable fatigue cycles. AdvancedSelfModel agents maintain more consistent internal predictions and leverage meta-cognitive feedback to modulate exploration-exploitation balance effectively (see Figure 6 and Figure 7).

Advanced Policy + Simple/Advanced SelfModel The AdvancedPolicy configurations (Section 3.3) represent the most integrated meta-cognitive design, dynamically modulating exploration and exploitation based on the agent's internal state. AdvancedPolicy + SimpleSelfModel already yields adaptive behavior with clear confidence cycles and interpretable fatigue dynamics. AdvancedPolicy + AdvancedSelfModel achieves the highest level of dynamic self-regulation, with optimized switching between exploration and exploitation and a well-balanced fatigue profile (see Figure 8 and Figure 9).

Quantitative Summary Table 2 summarizes the core scientific metrics across all single-agent configurations. Notably, AdvancedSelfModel consistently yields lower switching rates (indicating more stable behavioral modes), higher average confidence, and more adaptive fatigue regulation compared to SimpleSelfModel. AdvancedPolicy + AdvancedSelfModel emerges as the most balanced and self-aware configuration.

Table 2: Scientific Metrics Summary - Single-Agent Configurations

| Configuration       |   Switching Rate |   Avg. Confidence |   Avg. Fatigue | Time in Exploitation   | Time in Exploration   |
|---------------------|------------------|-------------------|----------------|------------------------|-----------------------|
| Dummy + Simple      |           0.9854 |            0.9962 |         0.7712 | 50.1%                  | 49.9%                 |
| Dummy + Advanced    |           0.914  |            0.9893 |         0.7511 | 53.3%                  | 46.7%                 |
| Hybrid + Simple     |           0.9948 |            0.9965 |         0.6865 | 52.1%                  | 47.9%                 |
| Hybrid + Advanced   |           0.9045 |            0.9905 |         0.6678 | 55.0%                  | 45.0%                 |
| RL + Simple         |           0.9999 |            0.9998 |         0.5927 | 50.9%                  | 49.1%                 |
| RL + Advanced       |           0.8806 |            0.9827 |         0.5721 | 55.9%                  | 44.1%                 |
| Advanced + Simple   |           0.8992 |            0.9881 |         0.565  | 55.7%                  | 44.3%                 |
| Advanced + Advanced |           0.8503 |            0.9805 |         0.5618 | 56.4%                  | 43.6%                 |

Discussion These results demonstrate the benefits of integrating meta-cognitive modeling within the agent architecture:

- AdvancedSelfModel consistently improves stability and internal state coherence, enabling more deliberate behavioral modulation.
- RLPolicy and AdvancedPolicy agents capitalize most effectively on self-model feedback, achieving dynamic yet controlled exploration-exploitation cycles.
- HybridPolicy configurations offer a robust compromise, benefiting from PPO guidance while retaining exploratory flexibility modulated by meta-cognitive signals.
- DummyPolicy agents, as expected, exhibit limited adaptive capacity, underscoring the value of learned policies coupled with self-modeling.

Overall, these findings validate that explicit self-modeling enriches agent behavior in single-agent settings, paving the way for more advanced meta-cognitive capabilities in multi-agent and lifelong learning scenarios, as explored in subsequent sections, even in relatively simple Gridworld environments, highlighting the generality of the approach.

Step

Step

Step

Step",A_MODULAR_SOFTWARE_F-with-image-refs,"The chart reveals that equipping the agent with both an advanced policy and an advanced self-model drives it to peak performance almost instantly and sustain it, as shown by a flat maximal reward line. Self-confidence sharply rises to near certainty and remains there, while fatigue levels stabilize at a moderate value and slowly creep up, indicating manageable cognitive load. The self-model’s confidence and mode predictions converge to the real values in under fifty steps, sending prediction errors to zero and confirming highly accurate internal monitoring. After this convergence the agent locks into exploitation mode, highlighting that it has identified the best strategy and sees little gain in further exploration. This combination therefore yields a self-aware, stable controller that minimizes unnecessary switching and maintains consistent performance.","The visualization shows that the agent reaches and maintains its maximum reward within the first few hundred steps, indicating that most learning occurs early and extended training yields limited gains. Its confidence level climbs to nearly 100% almost instantly, and the internal model’s estimates converge perfectly with actual confidence, demonstrating rapid and reliable self-assessment. Fatigue also rises sharply initially but then levels off around 0.82, suggesting that performance remains stable without additional cost. Because the agent quickly prioritizes its best-known strategy after the initial exploration phase, reducing exploration beyond that point could streamline training without sacrificing performance."
Adversarial-Resistan-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/Adversarial-Resistan-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/Adversarial-Resistan-with-image-refs_artifacts/image_000000_56d9cad6625947993039654d324cc3023d3cc284d88120fed50b29c3da75abd0.png,"## 3.1 Dual-LLM Architecture Overview

Our proposed system employs a dual-LLM framework consisting of a primary reasoning component and a specialized response validation module that operate in architectural isolation to prevent manipulation inheritance  [3][5] . The primary component utilizes DeepSeek Coder 7B or similar coding-CS specialized LLM, fine-tuned on curated CS50 educational materials and integrated with retrieval-augmented generation (RAG) using course-specific vector embeddings  [2] . The validation module employs another 7B or smaller parameter (let's suppose 1B) model trained specifically on adversarial examples to perform binary classification of response appropriateness, operating independently from the primary model's reasoning context  [4][5] .

The architectural separation principle ensures that emotional manipulation or adversarial prompts affecting the primary model's reasoning process cannot influence the validation component's assessment of pedagogical appropriateness  [4][5] . This design addresses fundamental vulnerabilities identified in single-LLM approaches where adversarial context can compromise both content generation and safety evaluation simultaneously  [4] .

Ensure conditions

Figure 1: Dual-LLM Architecture Overview showing architectural separation between reasoning (1st LLM) and validation (2nd LLM) components, with behavioral design elements including prompt processing and response trimming mechanisms",Adversarial-Resistan-with-image-refs,"The chart shows a two-step AI pipeline where a cleaned-up user prompt first goes to a specialized coding model that draws on course materials to generate tailored responses. A second, independent model then reviews and trims any dubious or manipulated content, blocking adversarial inputs from slipping through. By keeping the reasoning model and the validation model separate, the system ensures that harmful prompts can’t compromise both content creation and safety checks at once. This workflow also combines targeted data retrieval with automated trimming of inputs and outputs to maintain clarity and alignment with educational objectives. The result is reliable, high-quality feedback that prevents inappropriate or off-topic material from reaching learners.","The diagram outlines a multi-stage AI workflow that first sanitizes user inputs by stripping out emojis and extraneous expressions before they ever reach the core model. It then feeds the cleaned prompt into a specialized coding engine, enriched with context through retrieval-augmented generation to boost relevance and accuracy. A secondary “trimmer” model inspects the raw output for any hidden exploits or policy breaches, pruning away problematic snippets. A final set of automated checks enforces compliance and quality criteria before the answer reaches the user. This layered approach shows how combining input/output hygiene with domain-tuned models and safeguards can meaningfully reduce errors and security risks in automated code assistants."
Assessing_Data_Imbal-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/Assessing_Data_Imbal-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/Assessing_Data_Imbal-with-image-refs_artifacts/image_000014_9d576863b223cc632b0b9d3d396a3a01349d3e3f43a59c7911ead4a368dc4d61.png,"## 4.2 Model interpretation

Given that parameter interpretation of the penalised regression models was not possible due to the regularisation process, the standard logistic regression models were taken forward for interpretation. The model fitted with uncorrected data provided improved performance and thus was used for model interpretation. Ht was predicted to significantly increase collision probability ( 𝛽𝐻𝑡 = 0.811, [95 % CI: 0.079, 1.542], z = 2.173, p = 0.029) (see Table 2). For average levels of Hs and pupil diameter, a one standard deviation increase in Ht (~4 percentage points of normalised Ht ) resulted in an increase in the probability of a collision by 7 percentage points. Exponentiation of the standardised coefficient ( 𝑒 𝛽𝐻 𝑡 = 2.250) revealed that this was equivalent to a medium effect size, as defined by Rosenthal (1995).  This result implies that if the spatial distribution of gaze is at an average level and drivers have average levels of MWL (as indexed by mean pupil diameter), then an increase in the randomness of gaze transitions is predicted to increase the probability of a collision during critical transitions of control (see Figure 9).

Table 2: Model parameter estimates from uncorrected data GLM

| Predictors      |   Estimate |    SE |   z-value | p-value   |
|-----------------|------------|-------|-----------|-----------|
| 𝜷 𝟎             |     -2.664 | 0.339 |    -7.851 | <0.001    |
| 𝜷 𝑯 𝒕           |      0.811 | 0.373 |     2.173 | 0.029     |
| 𝛽 𝐻 𝑠           |     -0.064 | 0.267 |    -0.241 | 0.809     |
| 𝛽 𝐷 𝑚           |      0.05  | 0.345 |     0.146 | 0.883     |
| 𝜷 𝑯 𝒕 :𝑯 𝒔      |      0.517 | 0.243 |     2.123 | 0.033     |
| 𝛽 𝐻 𝑡 :𝐷 𝑚      |      0.231 | 0.356 |     0.65  | 0.515     |
| 𝛽 𝐻 𝑠 :𝐷 𝑚      |     -0.063 | 0.255 |    -0.249 | 0.802     |
| 𝛽 𝐻 𝑡 :𝐻 𝑠 :𝐷 𝑚 |      0.177 | 0.165 |     1.072 | 0.283     |

Figure 9: Relationship between standardised Ht and the probability of a collision for average levels of Hs and pupil diameter. As standardised Ht increases (i.e., as transitions of gaze become more random) the probability of a collision occurring during a critical takeover increase.

The model also highlighted a significant interaction between Ht and Hs ( 𝛽𝐻𝑡:𝐻𝑠 =  0.517, [95 % CI: 0.039, 0.994], z = 2.123, p = 0.033). For average pupil diameter levels, increasing Hs amplified the effect of Ht on increasing collision probability by a further 4 percentage points. Exponentiation of the standardised coefficient ( 𝑒 𝛽 𝐻 𝑡 :𝐻 𝑠 = 1.677) revealed that this was also equivalent to a medium effect size, albeit smaller than the effect of Ht individually. This result implies that when the spatial distribution of gaze is higher, and the fixations are highly random, there is a higher probability of a collision (see Figure 10).

Figure 10: Relationship between standardised Ht , Hs , and the probability of a collision for average levels of pupil diameter. When standardised Hs is lower than average (i.e., the spatial distribution of gaze is constrained), increasing standardised Ht appears to have minimal effects on the probability of a collision. However, when standardised Hs is over average (i.e., the spatial distribution of gaze is dispersed), an increase in the randomness of gaze transitions results in higher probability of a collision during a critical takeover.",Assessing_Data_Imbal-with-image-refs,"The chart shows that as drivers’ eye-movement transitions become more erratic, their risk of collision during a critical takeover climbs steadily—moving from a less random to an average level predicts about a 7-percentage-point increase, and risk accelerates even faster at higher levels. The shaded band indicates growing uncertainty around predictions at extreme levels of randomness. Other gaze patterns and pupil size (mental workload) are held steady at typical levels to isolate this effect. In practical terms, systems that track the unpredictability of gaze transitions could flag when a driver’s collision risk is rising. Timely alerts or assistance triggered by spikes in eye-movement randomness could help prevent accidents during critical control transfers.","The chart tracks how a standardized Hₜ score relates to the percentage-point rise in collision probability. When Hₜ is below or near its average (negative to zero values), the model shows almost no increase in collision chance and the shaded confidence band stays narrow, indicating stable, low risk. Once Hₜ climbs above average, the estimated collision probability accelerates sharply—nearly reaching a 20-point jump at one standard deviation above—and the confidence interval widens, reflecting greater uncertainty in high-stress conditions. This suggests that keeping Hₜ within normal bounds can effectively limit collision risk, while letting it rise leads to disproportionately larger and less predictable dangers."
AutoML__A_Tertiary_S-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/AutoML__A_Tertiary_S-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/AutoML__A_Tertiary_S-with-image-refs_artifacts/image_000000_ae26ea8dd64ded7a6250ce6fba5f2bdaf720e39d43b1ab35d66a11ca9d53e4b5.png,"## 3 Study Design

We conducted a tertiary review [21] to investigate the current state of AutoML research, synthesizing evidence from existing systematic and multivocal literature reviews, which are treated here as primary sources. To efficiently explore and identify appropriate methods and tools for practitioner use cases, we adopted a rapid review methodology [16], involving a domain practitioner (second author) to ensure relevance and accuracy. The complete research process, along with the number of papers retained at each stage, is illustrated in Fig. 1.

Fig. 1: Overview of the research process",AutoML__A_Tertiary_S-with-image-refs,"The chart maps a seven-step rapid tertiary review process, starting with a broad Google Scholar search supplemented by six seed papers to identify 106 initial records. An initial screening culled these to 33 studies, and citation snowballing contributed 34 additional sources. Final inclusion criteria narrowed the pool to 34 core papers, of which 32 provided sufficient data for extraction and synthesis. This staged combination of automated searches and manual citation tracking both maximizes coverage and enforces relevance. By transparently documenting each filtering milestone, the visualization demonstrates how a structured review can efficiently converge on high-quality evidence in fast-moving fields like AutoML.","The chart outlines a multi-stage filtering process that starts with 100 search hits plus six seed papers and narrows them down to 32 documents for detailed analysis. An initial screening picks 33 candidates, then reference-driven snowballing adds 34 more, showing that almost half of the final set emerges from manual follow-up rather than automated queries alone. Rigorous criteria then reduce 67 candidates to 32 included studies, highlighting a deliberate balance between breadth and focus. This combination of wide-reaching database searches and targeted reference tracking ensures comprehensive discovery without diluting quality."
Autonomous_identity--with-image-refs,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/Autonomous_identity--with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/Autonomous_identity--with-image-refs_artifacts/image_000003_9b1a95eac2345792bf0b7cfd791ff913dcf54d60e393c7e4351ca753cf25b112.png,"## Zero-Trust Architecture

Fig. 1. Overview of the trust-based access control model.

- f i (x) : Normalized function representing the observed metric for factor iii .
- n: Total number of factors considered.

Thresholds ( T ) can then be set to trigger responses:

If R &gt; T , then isolate or restrict identity.",Autonomous_identity--with-image-refs,"The diagram presents a zero-trust flow where every access request—whether from a user, device, or application—is routed through layered policy checks (policy-based, role-based, and attribute-based) to generate a normalized risk score. That score is continuously compared against a predefined threshold, automatically allowing, denying, or triggering additional authentication steps as needed. Continuous visibility and analytics span the entire sequence, catching subtle shifts in behavior before any sensitive data, services, or assets are exposed. By adapting decisions in real time based on these normalized metrics and policy rules, organizations can isolate anomalous identities and minimize blind spots. This dynamic, risk-driven approach ensures no identity is implicitly trusted and keeps critical resources under tight control.","The diagram illustrates how all access requests—whether initiated by people, devices, or applications—are routed into a unified verification system. It shows three layers of checks—rule-based policies, user roles (for example “manager” or “guest”), and contextual attributes (such as time of day or device security status)—working together to determine whether to allow, deny, or prompt for additional authentication. Approved requests then flow to the appropriate resources—data, services, applications, or hardware—while ineligible attempts are blocked or stepped-up. Underpinning the entire process is continuous visibility and analytics, giving security teams real-time insights into every access event. By combining multiple decision layers with ongoing monitoring, this approach adapts to changing conditions and keeps critical assets protected without unnecessary friction."
Bias_and_Fairness_in-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/Bias_and_Fairness_in-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/Bias_and_Fairness_in-with-image-refs_artifacts/image_000000_cf1a945d28634505596de274c2d81507c9fb08465b98eb5d486d2bab03064018.png,"## 2.2 Bias and Fairness in Med LLMs

While following responsible principles of development and deployment is important in any application of LLMs, Med LLMs must especially adhere to ethical constraints, including patient consent, privacy, compliance with regulations (such as HIPAA in the US and GDPR in Europe [35, 120]), as well as unbiased and fair treatment of individuals. Fig. 1 illustrates the various ethical aspects that should be addressed in Med LLMs.

Bias and fairness are key considerations in the deployment of Med LLMs, as these systems increasingly influence clinical decision-making and patient care [41, 50, 127]. Bias in the context of LLM refers to disparities in outcomes or representations, sourced from data and algorithms [45]. Fairness is related to parity in outcomes or opportunities

Fig. 1. Overview of Med LLM characteristics and ethical deployment considerations.

with respect to patient attributes, such as socioeconomic status and comorbidities [132]. This way, fairness is often considered as the absence of imparity between groups or individuals [30].

Med LLMs operate in safety-critical environments where incorrect or biased outputs can disproportionately impact underrepresented populations, exacerbating healthcare disparities [8, 104]. To tackle such challenges, Med LLMs require robust bias mitigation techniques, interdisciplinary collaboration with clinicians, explainability [56], and comprehensive validation frameworks to ensure their trustworthiness and alignment with evidence-based medical knowledge and guidelines. Accordingly, a dedicated review of studies related to this domain seems essential.",Bias_and_Fairness_in-with-image-refs,"The visualization breaks medical language models into three configuration choices—task specialization (general vs. domain-specific), deployment style (external vs. in-house), and transparency level (open vs. closed)—and pairs these with four ethical pillars: bias and fairness, explainability, infrastructure, and transparency. Each pillar is linked to concrete actions such as measuring outcome parity, providing clear AI justifications, involving clinicians and engineers collaboratively, and safeguarding patient privacy with regulatory compliance. This alignment shows that selecting the right model type must go hand in hand with bias detection, fairness metrics, and clear records of decision logic to protect underrepresented patient groups. Embedding legal requirements and clinical expertise into the technical infrastructure ensures accuracy and respects consent and data protection laws. Together, these elements form a practical roadmap for healthcare teams to deploy AI responsibly, monitor equitable outcomes, and maintain trust throughout the development process.","The chart outlines three dimensions to classify a medical language model: its level of task focus (broad versus specialized), its transparency (open versus closed design), and how it’s deployed (hosted in-house or by an external partner). It then presents a blueprint for responsible deployment by centering on infrastructure readiness, transparent operations, explainability of outputs, and ongoing bias and fairness monitoring. Key actions include safeguarding patient privacy, navigating regulatory requirements, and verifying clinical accuracy before use. The visualization emphasizes building systems that can justify their recommendations, detect and correct bias, and measure fairness metrics to ensure equitable patient outcomes. Finally, it underlines the need for interdisciplinary collaboration and resource planning to support safe and trustworthy model adoption."
Bioenergy_with_carbo-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/Bioenergy_with_carbo-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/Bioenergy_with_carbo-with-image-refs_artifacts/image_000002_64f61a1d6bc4bfc9d63ca2e4436f796a2fb242d6e20bcf6777d7f0ca15c3f6eb.png,"### 3.1. Emission reduction measures

The resulting trajectories of emission reduction for each decarbonization scenario (S1-S4) are shown in **Figure 4** . By 2100, S1 resulted in 572 Mt CO2 reduction while S2, S3, and S4 lead to 613, 719, and 830 Mt CO2 reduction from the baseline scenario (BL) of 683 Mt CO2. Across S1-S4, most of emission reduction are generated from increasing renewable energy (RE). About 536-548 Mt of CO2 emissions can be reduced in 2100 by expanding renewable power capacity. Meanwhile, emission reduction from BECCS varies across different decarbonization scenarios. BECCS contribution is not significantly required to achieve less stringent emission targets (S1 and S2). There is no BECCS required under S1 and insignificant amount of BECCS is required under S2 (5 Mt CO2, 0.8% of total emission reduction in 2100). However, BECCS contribution increased significantly as more stringent targets are considered (S3 and S4). Under S3 and S4, BECCS contribution increased to 105 and 265 Mt CO2 (15% and 32% total emission reduction in 2100). Emission reduction from BECCS comprised of both avoided fossil emission (BECCS-ZERO) and carbon dioxide removal (BECCS-CDR). Besides transitioning to non-fossil, reduction of fossil power emission also came from shifting to lower emitting fuels, i.e., shifting from coal to natural gas input for power generation. In addition, emission reduction via nuclear power is only selected under S3 and S4—with S3 resulting in larger nuclear capacity than S4.

Figure 4 | Development of emission reduction contribution (in Mt CO2 y-1) by technologies from 2020 to 2100 in different decarbonization scenarios (S1-S4).",Bioenergy_with_carbo-with-image-refs,"The chart shows that in all scenarios, expanding renewable power delivers the vast majority of emission cuts by mid-century and beyond, reducing over 500 Mt CO2 per year by 2100. For moderate targets (S1 and S2), renewables supplemented by shifting from coal to gas achieve most reductions without needing carbon capture, while deeper cuts under S3 require adding nuclear capacity and small amounts of bioenergy with carbon capture and storage (BECCS) for both avoided emissions and removals. In the most ambitious pathway (S4), large-scale BECCS becomes the dominant contributor after renewables, pushing total reductions above baseline levels and resulting in net negative emissions by century’s end. This implies that meeting stringent climate goals will depend not only on wind and solar but also on robust investment in biomass-based carbon removal. Policymakers should therefore pair aggressive clean power deployment with scalable BECCS infrastructure and consider targeted nuclear support if biomass availability or BECCS build-out faces limits.","The chart shows that all four pathways rely heavily on a rapid build-out of wind and solar power, yet the two least ambitious cases only bend emissions downward temporarily before they creep back up by century’s end. In contrast, the deeper-cutting scenarios layer in growing amounts of bioenergy coupled with carbon capture, driving net CO₂ flows below zero after mid-century. Small contributions from new nuclear plants and cleaner industrial processes help a bit, but by themselves they fall short of delivering a true turnaround. The clear takeaway is that while renewables are indispensable for cutting emissions, achieving—and sustaining—net-negative CO₂ will also require large-scale carbon removal technologies."
Continuous_Real-Time-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/Continuous_Real-Time-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/Continuous_Real-Time-with-image-refs_artifacts/image_000006_b24a088c19a49172abf751b13873162cd246204008534a88f523c567c0db7484.png,"## 3.1 Gesture decoding

The implemented neural network architecture proved to perform well in decoding hand gestures. We visualized the decoding performance using a circular plot (Figure 6) which showed predicted direction in polar coordinates. In this plot, 12 colors represented 12 target locations and the dotted position represented the cursor position. It can be noticed that our network architecture decoded the direction well, and errors occurred when the decoder attributed the direction to a neighboring sector. For example, the decoder confused 0 and 150 degrees for participant RK07. Despite these errors, the measurements of MSE showed that the trajectories were decoded well. The MSE for the trained model was compared to the MSE after a random shuffle of the decoded trajectories, and the difference was 0 293 . ± 0 03 for 100,000 shuffles. .

Figure 6: The scatter plots showing how direction was decoded from the OMG data with the multilayer perceptron. Each dot corresponds to one sample of the OMG data. Polar coordinates represent the decoded cosine and sine values of the direction. Each dot's color corresponds to the instructed direction, and its distance from the center represents the decoding result. The box plot (bottom, right) quantifies the prediction of click gesture.

The box plots shown in Figure 7 depict the predictions of clicks. In the box plot pairs, the box shown on the right corresponds to the probability of click detection for the fist clench gesture, and the box shown on the left corresponds to the probability for any other gesture. It is clearly seen that the model distinguished fist clinch from the other gestures. To avoid false detection of the click, a click threshold value was set for each subject individually.

Figure 7: Accuracy of click decoding represented as box plots. The box on the right corresponds to the decoding of fist clench. The box on the left corresponds to false detection of click from the gestures different from fist clench.",Continuous_Real-Time-with-image-refs,"The box plots compare how often the model assigns high click-probability scores to actual fist-clench gestures versus all other hand movements for one participant. Genuine clicks have a median probability around 0.75 with most values above 0.6, while non-click gestures stay clustered near zero, showing very few false positives. This clear gap demonstrates that the network can reliably distinguish click from non-click gestures. Because the click probabilities still vary across trials, setting a tailored threshold for each subject helps minimize both missed clicks and false alarms. Overall, these results confirm that the multilayer perceptron achieves strong, actionable click-decoding performance.","The chart displays two box plots comparing prediction scores for non-click versus click events in subject RK01. Non-click scores are almost all piled below 0.05, while click scores sit much higher, with half of them falling between about 0.15 and 0.8 and a typical score near 0.78. This clear separation suggests that a cutoff around 0.2 would identify most clicks while excluding nearly all non-clicks. A handful of unusual readings—a click score near zero and a few non-click scores in the mid-range—reveal occasional misclassifications that could be fixed with more fine-tuning."
Decoding_community_p-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/Decoding_community_p-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/Decoding_community_p-with-image-refs_artifacts/image_000005_4322d483f7304b335a3949f511b75c481106d8c625e9574153d98932cbd69b4e.png,"## 4.1.4 Named entity recognition

Using the GLiNER model, we identified named entities (Person, Location, Event, Organization, Issues) in both local and national news. Figure 4 illustrates the distribution of top named entities for each category. Local news focuses on local personalities and places, for example featuring names like Lucas Vuitel or cities such as Neuchâtel , La Chaux-de-Fonds , and the canton of Valais . National news highlights more widely recognized figures and broader locations, such as Donald Trump or Suisse , Genève , États-Unis .

538

539

540

541

542

543

544

545

546

547

548

549

550

(a) Local News - Person

(c) Local News - Location

(e) Local News - Organization

(g) Local News - Event

- (i) Local News - Issue

(j)

National News - Issue

(b) National News - Person

(d) National News - Location

(f) National News - Organization

(h) National News - Event

Fig 4. Comparison of Top NERs in local news vs national news across the 5 Categories.

The prominence of local personalities, places, and organizations in local news, contrasted with the broader and more nationally or internationally recognized entities in national news, underscores the distinct audience orientation of each media tier. Local news' focus on hyper-local entities reinforces its role in constructing and maintaining community identity, while national news' broader scope reflects its function as a mediator of national discourse. This finding supports the theoretical expectation that proximity is not just geographic, but also discursive and symbolic [59].",Decoding_community_p-with-image-refs,"The chart highlights a pronounced bias in local reporting towards a few core places, with Neuchâtel dominating location mentions, followed by the canton of Valais, Switzerland at large, and La Chaux-de-Fonds. Beyond these top four, attention falls off sharply through towns like Sion, Martigny and Lausanne, confirming that local news frames its stories around immediately familiar communities. This tight clustering of coverage reinforces how proximity drives not just geographic focus but also a shared local identity. For media planners and civic leaders, these insights suggest prioritizing content and engagement efforts in these key hotspots while also seeking ways to bring smaller municipalities into the spotlight.","The chart reveals that local news coverage is heavily skewed toward a few headline spots, with Neuchâtel cited nearly 20,000 times and the Valais region and Switzerland itself each appearing around 16,000–18,000 times. Beyond these leaders, mentions plunge sharply, leaving smaller municipalities and border areas with only a fraction of media attention. This uneven distribution suggests a clear opportunity for news outlets to diversify their coverage and elevate underrepresented communities. Proactively featuring stories from these quieter locations could strengthen audience engagement and build a more balanced regional narrative."
Delivering_Tactile_S-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/Delivering_Tactile_S-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/Delivering_Tactile_S-with-image-refs_artifacts/image_000004_af1c5c4d0d0a8e7ba3b26f12779414b65cea2169a4d60b1b4284d58a9bd559fd.png,"## Results

Figure  5.  Onset  lag  and  asynchrony  for  trimodal  stimuli.  a) Onset  lag  of  visual, auditory and haptic stimuli where 0 ms represents the stimulus onset trigger (websocket signal). b) Asynchrony of auditory and vibration stimuli compared to the visual stimulus onset (set to 0 ms). Dashed lines represent means of the distributions, in subplot b the solid line represents perfect synchrony.

Table 2. Stimulus Onset (Lag). Time in ms of measures from photodiode (light), auditory input (sound) and microphone input (vibration) measured via the RTbox. Timings are relative to expected stimulus onset of 0ms. Measurements are based on 987 measured events, mean and standard deviation of onset lags are shown, percentages indicate raw data percentiles.

| Event     |   mean |   std dev |   min |   25% |   50% |   75% |    max |
|-----------|--------|-----------|-------|-------|-------|-------|--------|
| light     |  35.91 |      4.28 | 17.83 | 33.16 | 35.79 | 38.5  |  51.49 |
| sound     |  40.91 |      3.44 | 23.26 | 38.82 | 41.03 | 43.19 |  52.46 |
| vibration |  78.64 |      5.05 | 56.08 | 75.54 | 78.5  | 81.6  | 111.65 |

Table 3. Asynchrony. Time in ms of sound and vibration measurements relative to the visual stimulus. Measurements are based on 987 measured events. Percentages indicate raw data percentiles.

| Event     |   mean |   std dev |   min |   25% |   50% |   75% |   max |
|-----------|--------|-----------|-------|-------|-------|-------|-------|
| sound     |   5    |      3.24 | -8.4  |  2.91 |  5.33 |  7.3  | 14.46 |
| vibration |  42.73 |      5.41 | 24.87 | 39.13 | 42.85 | 46.02 | 82.3  |

Relative  to  the  websocket  trigger  signal,  the  visual  stimulus  was  presented  with  a  lag  of 35.34 ms and a precision of 7.35 ms (Fig 5a). Similarly, the auditory stimulus was presented with  a  lag  of  38.72  ms  and  a  precision  of  4.49  ms.  Tactile  stimuli  were  presented  after  a longer  lag  of  84.20  ms  at  a  precision  of  6.07  ms.  Of  note,  the  larger  lag  in  the  tactile stimulation  may  be  attributable  to  the  ramp  period  in  vibration,  and  the  microphone  not detecting  vibration  stimuli  until  a  certain  amplitude  was  surpassed.  Since  the  microphone used  to  detect  the  vibrations  was  also  recording  during  data  collection,  we  were  able  to inspect  the  waveform  of  the  recorded  vibrations  (Figure 6a). On average, an initial ramp period before the triggering of the RT Box in these waveforms lasted 30.46 ms (SD: 5.28 ms; Figure 6b). We conducted a simple linear regression to examine whether this ramp period predicts the asynchrony from the visual onset (Figure 6c). The overall model was significant, F(1, 978) = 135.3, p &lt; .001, and explained 12.2% of the variance in asynchrony (R² = .122). The slope for ramp time was positive and significant (b 1 = 0.342, SE = 0.029, t(978) = 11.63, p &lt; .001, 95% CI [0.284, 0.400]) and so was the intercept (b0 = 32.23, SE = 0.91, t(978) = 35.46,  p  &lt;  .001,  95%  CI  [30.45,  34.01]).  This  intercept  means  that  the  model  predicts  an asynchrony of 32.23 ms even when the ramp period is at 0 ms, meaning there still exists an approximate 30 ms asynchrony when accounting for ramping of the vibration stimulus. This ramping  is  an  inherent  feature  of  vibration  stimulus  presentation,  and  is  therefore  an important factor for researchers to consider when designing experiments involving vibration. For example, one strategy might be to cue the vibration slightly earlier to ensure that its peak amplitude aligns more precisely with a visual stimulus.

b)

c)

0.10

0.05

0.00

10

Mean = 30.51 ms

20

30

40

50

60

R2 = 0.12, p &lt; .001

20

30

40

50

Marker Lag (ms)

Marker Lag (ms)

Figure 6. Analysis  of  ramp  onset  for  vibration  stimuli. a) Example  waveform illustrating the recorded vibration stimulus (blue), with onsets identified by a convolutional neural network (red) and event markers recorded by the RT Box (black). The marker lag (grey shaded area) between these two onset measurements quantifies the marker's timing

3

2

60

50

40

30

accuracy.  The  envelope  of  the  waveform,  calculated  using  root  mean  square  (RMS, yellow),  filters  out  spurious low-amplitude onsets to improve onset detection reliability. b) Distribution of marker  lag  values,  indicating  an  average  lag  of  30.51  ms  from  the neural-network-defined  onset. c) Scatterplot  demonstrating  a  positive  correlation  (R²  = 0.12, p &lt; .001) between marker lag and visual-vibration asynchrony, showing that marker timing variability partly explains differences in multimodal synchrony.

When  conducting  studies  where  response  time  relative  to  a  stimulus  is  the  dependent variable.  A  lag  in  stimulus  onset  can  often  be  accounted  for  by  subtracting a constant lag value from response times (where response times are calculated relative to a trigger signal), a  low  precision  of  onsets  makes this  calculation  less  reliable.  Browser  precision  is  worse compared  to  lab-based  alternatives  where  visual  and  auditory  onset  precision  have previously  been  recorded  at  0.35  ms  and  0.96  ms,  respectively  (Bridges  et  al.,  2020). Nevertheless,  even  in  browsers  this  appears  to  be a relatively good degree of precision. Standard deviations under 10 ms for both lags and asynchronies create a relatively stable experimental environment, as long as the expected effect size is large enough to stand out from the noise in signal onset. To assess how onset variability affects statistical sensitivity in typical  RT  studies,  we  used  our  Monte Carlo framework to determine the effect size (Δμ) required for 80 % power. When no onset noise was added, a drift rate increment of Δμ = -0.0421 (≈ 6.6 ms in RT space) was sufficient to reach 80 % power. Introducing 10 ms of onset variability  raised  the  required  effect  size  only  marginally  to  Δμ  =  0.0424  (≈ 6.7 ms). These findings indicate that the natural trial -to trial -variability in decision times overwhelmingly dominates the impact of a 10 ms uncertainty in stimulus onset.

Additionally, we can interpret the timing as the asynchrony between the stimuli. In this case we can ignore the websocket trigger and set our latencies relative to the onset of the visual stimulus  (captured  using  a  photodiode).  Previous  browser  based  audiovisual  synchrony recorded on a Windows 10 device using Chrome and PsychoPy was reported as 65.32 ms with  a  precision  of  3.01  ms  (Bridges  et  al.,  2020).  That  is,  auditory  stimuli  were  detected approximately 65 ms after the visual stimuli. Interestingly, the audiovisual synchrony on the mobile device was substantially better at 3.37 ms, but precision was worse at 6.49 ms (Fig 1b). The vibration stimulus lagged the visual onset by a larger 48.85 ms with a precision of 7.44 ms.",Delivering_Tactile_S-with-image-refs,"The chart shows that browser-based visual stimuli appear about 36 ms after the trigger with under 10 ms variability, auditory stimuli at roughly 41 ms, and vibrations at around 79 ms. When aligned to the visual event, sounds lag by about 5 ms on average, while vibrations lag by close to 43 ms because of their initial ramp-up. This built-in ramp adds roughly 30 ms of extra delay, so sending vibration cues earlier can help align them more precisely with visual targets. Overall, these results suggest that while browser-delivered visual and auditory signals are precise enough for most response-time experiments, vibration stimuli need timing adjustments to stay in sync across senses.","The chart reveals that light signals actually begin about 30 ms after the command, sound signals around 40 ms, and vibration signals around 80 ms, with each modality showing a tight cluster of trials around those means. When these are nominally presented together, sound ends up roughly 10 ms behind light and vibration nearly 50 ms behind on average. Because the distributions are narrow, these offsets are highly reliable and will consistently introduce perceivable asynchronies. Ignoring them can lead to unintended timing errors in experiments or multimedia experiences. To achieve true synchrony, sound should be launched about 10 ms earlier and vibration about 50 ms earlier relative to light."
Implementing_Reliabi-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/Implementing_Reliabi-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/Implementing_Reliabi-with-image-refs_artifacts/image_000001_d5ec5773cf3ec85fd122d800e6addd2c80e8565e234d942d533b549b831ce84f.png,"## 2 Evolution of Maintenance Strategies

Since the mid-1930s, the evolution of maintenance can be described in terms of three generations of development, as illustrated chronologically in Figure 2.

Figure 2: Historical evolution of maintenance strategies.",Implementing_Reliabi-with-image-refs,"The chart traces maintenance from a purely reactive “fix-it-when-it-breaks” stance in the mid-1930s through a mid-century phase of scheduled shutdowns powered by bulky mainframes, to today’s predictive, reliability-centered model.  As computers shrank and grew more powerful, organizations moved from simply planning routine overhauls to monitoring equipment health in real time and analyzing failure modes.  By embedding reliability and maintainability into the original design and leveraging intelligent systems alongside cross-functional teams, businesses can anticipate issues before they occur.  This progression shows that investing in condition monitoring and design-for-reliability not only cuts unplanned downtime but also optimizes maintenance resources and drives overall operational efficiency.","The chart outlines how maintenance approaches have transformed alongside computing breakthroughs. In the 1940s, teams only fixed equipment after it broke, but by the 1960s they scheduled shutdowns and used planning systems despite relying on large, slow computers. From the 1980s onward, organizations adopted continuous equipment health monitoring, careful analysis of potential failure points, and designs that prioritized reliability, all enabled by smaller, faster computers and collaborative tools. This progression highlights how moving from reactive repairs to data-driven maintenance strategies can drastically improve performance. Today, businesses can boost uptime and cut costs by embracing real-time analytics, design-for-reliability principles and cross-functional teamwork."
Incorporating_Partic-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/Incorporating_Partic-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/Incorporating_Partic-with-image-refs_artifacts/image_000000_4d72b1ae76ef84fb8108c4726da438cc7fd0a5eec5697fc3d9a8fdb99e8ce3f9.png,"## Data collection

We performed semi-structured, qualitative interviews with the DMR participants to understand what outcomes they experienced before introducing them to any frameworks (Figure 2). The interviews consisted of three parts, with each subsequent part being less inductive and focusing more on the frameworks: (1) opening question and responses; (2) framework introduction and specifying outcomes; and (3) mapping outcomes on the framework. An interview guide was created to structure questions or reminders according to the interview schedule (Supplemental file 1: Interview Guide). This three-stage approach to the interviews helped to collect a variety of comments on the outcomes and frameworks from the interviewees, but was treated as one interview during analysis.

Figure 2 : Visual overview of the research methods, showing how quotes from the three parts of the interview were coded in two phases, resulting in the final codebook.

All 105 participants in the 2021 edition of the DMR project were invited by email to take part in these interviews. A few interviewees (n=2) replied via email but the majority (n=14) was recruited by calling a randomised list of the remaining participants. We stopped recruitment at a feasible number of interviewees given our resources. The first interview was held with a participant who was a professional in CS and water management at the TU Delft, which was used as a test interview. The interview duration ranged from 25 to 75 minutes, depending on the conciseness of interviewees, how many outcomes they identified, and their willingness to enter into a longer discussion of their experiences. All interviews were conducted by the first author in Dutch, except for one interview with a participant who could understand Dutch but was more comfortable in English.

After an introduction to cover data-ethics best practices and establish rapport, part 1 started with the open question: 'Could you tell me in what ways participating in DMR has been meaningful for you, in terms of the outcomes or results that you experienced from the project?'. Probing questions were asked to help interviewees elaborate on their answers, and reminders of key aspects of the project (such as the measuring task, or information received via newsletters and webinars) were only given by the interviewer after the first top-of-mind answers were noted.

For part 2 of the interviews, a physical two-sided board was created that displayed the frameworks used in this study (Supplemental file 2: Framework board). It was used to introduce the interviewees to the concept of evaluation frameworks by explaining the outcomes and impacts one by one. As soon as the interviewees were familiar with the frameworks, they were asked whether they experienced any of these outcomes. After any clarification and structuring of the discussed outcomes, the final list of outcomes was agreed upon before continuing to part 3.

In part 3, the interviewees were involved in the process of classification of the outcomes. The interviewees were asked to score the outcomes that they experienced within the frameworks presented in part 2, which provided an opportunity to also share their view about the frameworks themselves. We explored this method to map the alignment of the outcomes to the frameworks, but did not design more formal methods for quantitative analysis due to resource constraints. Quotes from all three parts of the interview were used in the analysis, but the scores of part 3 were discarded.",Incorporating_Partic-with-image-refs,"The chart illustrates a rigorous workflow for extracting meaningful outcomes from semi-structured interviews with project participants. Three interview stages—open questioning, framework introduction, and alignment discussions—generate verbatim quotes that feed into an inductive coding phase, creating 140 initial codes. A targeted deductive pass then refines these into 27 detailed codes and, after inter-rater reliability checks, a streamlined codebook of ten core outcomes, with numerical scores deliberately set aside to emphasize qualitative depth. Iterative transcript reviews and supportive literature research underpin these ten themes, ensuring they accurately reflect participant experiences before shaping the study’s conclusions.","The chart outlines a step-by-step journey from interview recordings to a concise set of research findings. Interviews start with open questions to let participants describe outcomes freely, then introduce a guiding framework to confirm and align those insights, deliberately setting aside numerical scores. Transcripts are reviewed to identify patterns and generate an initial wide range of codes, which are then refined through a structured review and team reliability checks into a smaller set of main and sub-themes. This process is repeated until ten core themes remain in a final codebook, ensuring each theme is both rooted in participant words and consistent across reviewers. Finally, a targeted literature review helps situate these key outcomes within existing knowledge, linking real experiences to broader frameworks."
Interpretability_req-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/Interpretability_req-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/Interpretability_req-with-image-refs_artifacts/image_000000_6d98e21efcbfcb6784e049b55d15e9457519cea07be1b53360ccea15ab32cf0d.png,"## 2 Current state of theory and experiment

Without loss of generality, we discuss examples of interpretability work on input attribution and component attribution (Figure 1). For each, we cover complexity-theoretic and experimental efforts.

Figure 1: Interpretability research illustrated through two types of questions it attempts to answer: which input features and which components of the model explain various tasks of interest?",Interpretability_req-with-image-refs,"The diagram highlights two distinct paths for interpreting neural models: tracing outputs back to input features versus tracing them through internal components. Input-level methods shine a light on which tokens or pixels most influence a given task, while component attribution zeroes in on the hidden neurons and connections that form the model’s internal “circuits.” By applying both approaches across multiple tasks (illustrated by color-coded Task A and Task B), the visualization shows that researchers can tailor their analysis to the level of detail they need. This split underscores the practical advice that choosing between feature-based and circuit-based interpretations should be driven by the specific question at hand.","The visualization compares two explainability strategies for neural networks: one that probes internal circuit components and one that attributes predictions directly to input features. On the left, a circuit query feeds into a component attribution method to highlight how particular pathways and units drive the computed output, using color-coded traces for distinct tasks. On the right, an input query with an attribution method produces a saliency map that pinpoints which raw inputs most influence the network’s decision. The central dashed link underscores that the component-focused technique can be adapted to multiple tasks, revealing task-specific subnetwork contributions that might go unnoticed through input saliency alone. Together, these approaches offer complementary insights—mapping mechanistic underpinnings and surface-level drivers—to guide more targeted model refinement and debugging."
JusticeNetBD__A_Retr-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/JusticeNetBD__A_Retr-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/JusticeNetBD__A_Retr-with-image-refs_artifacts/image_000004_eec45bb42b8644b5e6e07b2d45ff823fb2f708f9d53def55b2e9e1fdfb9ab3f8.png,"## Model Performance Comparison

Figure 5: Model Performance Comparison

General-purpose state-of-the-art large language models such as DeepSeek V3, Gemini,2.5 Flash, and ChatGPT-4o underperform in this domain due to their lack of access to domain-specific legal statutes relevant to Bangladesh, as shown in Figure 5. As a result, these models tend to produce generic legal advice or reference incorrect statutory sections, which adversely affects lexical overlap metrics (e.g., ROUGE-L) and, to a lesser extent, semantic similarity measures such as BERTScore. Additionally, their responses often include hedging phrases (e.g., 'laws may vary...') rather than directly citing precise punitive clauses.

Incorporating exact statutory text segments into the prompt allows the RAG model to:

- Mitigate hallucination : ensuring that every generated claim can be traced to retrieved source text.
- Enhance lexical fidelity : resulting in improved ROUGE-L scores due to accurate legal phrasing.
- Preserve linguistic fluency : maintaining BERTScore values comparable to human-generated responses.

However, despite strong retrieval performance, approximately 10% of queries fail to retrieve the gold-standard statutory chunk. Future work could explore approaches such as query rewriting or hybrid sparse-dense retrieval methods to further improve recall towards 1.0. In the context of providing high-stakes legal advice, the grounded RAG system substantially outperforms closed-book state-of-the-art LLMs. The pipeline not only enhances factual accuracy but also offers transparent evidence retrieval, thereby providing a safer and more reliable tool for supporting women's rights initiatives in Bangladesh.",JusticeNetBD__A_Retr-with-image-refs,"The chart shows that the grounded retrieval model tailored to Bangladeshi statutes roughly doubles lexical overlap scores (ROUGE-L) compared to general-purpose LLMs, rising to about 0.46 versus 0.21–0.24 for DeepSeek V3, Gemini 2.5 Flash, and ChatGPT-4o. It also edges out those models on semantic similarity (BERTScore), reaching around 0.90 versus 0.85–0.86, which means precise statutory citations boost both factual accuracy and linguistic fluency. This gap makes it clear that injecting exact legal text into prompts effectively prevents hallucinations and ensures claims can be traced back to authoritative sources. Anyone building high-stakes legal advice tools should therefore integrate domain-specific retrieval of statutes to deliver more accurate, transparent, and reliable outputs.","The chart compares four text-generation models on two fronts: how closely they match the exact wording of a reference (ROUGE-L) and how well they capture its meaning (BERTScore). JusticeNetBD clearly outperforms the others, roughly doubling their ROUGE-L scores (about 0.46 versus 0.21–0.24) while also leading in semantic similarity (around 0.90 versus 0.85–0.86). DeepSeek V3, Gemini 2.5 Flash and ChatGPT 4o Turbo form a tight cluster with moderate semantic fidelity but low lexical overlap, suggesting they grasp meaning but struggle to reproduce phrasing. This contrast reveals a common trade-off between capturing nuance and preserving exact wording. Teams seeking both precise wording and deep semantic alignment should investigate the techniques behind JusticeNetBD’s balanced performance."
Learning_to_operate_-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/Learning_to_operate_-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/Learning_to_operate_-with-image-refs_artifacts/image_000003_72900c0e39b599e56f813b7bf70856e716cd54602a728de3e78304f3a55a8229.png,"## Statistical tests

To  statistically  validate  our  results,  we  analyzed  data  from  healthy  participants  and  the amputee separately. For each healthy subject, we compared measurements from stimulation and non-stimulation trials. Normality of each dataset was assessed using the Shapiro-Wilk test. If normality was  confirmed, we  applied an independent  t-test; otherwise, the non-parametric  Mann-Whitney  test  was  used.  False discovery rate (FDR) correction was applied within each subject, separately for behavioral and visuomotor measurements.

A  similar  procedure  was  followed  for  the  amputee.  Measurements  from  stimulation  and non-stimulation  trials  were  compared across three experimental sessions. FDR correction was  applied  across  all  comparisons  from  the  three  days.  Comparisons  with  a  corrected p -value below 0.1 were considered statistically significant.

Figure  3.  Different  phases  of  the  recordings.  Red  line represents normalized hall-effect sensor data, while the green line shows its gradient. When the wrist is opened, the sensor's data is equal to 1, if closed - then equal to 0. Threshold equal to ± 0.025 for signal velocity allowed to acquire best accuracy for correct trial separation into phases. It allows us to get information on the position of positive and negative peaks of the signal",Learning_to_operate_-with-image-refs,"The chart demonstrates that by combining a normalized hall-effect sensor signal of hand aperture with its time derivative, it is possible to automatically and accurately segment a pick-and-place action into five distinct phases: reach, grasp, transport, release and button reach. During reach, the hand remains fully open (signal plateau at 1) before a rapid negative velocity peak marks the onset of grasp, followed by a stable closed state in transport and a positive velocity peak indicating release. Applying fixed velocity thresholds (±0.025) consistently identifies these transitions without manual intervention, ensuring precise temporal alignment across trials. This robust phase detection underpins reliable statistical comparisons of stimulation versus non-stimulation conditions and could readily be adapted in other sensor-driven movement analyses to improve reproducibility and reduce labeling bias.","The chart segments a single button-press task into five clear stages—Reach, Grasp, Transport, Release, and Button reach—by plotting hand‐aperture (red), closing/opening velocity (green), and button‐press events (blue) over time. During Reach the hand stays wide open with almost zero velocity, then in Grasp a steep drop in aperture and a sharp negative velocity peak mark the closing action. The Transport phase holds the gripper fully closed until the Release trigger produces a rapid positive velocity spike and reopening of the hand. Finally, the Button reach phase shows the hand back at its open position just before a brief button‐press pulse closes the sequence. Plotted this way, the data provide a reliable fingerprint for automatically identifying each subtask in a pick-and-press motion."
Machine_Learning-Bas-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/Machine_Learning-Bas-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/Machine_Learning-Bas-with-image-refs_artifacts/image_000000_a0cb01ae387bc9e5bae1be9606fddc51090d254d813ff395050ca959da8bd69a.png,"## 4.2 Random Forest

- Accuracy: 98.3%
- Precision: 99.1%
- Recall: 97.6%
- ROC-AUC: 0.995

Figure 1: Top features by importance from the Random Forest classifier. Variance, entropy, and energy were among the strongest predictors.",Machine_Learning-Bas-with-image-refs,"The chart reveals that a small group of EEG-derived metrics—particularly measures of variance, entropy, and energy in channels numbered 159, 44, and 158—dominate the Random Forest’s decision process. These top three features alone contribute a disproportionate share of the model’s high accuracy (98.3%) and near-perfect ROC-AUC (0.995). Narrowing the model to these most influential predictors could simplify data acquisition and feature computation, yielding faster, more cost-effective analyses without sacrificing precision or recall. It also pinpoints the specific neural signal characteristics most critical for differentiating mental states, offering valuable direction for future experimental designs. By focusing on these key channels and metrics, teams can optimize both hardware setups and algorithmic efficiency in real-world applications.","The chart shows that the Random Forest model relies most heavily on channel EEG_159, with an importance score around 0.02, making it roughly twice as influential as the lowest-ranked channel in this top ten. Channels EEG_44 and EEG_158 also contribute significantly, together forming the model’s core decision-making signals. In contrast, channels like EEG_12, EEG_71, and EEG_127 have much smaller importances and could be candidates for removal or simplified treatment. Prioritizing the high-impact electrodes (159, 44, 158) can streamline feature selection, reduce dimensionality, and maintain or even boost predictive accuracy. Focusing analysis and signal processing efforts on these key channels will help optimize both performance and interpretability."
Beyond_the_Exponenti-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/life_articles/processed_articles/Beyond_the_Exponenti-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/life_articles/processed_articles/Beyond_the_Exponenti-with-image-refs_artifacts/image_000003_db7b6f050cd07cbb9fa07a51821340c2a200c115cc680ee838c9e4986e8b2911.png,"## 5 General Discussion

These results suggest that the rational function family (hyperbolic and its generalizations) is not only descriptively valid but also normatively grounded in physical principles. The brain, as an energy-constrained prediction machine, naturally discounts outcomes that are temporally or informationally distant. This bridges psychological, economic, and thermodynamic models of behavior, offering a powerful interdisciplinary synthesis (Callender, 2021; Friston, 2010).

Thermodynamic Metaphor for Delay Discounting

Figure 3: Thermodynamic metaphor for delay discounting: entropy cost accumulates over time while value decays.

General Forms of Discounting

Figure 4: General discounting curves across domains: time, probability, effort, and cognitive cost.",Beyond_the_Exponenti-with-image-refs,"The chart shows that as any cost rises—be it waiting time, uncertainty, or required effort—the subjective value of a reward falls, but at different rates: delays erode value most slowly, uncertainty more steeply, and effort most sharply. This pattern suggests we are relatively patient with waiting but highly averse to expending effort or facing risk, consistent with a brain that balances reward against energy and information costs. By fitting all three as curved, hyperbolic declines, the visualization reveals a common thermodynamic principle: accumulating entropy or cognitive–physical load diminishes perceived value. Slow entropy buildup under time delays yields a gentle value drop, while the rapid energy demands of effort produce a steep decline. Altogether, these curves unify psychological discounting across domains into a single, energy-grounded framework.","The chart shows that the perceived worth of a reward declines as its associated cost in time, uncertainty, or effort increases. The top curve (temporal) tells us that people devalue delayed rewards but still hold onto a fair amount of their initial appeal. The middle curve (probabilistic) reveals that introducing uncertainty causes a steeper drop in perceived value, so gambles feel less attractive than simply waiting. The bottom curve (effort) makes it clear that even modest work requirements lead to the sharpest loss of appeal, as people tend to avoid tasks that demand physical or mental exertion. This suggests that reducing effort barriers will boost motivation more effectively than minimizing delays or uncertainty."
Oral_Language_Outcom-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/life_articles/processed_articles/Oral_Language_Outcom-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/life_articles/processed_articles/Oral_Language_Outcom-with-image-refs_artifacts/image_000000_9d456dd074f3e6056dcaed5efa793175e329021217b820cdc124457761fa03b9.png,"**Language Macrostructure**

Table 3 and Figure 1 show the average language macrostructure outcomes for the retelling and personal narratives during baseline, intervention, and maintenance phases in English and Spanish interventions. Additionally, we provide a detailed summary table of the retelling result indicators in Appendix B.

| Table 3.                                             | Table 3.                                             | Table 3.                                             | Table 3.                                             |
|------------------------------------------------------|------------------------------------------------------|------------------------------------------------------|------------------------------------------------------|
| Average macrostructure outcomes for all participants | Average macrostructure outcomes for all participants | Average macrostructure outcomes for all participants | Average macrostructure outcomes for all participants |
| Variables                                            | Baseline                                             | Intervention                                         | Maintenance                                          |
| Participant 1                                        |                                                      |                                                      |                                                      |
| English                                              |                                                      |                                                      |                                                      |
| Total retelling                                      | 5.5 (3)                                              | 13.83 (0.40)                                         | 13.5 (2.12)                                          |
| Story grammar                                        | 4.25 (0.50)                                          | 8.50 (0.83)                                          | 8 (0)                                                |
| Language complexity                                  | 0 (0)                                                | 0.33 (0.51)                                          | 1.5 (0.70)                                           |
| Episode                                              | 1.25 (2.50)                                          | 5 (0)                                                | 4 (1.41)                                             |
| Total personal story                                 | 0.75 (0.95)                                          | NA                                                   | 3 (2.12)                                             |
| Story grammar                                        | 0.75 (0.95)                                          | NA                                                   | 3 (2.12)                                             |
| Language complexity                                  | 0 (0)                                                | NA                                                   | 0 (0)                                                |
| Episode                                              | 0 (0)                                                | NA                                                   | 0 (0)                                                |
| Spanish                                              |                                                      |                                                      |                                                      |
| Total retelling                                      | 14 (0.81)                                            | 14 (2.75)                                            | 15.5 (0.70)                                          |
| Story grammar                                        | 8.75 (0.95)                                          | 7.16 (1.72)                                          | 8.5 (0.70)                                           |
| Language complexity                                  | 0.25 (0.50)                                          | 1.83 (1.69)                                          | 2 (0)                                                |
| Episode                                              | 5 (0)                                                | 5 (0)                                                | 5 (0)                                                |
| Total personal story                                 | 7.25 (2.97)                                          | NA                                                   | 11 (7.77)                                            |
| Story grammar                                        | 4.25 (1.25)                                          | NA                                                   | 6 (4.24)                                             |
| Language complexity                                  | 0.25 (0.50)                                          | NA                                                   | 0 (0)                                                |
| Episode                                              | 2.75 (1.06)                                          | NA                                                   | 5 (3.53)                                             |
| Participant 2                                        |                                                      |                                                      |                                                      |
| English                                              |                                                      |                                                      |                                                      |
| Total retelling                                      | 12.5 (1.73)                                          | 12.16 (2.32)                                         | 10 (2.82)                                            |
| Story grammar                                        | 8 (0.81)                                             | 7 (1.54)                                             | 6 (1.41)                                             |
| Language complexity                                  | 0 (0)                                                | 0.50 (0.54)                                          | 0 (0)                                                |
| Episode                                              | 4.5 (1)                                              | 4.66 (0.81)                                          | 4 (1.41)                                             |
| Total personal story                                 | 1.75 (3.5)                                           | NA                                                   | 1.5 (2.12)                                           |
| Story grammar                                        | 1 (2)                                                | NA                                                   | 1.5 (2.12)                                           |
| Language complexity                                  | 0 (0)                                                | NA                                                   | 0 (0)                                                |
| Episode                                              | 0.75 (1.5)                                           | NA                                                   | 0 (0)                                                |
| Spanish                                              |                                                      |                                                      |                                                      |
| Total retelling                                      | 5 (3.53)                                             | 9.83 (1.32)                                          | 7.5 (0.70)                                           |
| Story grammar                                        | 5 (1.41)                                             | 5.16 (0.98)                                          | 4.5 (0.70)                                           |
| Language complexity                                  | 0 (0)                                                | 0 (0)                                                | 0 (0)                                                |
| Episode                                              | 1.5 (2.12)                                           | 4.66 (0.81)                                          | 3 (0)                                                |
| Total personal story                                 | 0 (0)                                                | NA                                                   | 0 (0)                                                |
| Story grammar                                        | 0 (0)                                                | NA                                                   | 0 (0)                                                |
| Language complexity                                  | 0 (0)                                                | NA                                                   | 0 (0)                                                |
| Episode                                              | 0 (0)                                                | NA                                                   | 0 (0)                                                |
| Participant 3                                        |                                                      |                                                      |                                                      |
| English                                              |                                                      |                                                      |                                                      |
| Total retelling                                      | 20.5 (0.57)                                          | 22.16 (1.83)                                         | 22.5 (0.60)                                          |
| Story grammar                                        | 14.25 (1.25)                                         | 15.33 (2.06)                                         | 15.5 (0.70)                                          |
| Language complexity                                  | 1.25 (0.95)                                          | 1.83 (0.75)                                          | 2 (1.41)                                             |
| Episode                                              | 5 (0)                                                | 5 (0)                                                | 5 (0)                                                |
| Total personal story                                 | 8.75 (9.42)                                          | NA                                                   | 12 (0)                                               |
| Story grammar                                        | 6.25 (7.08)                                          | NA                                                   | 6.5 (0.70)                                           |
| Language complexity                                  | 0.50 (0.57)                                          | NA                                                   | 0.50 (0.70)                                          |
| Episode                                              | 2 (2.44)                                             | NA                                                   | 5 (0)                                                |
| Spanish                                              |                                                      |                                                      |                                                      |
| Total retelling                                      | 12.25 (1.50)                                         | 15 (0.89)                                            | 11 (1.41)                                            |
| Story grammar                                        | 8.5 (1.73)                                           | 9.66 (1.03)                                          | 9 (1.41)                                             |
| Language complexity                                  | 0.25 (0.50)                                          | 1 (0.63)                                             | 0.50 (0.70)                                          |
| Episode                                              | 3.5 (1)                                              | 4.33 (1.03)                                          | 1.5 (2.12)                                           |
| Total personal story                                 | 3.25 (1.70)                                          | NA                                                   | 8 (5.65)                                             |
| Story grammar                                        | 2.25 (1.50)                                          | NA                                                   | 4 (2.82)                                             |
| Language complexity                                  | 1 (1.41)                                             | NA                                                   | 1 (0.70)                                             |
| Episode                                              | 0 (0)                                                | NA                                                   | 3 (1.12)                                             |

*Note.* For Participants 1 and 2, total scores could be a maximum of 25 points; story grammar was a maximum of 10 points, language complexity 10 points, and episode 5 points. For Participant 3, total scores could be a maximum of 32 points; story grammar was a maximum of 17 points, language complexity 10 points, and episode 5 points.

Participant 1's baseline retelling scores in English showed some variability ranging from 4 to 10. Spanish baseline scores were higher than English and stable, ranging from 13 to 15. This suggests that at the beginning of the study, Participant 1 had better narrative skills in Spanish than in English. The visual analysis revealed a clear effect of the intervention with no overlap between baseline and intervention scores in English. The intervention line was flat and stable above baseline from session one. In Spanish, we observed an upward trend in the intervention in session four, but scores dropped to baseline levels in session six. Only two out of six intervention points were above 14, the highest baseline point. The change from baseline to intervention was 8.33 points in English and 0 in Spanish. These results were confirmed by the effect size calculations where we observed a Tau-U of .95 ( *p* = .01) for English and .04 ( *p* = .91) for Spanish, indicating that the intervention was more effective for English than Spanish narrative skills. The positive effect of the intervention in English scores was maintained a week and a month after the intervention. We observed increases in personal stories where Participant 1 increased 2.25 points in English and 3.75 points in Spanish from baseline to maintenance, indicating a possible generalization of story grammar elements from the intervention.

Progresses in Participant 1’s narrative abilities were primarily related to the story grammar and episode measures in both language interventions. For story grammar, Participant 1 consistently used the specific name of the character, the problem, the action, and a clear ending. However, Participant 1 frequently used general terms such as “bad” or “happy” instead of more precise emotional descriptors for the feeling. Participant 1 consistently employed a complete narrative structure for the episode (problem, action, and ending), indicating good planning and sequencing skills. However, challenges were observed in language complexity. For example, in English, Participant 1 used only basic temporal connectors such as “then” and “before” during the intervention.

Participant 2’s baseline scores in English were stable between 10 to 14 points. Spanish baseline scores were lower than English, at 4 and 9 points, although only two data points were available. The visual analysis showed variability in English intervention scores, with only two intervention points above the highest baseline point. In Spanish, we observed an intervention line initially higher than baseline but with a downward trend. Out of the six intervention points, four were above 9, the highest baseline point. The change from baseline to intervention was -.34 points in English and 4.83 in Spanish. The effect size calculations showed that the intervention was neither effective for English (Tau-U = -.04, *p* = .91) nor Spanish (Tau-U = .83, *p* = .09). Participant 2 exhibited difficulty using connectors and constructing basic (problem, ending) or complete (problem, action, and ending) narrative episodes. No intervention effect was observed for personal narratives in either language (baseline to maintenance).

Participant 3 showed higher retelling scores during baseline in English (20 and 21 points) than in Spanish (11 to 14 points). Baseline was stable for both English and Spanish, although an upward trend was observed in the last point of the Spanish baseline. The visual analysis revealed a slight intervention effect for English and Spanish, with four out of the six intervention points above baseline in both language interventions. The intervention line showed some variability and an upward trend in sessions four to six in English, and it was flat in Spanish. The change from baseline to intervention was 1.16 points in English and 2.75 in Spanish. The effect size calculations showed that the intervention was effective for increasing Spanish retelling scores (Tau-U = .79, *p* = .04), but it did not reach significance for English (Tau-U = .66, *p* = .08). However, this increase in Spanish scores was not maintained a week and a month after the intervention. We observed increases in personal stories. Participant 3 increased 3.25 points in English and 4.75 points in Spanish from baseline to maintenance, indicating a possible generalization of story grammar elements from the intervention.

Participant 3's improvements in retelling and personal narratives were influenced by the addition of more story elements, such as a specific ending and feelings. Additionally, Participant 3 used more connectors while retelling the story in English compared to Spanish, such as causal (""then"" and ""because""), consequence (""because""), and temporal (""when"") connectors. However, Participant 3 showed some difficulty when expressing the ""plan"" and ""specific consequence"" of the stories.

Figure 1.

*Total retelling scores for all participants*

*Note.* For Participants 1 and 2, total scores could be a maximum of 25 points; story grammar was a maximum of 10 points, language complexity 10 points, and episode 5 points. For Participant 3, total scores could be a maximum of 32 points; story grammar was a maximum of 17 points, language complexity 10 points, and episode 5 points.

Overall, the intervention was effective for increasing retelling scores in Participants 1 and 3’s weakest language (English and Spanish, respectively), but no significant effect was observed in the other language. This effect seemed to generalize to personal narratives in English and Spanish for both Participants 1 and 3. No significant intervention effect was observed for Participant 2 in English or Spanish. All participants achieved higher scores in the retellings than in personal narratives in both English and Spanish.",Oral_Language_Outcom-with-image-refs,"The chart tracks each participant’s total retelling scores in English and Spanish across baseline, intervention, and maintenance phases. Participant 1’s English scores rose well above baseline during the intervention and stayed high in maintenance, while Spanish scores remained flat, showing that the intervention most strongly boosted their weaker language. Participant 2’s scores in both languages fluctuated around baseline with no clear upward trend, indicating minimal intervention impact. Participant 3 experienced a marked increase in Spanish scores during intervention but less consistency afterward, and only modest gains in English. These patterns suggest that focused storytelling practice can effectively strengthen narrative retelling when initial skills are low, but that ongoing reinforcement may be needed to sustain and generalize improvements.","The chart tracks English and Spanish total scores across baseline, intervention, and maintenance for three participants. Initially, two participants scored higher in Spanish than English while one showed stronger English performance, but all three experienced notable increases in English scores during the intervention phase. Spanish scores also improved or stabilized for participants across the same period, and both language gains persisted into the maintenance phase. This pattern indicates the bilingual intervention effectively boosted weaker language skills and reinforced stronger ones, leading to balanced improvement. These durable gains suggest that integrating targeted bilingual strategies can sustainably enhance proficiency in both languages."
The_Promise_of_Maste-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/life_articles/processed_articles/The_Promise_of_Maste-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/life_articles/processed_articles/The_Promise_of_Maste-with-image-refs_artifacts/image_000002_a15a2a99eb3134bdd94c4ca37e0e43b09731c759fcb7823973c81eea25f3b6ce.png,"## 3.1 Struggling Students Were More Likely to Repeat Tests

The relationship between (a) the first-attempt scores for all students on all mastery tests, and (b) whether students chose to repeat each test is displayed in Figure 3. As predicted, students who struggled on their first attempt were much more likely to repeat the test: a one standarddeviation decrease in first attempt scores was associated with a more than 3x increase in the odds of a student repeating the test at least once, p &lt; .001. Students were likely to repeat unit tests at least once if they attained lower than an 80% on their first attempt (with 81% of students doing so), but only 21% of students who attained a score greater than 80% chose to retake that test. A horizontal line has been added to Figure 3 to emphasize this criterion that students appeared to impose on themselves.

Figure 3. First attempt scores vs. the proportion of students repeating each unit test.

3.2 Use Of The Testing System Was Associated With Students Returning To Practice Course Resources From Previous Units

Figure 4. Use of the mastery testing system vs. online chemistry problems completed in all units before (4A) and after (4B) the first corresponding mastery-based test. Plotted lines show predicted values from the regression models used to test the relationships. Error envelopes represent +/- 1 standard error.

On average, students chose to repeat approximately six tests over the duration of the course. Figure 4 shows the relationship between the number of repeated tests that each student took and the number of problems that they completed in relevant units of the online courseware, both before (4A) and after (4B) attempting each test for the first time. Although usage of the mastery testing system was unrelated to the number of problems that students completed before their first attempts,  (232) = 0.506, t p = .613, each additional repeated test was associated with a student circling back to a previous unit and completing an additional 95.4 practice problems after their first test attempt,  (232) = 8.75, t p &lt; .001, an approximately 10% increase in total studying per repeated attempt. This pattern of results is consistent with the mastery testing system encouraging students to return to course materials to prepare themselves for subsequent mastery attempts, increasing the total studying students completed.

3.3 Studying Between Mastery Attempts Was Associated With Improved Performance On Repeated Tests

Figure 5. Problems completed between repeated tests, compared with change in performance between attempts. Each point represents one unit test for each student. The plotted lines show predicted values from the linear mixed-effects models used to test the relationship for submissions one and two (5A) and submissions two and three (5B).

What happened when students decided to retake a unit test and revisit that unit's contents? Was the additional practice associated with learning? As Figure 5 shows, studying relevant content between test attempts was associated with improved performance; each additional 100 practice problems completed predicted a 5.1 point improvement in test scores between attempts one and two, F (1, 965) = 45.77, p = .001 (Figure 5A), and a 4.9 point improvement between attempts two and three, F (1, 321) = 5.01, p = .025 (5B). Notably, this analysis suggested that students were unlikely to show improvement in test scores unless they studied between attempts.",The_Promise_of_Maste-with-image-refs,"The chart illustrates a steep decline in retake rates once first‐attempt scores exceed 80%: over 80% of students scoring below this mark choose to retake, compared with about 20% of those above it. This pattern indicates that learners impose an informal 80% mastery threshold when deciding whether to revisit a test. Educators can leverage this natural decision point by offering targeted review resources or prompts for students hovering near that boundary to encourage deeper engagement. Aligning feedback and retake opportunities with this threshold could boost overall mastery while respecting the confidence of higher‐scoring students.","The chart shows that students scoring below the passing cutoff of 80 continue to retake the test at high rates—roughly 75% to 90%—while those hitting or exceeding the threshold see repeat rates plunge to around 25% and 10%. This consistent behavior among all failing‐score groups indicates a uniform drive to improve regardless of how low the initial grade was. To curb the volume of retakes, schools might concentrate remedial resources on learners just below the pass mark where extra support can quickly translate to fewer repeats. Conversely, offering advanced modules or enrichment opportunities for students who pass could sustain engagement and encourage ongoing skill development."
"Yu,_Heng,_Arden-Gard-with-image-refs","/home/mlazar/projects/thesis/tbp_articles/life_articles/processed_articles/Yu,_Heng,_Arden-Gard-with-image-refs.md","/home/mlazar/projects/thesis/tbp_articles/life_articles/processed_articles/Yu,_Heng,_Arden-Gard-with-image-refs_artifacts/image_000003_97c4e4769b9d432d26613fc4b5c432f31defbf4903a118ce7cc0b09d525993f4.png","## Experiment 2 Criterial Test Results

Table 2 Experiment 2 Pairwise Comparisons of Learning Condition and Its Interaction with

Question Type on the Criterial Test

| Analysis type                  | Pairwise comparisons           |     b |    SE | 95% CI         | p        |     d |
|--------------------------------|--------------------------------|-------|-------|----------------|----------|-------|
| Main                           | Feedback vs. Control           | 0.058 | 0.045 | [-0.047, 0.16] | .396     | 0.2   |
| effects                        | Feedback vs. Non-feedback      | 0.043 | 0.045 | [-0.063, 0.15] | .606     | 0.15  |
|                                | Non-feedback vs. Control       | 0.016 | 0.045 | [-0.090, 0.12] | .936     | 0.053 |
|                                | Tested vs. Untested questions  | 0.028 | 0.037 | [-0.044, 0.10] | .437     | 0.097 |
| ModeratingFeedback vs. Control | ModeratingFeedback vs. Control | 0.36  | 0.089 | [0.15, 0.58]   | <.001*** | 1.24  |
| effects of                     | Feedback vs. Non-feedback      | 0.27  | 0.089 | [0.056, 0.49]  | .005**   | 0.93  |
| question type                  | Non-feedback vs. Control       | 0.093 | 0.089 | [-0.12, 0.31]  | .299     | 0.32  |

Note. Main effects refer to pairwise comparisons of learning conditions. Moderating effects of question type refer to pairwise comparisons of the interaction between learning condition and question type. * = p &lt; . 05; *** = p &lt; .001.","Yu,_Heng,_Arden-Gard-with-image-refs","The chart visualizes how three study conditions affect accuracy on practiced (tested) versus unpracticed (untested) questions. In the feedback condition, median accuracy on practiced items climbs to about 60% and individual scores are tightly clustered, outperforming both the no-feedback and control groups. By contrast, for unpracticed items the feedback group yields the lowest median accuracy, while no-feedback and control participants maintain similar, higher performance. Prequestioning without feedback fails to boost accuracy on practiced questions and shows no detriment on unpracticed ones. Together, these patterns reveal a clear interaction: feedback enhances retention of specifically probed content but does not generalize— and may even hamper—performance on new material.","The chart shows how three teaching methods—prequestioning with feedback, prequestioning without feedback, and simply presenting learning objectives—impact students’ correct answers on questions they’d seen before versus entirely new ones. When students answered questions before studying and then received feedback, they achieved the highest scores on those practiced items but the lowest performance on new, untested questions. Prequestioning without feedback led to modest gains on practiced questions and better transfer to new ones, suggesting a broader but shallower grasp. Relying on learning objectives alone produced steady, middle-of-the-road results for both familiar and novel questions. This pattern indicates that feedback after pretesting sharpens recall of specific facts, while fewer cues or more general objectives encourage more flexible application to new problems."
A_Human-centered_Con-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/social_articles/processed_articles/A_Human-centered_Con-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/social_articles/processed_articles/A_Human-centered_Con-with-image-refs_artifacts/image_000003_f98fe0c0fd41f25cb7f9adfefd58f4f30624d39915dee299cb92e27fe12c4333.png,"## Reimer et al.

owners indicated that their charging behavior is triggered more by a situation in which the state of charge falls to a certain level.

In their reported frequency of how often they charge their car at a public charging station, EV and PHEV owners show a remarkable overlap in their responses (see Figure 3 ). Overall, 29.8% of electric (EV) car owners charge their car less than once a month, while 40.4% charge it several times a month, 25.4% several times a week, and 4.4% daily. EV users appear to use public charging stations at a similar rate as PHEV users, with a 4% larger group of PHEV users reporting that they charge their car several times a week at a public charging station (29.4% PHEV vs 25.4% EV users).

How often do you use public electric charging stations to charge your vehicle?

Figure 3 Percentage of Users Reporting Different Frequencies of Charging Separately for Electric Car and Plug-in Hybrid Car Users

empty as possible. This percentage rose to 24.8% of hybrid car users, but was still very low compared to 1 drivers of cars with combustion engines in the Philipsen et al. ( 1 ) study. 2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

Charging frequency

Figure 4 Reported Frequencies of Critical Battery/Fuel Levels that Cause Discomfort among Drivers Compared to the Reported Frequencies in Philipsen et al. ( 1 )",A_Human-centered_Con-with-image-refs,"The chart reveals that most EV drivers start to feel uneasy once battery levels drop to around 50%, with 57.9% of EV and 46.8% of PHEV owners reporting discomfort at this point. Only about 5% in both groups wait for a warning light or an almost empty battery, and just over 20% tolerate levels below 25%, indicating a proactive approach to charging well before critically low states. Compared with data from 2018, today’s electric vehicle users show even greater sensitivity at mid-level charges than earlier battery-electric and conventional car drivers. This suggests that public charging networks and onboard reminders should focus on supporting drivers when batteries are still half full to maintain confidence and avoid last-minute charging surges.","The chart shows that today’s pure electric-vehicle drivers begin to feel uneasy as soon as their state of charge falls below 50%, with almost 60 % reporting discomfort at that point, while plug-in hybrid owners are somewhat less worried (about 47 %). In contrast, drivers in earlier studies—both battery-electric and conventional gasoline cars—only reported anxiety much closer to an empty tank or battery, with fewer than 5 % uncomfortable at the 50 % mark. Both modern EV and PHEV users also show noticeable concern once charge drops below 25 %, whereas legacy drivers mainly react when warned of an “almost empty” battery or tank. This pattern highlights that current EV adopters exhibit significant mid-range “range anxiety,” suggesting that improving public charging infrastructure and clearer charge-level information could help ease their concerns."
Effector-specificity-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/social_articles/processed_articles/Effector-specificity-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/social_articles/processed_articles/Effector-specificity-with-image-refs_artifacts/image_000000_cd4d25ddcc675cde345fa457fba2feaaa7e81889a478ae219f18d689c6ef6a5b.png,"## Research Gap and Hypotheses

The literature is currently mixed regarding the superiority of one effector over another in beat synchronization tasks. While some studies report that SMS to music is more natural with the lower limbs, whether this translates to improved performance over SMS with the more dexterous fingers has yet to be investigated directly. Therefore, this experiment directly compares the biomechanical (via the intercept) and cognitive (via the slope) performance of finger and foot synchronization to music of varying rhythmic complexity. We propose three competing hypotheses:

Naturalness hypothesis. Naturalness is associated with greater ability, so foot tapping should outperform finger tapping. This would be reflected in both a lower intercept, indicating lower baseline variability, and a flatter slope, indicating less variability at higher rhythmic complexity.

Dexterity hypothesis . Because humans have better motor control over their fingers than their feet, finger tapping should be less variable than foot tapping. This would manifest in lower intercepts and flatter slopes with finger tapping, suggesting fewer biomechanical and cognitive constraints.

Central pattern generator hypothesis. Sensorimotor synchronization is not effector-specific at all and is instead governed by a unified neural system that coordinates synchronization equally well with any limb, regardless of rhythmic complexity. Thus, both the intercepts and slopes should be equivalent for SMS with the foot or finger.

It is important to note that because we are assessing two different parameters of the linear model, the intercepts and the slopes, it is possible to observe some combination of the above hypotheses. For instance, foot synchronization may exhibit a higher intercept than finger synchronization due to the inherent variability in moving a larger effector, however, foot synchronization may still exhibit a flatter slope with rhythmic complexity owing to its more natural ability to synchronize to slower, beat-related frequencies. A schematic of all three hypotheses is depicted below in Figure 1.

Figure 1 . Schematic illustrations of the three hypotheses. The left column represents the pattern of results that would support the naturalness hypothesis, the middle column represents the pattern of results that would support the dexterity hypothesis, and the right column represents the pattern of results that would support the central pattern generator hypothesis. The top row illustrates intercept effects while the bottom row illustrate slope effects.",Effector-specificity-with-image-refs,"The chart lays out three distinct patterns of how tapping variability might behave for fingers (blue) versus feet (red) as the rhythmic complexity of the music increases. Under the naturalness hypothesis (left column), foot taps would start off steadier (lower baseline variability) and show a gentler increase in variability with more complex rhythms compared to finger taps. The dexterity hypothesis (middle column) predicts the reverse: finger taps begin with less variability and resist complexity better, yielding flatter slopes than foot taps. The central pattern generator hypothesis (right column) shows both effectors overlapping almost perfectly, indicating no meaningful difference in either their starting variability or how much that variability grows. By examining actual intercepts (initial variability) and slopes (rate of change with complexity) against these idealized patterns, researchers can pinpoint whether natural limb preferences, fine motor control, or a shared neural timing system best explains sensorimotor synchronization.","The chart shows how tapping variability grows for both fingers and feet as rhythms become more complex, with separate panels illustrating three competing theories about why this happens. Only the naturalness account predicts—and the real data confirm—that fingers start off less consistent and lose precision faster than feet as complexity rises. In contrast, a dexterity-based view would expect feet to be more variable, and the idea of a shared internal rhythm generator would predict no limb difference, but neither of those matches what’s observed. This clear pattern highlights an innate advantage in foot timing and suggests that rhythm training or therapy might be most effective when it builds on our natural gait rhythms."
Generative_Artificia-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/social_articles/processed_articles/Generative_Artificia-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/social_articles/processed_articles/Generative_Artificia-with-image-refs_artifacts/image_000001_dc35201b83ef8c5abed17e7f72aebc59dc00780b6d0ff4c78a76697dbb5cf338.png,"## Sex Invariance

Measurement invariance across biological sex was examined using a stepwise multi-group CFA. The configural model, which imposes no equality constraints, demonstrated acceptable fit ( χ² (82) = 186.28, p &lt; .001, CFI = .97, TLI = .95, RMSEA = .08, SRMR = .04), indicating that the hypothesized factor structure was similar for males and females. Additionally, metric invariance was supported, as the model constraining factor loadings to equality showed only a minimal change in fit (ΔCFI = .004), with improved AIC and BIC values compared to the configural model. Scalar invariance, which further constrained item intercepts, was also supported, with an additional negligible change in fit (ΔCFI = .002) and further reduced AIC and BIC values. Finally, the residual invariance model (equal loadings, intercepts, and residuals) yielded a slightly larger decline in fit (ΔCFI = .014; Table 4) . Together, these results provide evidence for scalar, but not residual 1 invariance across sex.

Table 4 Summary of Model Fit Statistics for Sex Invariance

| Model                   |   CFI | ΔCFI   |     AIC |     BIC |
|-------------------------|-------|--------|---------|---------|
| Configural              | 0.966 |        | 9961.48 | 10250.6 |
| Metric (vs. Configural) | 0.962 | .004   | 9963.79 | 10212.8 |
| Scalar (vs. Metric)     | 0.96  | .002   | 9962.07 | 10183   |
| Residual (vs. Scalar)   | 0.946 | .014   | 9975.32 | 10152   |

Note. ΔCFI = change in CFI relative to the previous model. Lower AIC and BIC values indicate better model fit. All ΔCFI values are below the recommended cutoff of .01 (Cheung &amp; Rensvold, 2002).

1 Modification indices suggested that partial strict invariance could be attained by freeing the residual variances of 'I have trouble completing work or other responsibilities without generative AI' and 'I get frustrated or irritable when I am unable to use generative AI'.

Figure 2 CFA Model of the Generative AI Dependency Scale

Note. N = 410. All estimates refer to standardized estimates. All estimates were statistically significant with p s &lt; .001.",Generative_Artificia-with-image-refs,"The chart depicts a hierarchical model where a single dependency factor underlies three linked patterns of AI use: persistent preoccupation with using AI, real-life negative consequences, and withdrawal symptoms when access is blocked. Each pattern loads very strongly onto overall dependency (.93, .91, and .96 respectively), confirming they are all central components of AI reliance. Item‐level loadings range from .60 to .89, showing that every question in the scale meaningfully measures its intended issue. The especially high link to withdrawal symptoms suggests that emotional distress (restlessness, irritability, distraction) when AI access is cut off is the clearest sign of problematic dependence. These results validate the scale’s robustness and point to withdrawal indicators as key early warning signs for identifying excessive AI reliance.","The chart portrays that heavy reliance on generative AI appears across three areas: persistent thoughts about using it, real-life downsides like work disruption, and emotional distress when it’s unavailable. Each feature—such as urges to use AI without real need, trouble finishing responsibilities, or feeling irritable when access is cut off—is strongly connected to its area, with most linkages indicating very close relationships. The exceptionally high connections among these three areas suggest they combine into a single pattern of dependency on generative AI. This means efforts to curb problematic use should simultaneously address obsessive focus on AI, mitigate its negative impact on productivity and confidence, and alleviate withdrawal discomfort when access is limited. By taking this holistic approach, users can maintain a healthier, more balanced engagement with AI tools."
Identifying_Psycholo-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/social_articles/processed_articles/Identifying_Psycholo-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/social_articles/processed_articles/Identifying_Psycholo-with-image-refs_artifacts/image_000001_1c7f013d40d0e3ac2ab64a12936e677201f61773457112aa7ec3f2ecf045b911.png,"## CIBER plot for potential determinants of safe listening

Means and associations (d) with Safe Listening (R? = .4 | .57)

Figure 2. Confidence interval -based estimation of relevance (CIBER) plot with, on the left panel, psychological factors ' distributions and sample means (purple diamonds for participants listening ≤ 2.5 hours/week at a loud volume, and green diamonds for those listening &gt;2.5 hours/week at a loud volume) and, on the right panel, Cohen's d for associations of psychological factors with safe ear- and headphone listening.",Identifying_Psycholo-with-image-refs,"The chart shows that people who keep their loud-music exposure within safe limits consistently report stronger self-regulation skills (capacity), greater anticipation of regret if they listen unsafely, more practical (“instrumental”) attitudes toward protecting their hearing, a future-focused outlook, higher awareness of risks, and more willingness to act than those who exceed 2.5 hours per week. On the right, the size of these associations (Cohen’s d) is largest for capacity and anticipated regret, indicating these are the most powerful levers for encouraging safe listening. In contrast, enjoying the thrill of loud music (experiential attitude) is strongly linked to unsafe habits, and perceiving oneself as threatened by hearing loss shows a moderate negative relationship with safe behavior. Social norms and locus of control barely move the needle, suggesting that simply telling people “everyone else is doing it” or “you’re in control” won’t shift habits much. Focusing on building self-regulatory skills, highlighting potential regret, and framing hearing protection as a practical, future-oriented choice should therefore drive the greatest improvements in safe listening.","Participants overwhelmingly rate their own Awareness and Future Orientation high, while giving relatively low scores to Threat Susceptibility and Habit, suggesting they feel informed and forward-thinking but aren’t strongly driven by fear or routine. The right-hand plot reveals that stronger Awareness, positive Social Norms, greater Willingness, and a practical (Instrumental) Attitude are the most powerful drivers of the desired action, with confidence intervals entirely above zero. In contrast, negative feelings (Experiential Attitude) and fear-based perceptions (Threat Susceptibility) actually predict lower uptake. Locus of Control and Habit contribute positively but more modestly. These insights imply that interventions should boost clear information, emphasize practical benefits, and leverage social encouragement rather than rely on scare tactics."
Impact_of_cash-out_a-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/social_articles/processed_articles/Impact_of_cash-out_a-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/social_articles/processed_articles/Impact_of_cash-out_a-with-image-refs_artifacts/image_000007_693ab24a53bef8712e68f103c1d2f105c0d645858890177365c821fced2d758b.png,"## Moderation by PGSI

There was no interaction between continuous PGSI score and cash-out availability for either stake size ( β =-0.02, SE =0.02, p =0.15) or implied win probability of placed bets ( β&lt; 0.01, SE&lt; 0.01, p =0.44). Cash-out availability also did not significantly predict either outcome in the models adjusted for PGSI (stake size β =0.08, SE =0.08, p =0.32;  implied  win  probability β =-0.01, SE =0.01, p =0.35).  See  Table  S9  in  the supplement for the full models.

Fig  5. Subgroup  analyses  of  effects  of  cash-out  availability  among  low-  (&lt;  3)  and  high-PGSI  (&gt;=  3) participants. The low-PGSI subgroup showed a significant tendency to place larger bets when cash-out was available, which was not observed in the high-PGSI subgroup (A). Neither subgroup showed an effect of cash-out availability on bet riskiness (B). Note: Individual data points have been jittered by up to £0.01 in the stake-size plots and up to 0.002 in the riskiness plots for ease of viewing.

As the effects of gambling products on those most at risk of gambling harms are of particular importance, we also pre-registered re-running all the analyses for H2 and H3 for those participants at moderate-to-high risk of gambling harms (PGSI &gt;= 3). For comparison, we also provide exploratory analyses of the lowPGSI subgroup.

Figure 5 shows the result for stake size (Panel A) and riskiness (Panel B) split by PGSI subgroup. There was no significant effect of cash-out availability on stake size among the high-PGSI subgroup ( W =1699.5, p =0.89, n=117). In contrast, the low-PGSI subgroup showed mixed evidence: median stake was almost 50% greater when cash-out was available (no-cash-out £0.69, IQR: £0-£1.00; cash-out-available £1.00, IQR £0-£1.00), and this difference was significant (W=6909.5, p =0.0076, n=260). As with the full sample, there were no effects of cash-out in the mixed-effects regression for either subgroup (low-PGSI subgroup: (β=0.11,  SE=0.07, p =0.16;  high-PGSI  subgroup:  β=-0.18,  SE=0.12, p =0.16).  See  Table  S10  in  the supplement for the full results.

Similar to the results from the analysis of the full sample, there were no significant effects of cash-out availability on bet riskiness in either the high-PGSI ( W =985, p =0.55, n=90) or the low-PGSI subgroups ( W =5654.5, p =0.18, n=206). Neither subgroup showed any significant association in the linear mixedmodel analysis either (see Table S11 in the supplement).",Impact_of_cash-out_a-with-image-refs,"The chart compares how risky bets are—measured by the implied probability of winning—for low- and high-PGSI participants when a cash-out option is either unavailable or available. In both subgroups, the medians and spreads of bet riskiness overlap almost completely regardless of cash-out availability, and statistical tests confirm no significant differences (p = 0.18 for low-PGSI; p = 0.55 for high-PGSI). This indicates that offering bettors the chance to lock in a partial return does not meaningfully change how conservatively or aggressively they wager. In practical terms, adding a cash-out feature appears neutral in its impact on the inherent risk level of bets, even among those at higher risk of gambling harm.","The chart examines whether giving bettors an early cash-out option changes how conservative or risky their wagers are, comparing those with low versus high problem-gambling severity. In both groups, the typical bet’s implied win probability – a proxy for risk level – sits at virtually the same point whether cash-out is offered or not, and each group shows a similarly wide range of bet risk. Statistical tests show these tiny shifts aren’t meaningful, meaning cash-out availability doesn’t systematically push bettors toward safer or bolder plays. In practical terms, simply adding a cash-out feature doesn’t appear to alter betting behavior, even among individuals with more severe gambling issues."
Instrumental_variabl-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/social_articles/processed_articles/Instrumental_variabl-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/social_articles/processed_articles/Instrumental_variabl-with-image-refs_artifacts/image_000002_1a7094d50a98c33d5d1dee5d30bfb19a81e16de5d3856b51477d484e1f7b9a08.png,"## 1 Instrumental Variables Regression with Latent Variables and Differential Item Functioning

Instrumental variables regression (IVR) is an approach to reducing bias in the estimate of a treatment effect when the treatment is endogenous-that is, not randomly assigned (Angrist &amp; Krueger, 2001; Angrist &amp; Pischke, 2009; Murnane &amp; Willett, 2010). Most treatments of IVR use the econometric two-stage least squares (2SLS) approach to estimation. However, IVR can also be represented within a structural equation modeling (SEM) framework by allowing for correlated error terms between the endogenous treatment variable and the outcome, effectively modeling it as a type of mediation (Maydeu-Olivares et al., 2019; Murnane &amp; Willett, 2010).

Figure 1 shows a path diagram illustrating this IVR-as-SEM approach with one instrument. When the model assumptions are met, this approach yields an unbiased estimate of the treatment effect and provides accurate standard errors, equivalent to those generated in 2SLS using a robust sandwich estimator (Maydeu-Olivares et al., 2019, p. 880).

Figure 1: Path diagram for instrumental variables regression conceived as a structural equation model

Notes: ε 0 is the error term for the outcome; ε T is the error term for the endogenous treatment.

Although Maydeu-Olivares et al. (2019) limited their analysis to observed variables, one of the strengths of SEM is the ability to model latent variables and estimate relations between them that are corrected for measurement error. This involves embedding a confirmatory factor analysis (CFA) model (J¨ oreskog, 1969) within a larger structural model, where the CFA defines the latent

variable as an unobserved common cause of observed variables known as its indicators. Figure 2 extends the SEM treatment of IVR to the case of a latent outcome variable. To make this more literal and preview the focal analysis of this paper, we have replaced 'instrument,' 'treatment,' and 'outcome' with actual variables from an analysis of the influence of time spent on homework on math achievement that we expand upon below (Gustafsson, 2013). This model aims to estimate the effect of time spent on homework on math achievement. However, the original study was motivated by a concern about 'reverse causation': time spent on homework may positively influence math achievement, while poor math achievement may also drive more time spent on homework. The presence of the instrument, teacher-assigned homework time, enables the separate estimation of the relation between exogenous homework time in the presence of reverse causation if the model's assumptions are met. Furthermore, modeling math achievement as a latent variable corrects for attenuation bias in the main effect induced by outcome measurement error. Without correction, measurement error would affect standardization of the model coefficients and reduce power to detect statistically significant relations (Gilbert, 2025; Kline, 2023; Shear &amp; Briggs, 2024).

Figure 2: Path diagram for instrumental variables regression conceived as a structural equation model with a latent outcome

Notes: ε 0 is the error term for the outcome; ε T is the error term for the endogenous treatment. I1-I5 are observed indicators of latent math achievement. Squares represent observed variables, circles represent latent variables.

IVR relies on several well-known assumptions, most notably, that the instrument affects the outcome only through its influence on the treatment (the exclusion restriction), and that the instrument

is strong and applied to a sufficiently large sample (Murnane &amp; Willett, 2010; Winship &amp; Morgan, 1999)-When the outcome is modeled as a latent variable, an additional assumption comes into play: that the measurement model, specifically, the confirmatory factor analysis (CFA) portion, is invariant across levels of the endogenous treatment. In the context of Figure 2, this means that student homework time should affect performance on individual items (I1-I5) via its influence on the underlying latent variable Math . In other words, the relationship between homework time and item responses must be fully mediated by the latent variable (Stoetzer et al., 2025). This assumption is referred to as factorial invariance (Meredith, 1964, 1993; Thissen, 2025), and as differential item functioning (DIF) in the Item Response Theory (IRT) tradition (Angoff, 1981; Angoff &amp; Ford, 1973; Holland &amp; Thayer, 1986). Both frameworks deal with the question of whether the given latent trait is being measured equivalently across different groups or conditions. Factorial invariance, i.e., a lack of DIF, is generally an assumption of causal models that estimate the effect of a treatment on a latent variable (P. Halpin &amp; Gilbert, 2024; Stoetzer et al., 2025; VanderWeele &amp; Vansteelandt, 2022).

A violation of the assumption described above implies that some test items are directly influenced by time spent on homework, beyond its mediated effect through the latent variable of math achievement. This situation, illustrated in 3, introduces item-specific effects into the model. When this occurs, it becomes necessary to distinguish between the overall treatment effect, referred to as impact, and deviations at the item level, known as DIF.

Importantly, DIF is not inherently problematic or informative. Its implications depend on the specific context of the analysis, particularly the groups being compared and whether the DIF is construct relevant. In some cases, DIF may signal measurement bias that undermines valid estimation of impact (P. Halpin &amp; Gilbert, 2024; P. F. Halpin, 2024; VanderWeele &amp; Vansteelandt, 2022). In others, it may reveal meaningful differences in how treatment effects unfold across items, offering insight into underlying mechanisms of impact (Gilbert, Himmelsbach, Soland, et al., 2025; Gilbert, Kim, &amp; Miratrix, 2024; Gilbert et al., 2023). This study introduces two analytic

approaches to handling potential DIF when applying IVR with latent variables, and demonstrates their application.

Figure 3: Path diagram for instrumental variables regression conceived as a structural equation model with potential DIF

Notes: ε 0 is the error term for the outcome; ε T is the error term for the endogenous treatment. I1-I5 are observed indicators of latent math achievement.",Instrumental_variabl-with-image-refs,"The diagram shows how teacher-assigned homework time is used as a clean source of variation to trace the true effect of students’ own homework habits on their deeper math skill, which is inferred from five test questions. Solid arrows map out the intended chain: more assigned homework leads to more student homework, which then improves overall math ability and raises scores on all items. A curved line between unmeasured influences reminds us that unknown factors may simultaneously affect both homework time and math performance. Red dashed lines flag that some test questions could be directly influenced by homework time itself, not just by underlying ability, introducing potential bias in those item scores. Spotting and correcting for these direct item effects is vital to ensure that the measured homework impact truly reflects gains in math skill rather than quirks in individual questions.","The chart shows that teachers’ homework assignments boost students’ homework engagement, which then builds their underlying math ability. That math ability drives performance on each of five test questions, but there’s also a direct influence from students’ homework engagement to each question—revealing potential bias in those item scores. The linked error terms between homework engagement and math ability suggest other common factors at play. Overall, this structure highlights the importance of accounting for students’ homework habits when interpreting test results to ensure scores truly reflect math skill rather than extra homework effects."
Interpersonal_Cardia-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/social_articles/processed_articles/Interpersonal_Cardia-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/social_articles/processed_articles/Interpersonal_Cardia-with-image-refs_artifacts/image_000000_d05c1ac83ff7eafab2043bbab59656ee59686490d5754f6824434cde0c9f65b8.png,"## 2.4 ECG analysis

We preprocessed ECG signals using Neurokit2 (Makowski et al., 2021) in Python. We trimmed the raw 130 Hz ECG data to retain only the 15-minute interaction phase, ensuring consistency across recordings. Of the 40 dyads recorded with ECG, 11 dyads were excluded due to technical issues such as missing video, audio problems, or sensor malfunctions, resulting in a final sample of  28  dyads  (56  participants)  for  ECG  analysis.  An  additional  dyad  was  then  excluded  due  to excessive ECG data loss (&gt;5% missing; (Heathers et al., 2014)). For the 28 remaining dyads, we applied cubic spline interpolation to ensure data continuity. To remove artifacts, we applied a 0.5 Hz high-pass  Butterworth  filter  to  correct  for  baseline  drift,  a  60  Hz  notch  filter  to  eliminate electrical noise, and a 20 Hz low-pass filter to remove high-frequency components. We detected

Rpeaks  using  Neurokit2's  ecg\_findpeaks  function,  with  manual  corrections  for  erroneous detections based on peak-to-peak interval discrepancies. We computed inter-beat intervals (IBIs) and  removed  IBIs  outside  the  600 -1000  ms  range  using  manual  thresholding.  Across  all participants, an average of 2.7% of IBIs were rejected during preprocessing due to falling outside the acceptable range. To ensure that data quality did not influence our main results, we examined whether the proportion of excluded IBIs was associated with key outcome variables. No significant associations were found, indicating that data quality did not drive the observed effects. We then calculated  HRV  using  root  mean  square  successive  RR  interval  differences  (RMSSD)  via Neurokit2's ecg\_inte rvalrelated function, a validated index of vagal tone (Appelhans &amp; Luecken, 2006; Shaffer et al., 2014).

Figure 1 IBI Timeseries Sample Plot and Cross-Correlation between 2 residualized IBI series

Note. This figure displays a sample plot of a part of an IBI time series for a support SP (blue) and SR (green), along with the corresponding cross-correlation plot. The left panel (A) shows the IBI fluctuations  over  time,  while  the  right  panel  (B)  illustrates  the  cross-correlation  values  across different time lags.",Interpersonal_Cardia-with-image-refs,"The chart shows how the time between heartbeats fluctuates for both the support provider and receiver, illustrating that each person’s cardiac rhythm naturally rises and falls during their 15-minute interaction. The accompanying cross-correlation plot reveals a modest peak at small positive and negative lags, meaning each partner’s heartbeat pattern tends to mirror the other’s with only a brief delay. This subtle timing alignment suggests a dynamic physiological coupling, where shifts in one person’s autonomic state are quickly reflected in the other’s. Such moment-to-moment synchrony may signal the partners’ emotional attunement and shared engagement as they provide and receive support.","The chart tracks how long it takes between heartbeats for each person, showing that one individual’s rhythm stays steadier and slower while the other’s speeds up and slows down more dramatically. Despite these differences, their rhythms tend to rise and fall in tandem. A correlation analysis reveals that when one person’s heartbeat interval shifts, a similar change often appears in the other’s signal a few moments later. This time-shifted matching suggests the pair is physiologically attuned during their interaction. Monitoring such real-time heart-rhythm alignment could offer a practical way to assess and strengthen supportive connections."
Introducing_the_koll-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/social_articles/processed_articles/Introducing_the_koll-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/social_articles/processed_articles/Introducing_the_koll-with-image-refs_artifacts/image_000004_2e73605534ef545a6d763fa20d9b75c5543f4f7dcb0a155f53e23fd8db8e76a9.png,"## Examining the impact of parameter settings on fixation classification

Visualizations are useful for selection of event classification and their algorithms. Figure 5 shows the fixations identified by the I-VT algorithm using three different threshold parameter values in a segment of data from participant 1. Data are plotted in a 2D space. The plot was generated by the kollaR function filt\_plot\_2d . Reassuringly, the three threshold values lead to consistent results in this data segment. If results look

similar for a range of different sub-sections of the data set, it suggests that the choice of algorithm is not affecting the results much.

Gaze position Y

Figure 5. Fixations of classified by the I-VT algorithm at different velocity threshold values. Data from participant 1. Gaze position is shown in pixels.

Figure 6 shows the fixations detected by two different algorithms during a period of data from participant 2. Although results ares similar, the Identification by 2 means clustering (I2MC) algorithm reports a fixation in the upper part of the plot which is not reported by the I-VT algorithm. Visualizations like these over a range of data segments can indicate

systematic differences between algorithms. This would merit further examination before selecting an algorithm. It would be important to examine whether the discrepancies between algorithms are more likely to be found in time segments characterized by noisy data. Researchers examining group differences (for example, in fixation time between autistic and non-autistic children) would be interested in whether data from both groups are equally sensitive to differences between algorithms. It would also be of interest to know which outcome measures these discrepancies could affect. For example, do the algorithms disagree on the number of fixations, but not on their total duration or location? This could indicate a problem for a study that examines the number of fixations, but not necessarily pose a threat to an analysis of total fixation duration.

Gaze position Y",Introducing_the_koll-with-image-refs,"The chart compares fixation detection by the I-VT algorithm using 25, 30 and 50°/s velocity thresholds, and reveals two stable fixation clusters at identical screen locations and with similar durations across all settings. This consistency implies that selecting any threshold in this range will not substantially change the number, position or size of identified fixations for this participant’s data segment. Consequently, fixation-based metrics like count and spatial distribution can be considered reliable and not overly sensitive to moderate shifts in velocity cutoff. For thoroughness, similar plots should be generated on noisier time windows to confirm that this robustness generalizes across the entire data set.","The chart reveals two tightly clustered fixation points in each condition, one near the left and one near the right of the display, connected by a long horizontal saccade. These clusters—highlighted by the colored circles—overlap almost perfectly across the 25°, 30° and 50° conditions, indicating that the large saccade consistently lands at the same positions regardless of angle. The dense clouds of small dots after the right cluster show a similar upward drift or exploratory scanning pattern in all three cases. This consistency suggests that the main saccade targeting is robust to changes in the intermediate viewing angle. Researchers can therefore treat initial and secondary fixation locations as stable anchors even when experimental angles vary."
Learning_visual_appe-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/social_articles/processed_articles/Learning_visual_appe-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/social_articles/processed_articles/Learning_visual_appe-with-image-refs_artifacts/image_000002_c1dbb446a7a7cd54abd1453b153f78712316e5932f3cbc3718380c64a85ad46b.png,"## Blind and sighted people infer object colorfulness from intended function

Both  sighted  and  congenitally  blind  participants  judged Colorfulness-Intent ( Intent ) artifacts to have more colors than No-Colorfulness-Intent  ( No-Intent ) artifacts (subject-wise repeated measures ANOVA, 2 conditions ( Intent  No-Intent , ) x 2 groups  (sighted, blind): main  effect of condition, F (1,36) = 441.29, p &lt; .001; Figure 2).

All  groups  assigned  more  colors  to  artifacts  for  which colorfulness is intended to facilitate function (i.e., Colorfulness-Intent artifacts; all p s &lt; .001). This effect was highly  significant  in  each  group  and  statistically  identical across groups (group by condition interaction, F (1,36) = 0.03, p = .88; main effect of group, F (1,36) = 1, p = .32).

Figure 2: Average item-wise colorfulness judgments across groups. Average non-normalized judgments for all 60 artifacts are displayed for each group. Paired artifacts (e.g., 'fairy  tale  book,'  'instructional  manual')  are  connected  by lines.

In  both  groups,  colorfulness  judgments  were  highly  and equally correlated with ratings of how helpful colorfulness is to  function  collected  in  a  separate  online  study  (sighted: ρ  =  .84, p &lt; .001,  blind:  ρ  =  .82, p &lt; .001).  These  results support  the  hypothesis  that  people  born  blind  use  this information to predict the number of colors an object is likely to have.

Further evidence for the idea that blind people can infer colorfulness  from  intention  comes  from  analyses  of  itemwise variation. Since items within a pair were matched on shape, differences in the number of colors assigned within Intent No-Intent / pairs  offers  a  more  fine-grained  test  of whether the degree to which colorfulness facilitates function influences estimates of colorfulness. Difference scores were highly  correlated  across  the  sighted  samples  (ρ  =  .86, p &lt; .001) and were also correlated across sighted and congenitally blind people (ρ = .51, p = .004). Notably, the two sighted samples exhibited numerically higher agreement than sighted and blind participants, suggesting that sighted people may rely partially on visual memory when making colorfulness judgments, whereas people born blind may rely more on inferences about intention.

Finally,  we  observed  numerical  differences  in  the  color labels that blind and sighted people used across Intent and No-Intent artifacts, whereby brighter colors (e.g., blue, red, green)  were  produced  more  often  for Intent artifacts  and neutral colors (e.g., black, white) were produced more often for No-Intent artifacts (Figure 3). Differences in color label (ρ = .98, all p s &lt; .001).

comparison group (sighted reference, GPT-4), F (1,36) = 54.45, p &lt;  .001).  This  finding  suggests  that  although  GPT-4  can acquire information about artifact colorfulness from language, it does not fully capture human performance.

use  across  conditions  was  highly  consistent  across  groups                  4, and the human groups (item-wise repeated measures Like blind and sighted people, GPT-4 assigned more colors to  artifacts  for  which  colorfulness  is  intended  to  facilitate function. (Figure 2; Intent vs No-Intent artifacts t (58) = 4.56, p &lt; .001). The size of this effect was not different between GPTANOVA,  2  conditions  ( Intent , No-Intent ) x 3 groups (sighted,  blind,  GPT-4),  group  by  condition  interaction, F (2,116) =  0.27, p =  .76;  main  effect  of  group,  F (2,116) =  0, p = 1).

Together, these results suggest that people born blind infer colorfulness of artifacts by appealing to the intentions of the maker (e.g.,  fairy  tale  books  are  colorful  because  they  are intended to capture the reader's attention; Table 1).

Table 1: Example explanations from each group. For each object,  participants  were  asked,  'why  did  you  choose  that number of colors?'

|         | Fairy tale book                                                             | Instructional manual                                                           |
|---------|-----------------------------------------------------------------------------|--------------------------------------------------------------------------------|
| Sighted | The ones we own are colorful.                                               | They are usually not too colorful from my experience.                          |
| Blind   | They are usually geared toward children, and colors grab a kid's attention. | It would have to have one color to be easy to find among other booklets.       |
| GPT-4   | Most have colorful illustrations to make the stories more engaging.         | Most I've seen usually have a base color, a color for accents, and black text. |",Learning_visual_appe-with-image-refs,"The chart shows that across sighted people, congenitally blind people, and GPT-4, artifacts intended to be colorful are consistently judged to have more colors than those not intended to be colorful. Sighted and blind participants exhibit nearly identical increases in color counts for “colorfulness-intent” items, demonstrating that people born without vision use an object’s intended function to infer how many colors it likely has. GPT-4 also assigns more colors to those intent-driven items, but with a somewhat smaller overall effect, indicating that language alone can capture some—but not all—of the human pattern. These results reveal that intention-based reasoning, rather than direct sensory experience, plays a key role in predicting an object’s appearance.","The chart shows that when asked to “be colorful,” all three groups—sighted people, individuals born blind, and GPT-4—use significantly more distinct color terms than when no color emphasis is given. Sighted participants jump from only a few colors to around a dozen on average, while those without any visual experience still increase their color counts, demonstrating that color language can be learned and used without perceptual input. Remarkably, GPT-4 mirrors these human patterns, suggesting that its training captures the link between communicative intent and color-rich description. Together, these findings highlight that vivid color use is driven more by our goals in conversation than by direct seeing, and that even AI can adopt this expressive strategy."
"Power,_Privilege,_an-with-image-refs","/home/mlazar/projects/thesis/tbp_articles/social_articles/processed_articles/Power,_Privilege,_an-with-image-refs.md","/home/mlazar/projects/thesis/tbp_articles/social_articles/processed_articles/Power,_Privilege,_an-with-image-refs_artifacts/image_000000_108c0a4901f0b099f12637570fad2df6b51ee033a6809034d28bbbee15a3982e.png","## Principle 2: Fostering an acculturated climate via consciousness-raising

An inclusive intellectual climate is one that is not merely diverse but acculturated-actively resisting the gravitational pull of dominant norms (i.e., enculturation) in favor of pluralistic values. Achieving such a climate requires critical reflection on common entrenched practices and unconscious psychological mechanisms that inculcate and sustain privilege. Consciousnessraising as a means towards acculturation is therefore an elemental component of equity and inclusion.

Cultivating an acculturated environment depends in part on the presence of acculturated leadership (Dupree &amp; Boykin, 2021). However, acculturated leadership by individuals from under-represented backgrounds can challenge dominant assumptions about what constitutes effective leadership. Prevailing norms continue to favor masculinized models of authority, reinforcing narrow expectations that can devalue alternative leadership styles shaped by diverse identities and experiences (Cheryan &amp; Markus, 2020; Gruber et al., 2020). When new leaders depart from these traditional norms, frequently as an expression of their identities and experiences, the reception of their leadership becomes an inflection point for either resistance or cultural transformation. In particular, a departure from traditional norms via diverse leadership representation can introduce role incongruity, wherein leaders whose identities depart from established prototypes must simultaneously meet expectations associated with both their perceived social identity and their leadership role. When these expectations conflict, leaders often encounter heightened challenges, including prejudice and bias (Eagly &amp; Karau, 2002). For

instance, Asian women leaders are reported to adopt a relational and collaborative leadership style that emphasizes empowering others, distributing power, and leveraging collective strengths to achieve shared goals (Kawahara et al., 2013). Yet, despite its collaborative orientation, this style is often perceived as less agentic and, therefore, less competent (Rosette et al., 2018). Leadership effectiveness in such cases is greater when the surrounding environment engages with, rather than resists, alternative leadership styles (Kawahara et al., 2013).

Efforts to diversify leadership must therefore be accompanied by an institutional reckoning with the unique challenges faced by leaders from historically excluded groups, challenges often invisible and unacknowledged by department citizens and central administration. Without deliberate cultural shifts and institutional support for diverse leaders, they may find themselves on a constant quest for legitimacy, undermining both their effectiveness and retention. For example, Asian female leaders often find themselves continually challenged on issues never broached with their more traditionally positioned predecessors or peers. Continual second-guessing, microaggressions, and other subtle but pervasive forms of questioning authority contribute to fatigue, frustration, and eventual burnout (Võ, 2012). This underscores the importance of cultivating an acculturated climate that not only elevates individuals from underrepresented backgrounds to positions of authority, but also equips them with the institutional support necessary to succeed and thrive in those roles.

A parallel dynamic can be observed in everyday interactions within departments that can sustain enculturated practices. For example, it remains common for those in positions of power to support and advocate for others that share their own background characteristics, interests, or traits, reflecting homophily bias (Stewart &amp; Valian, 2018). This tendency to afford opportunities to like-minded individuals fosters a homogeneous and enculturated community rather than a

diverse acculturated departmental community, where conformity to a dominant identity becomes the primary criterion for inclusion, often at the expense of merit (van den Brink &amp; Benschop, 2014). The establishment of heterophilic networks requires department citizens to recognize and intentionally disrupt homophilic tendencies. This demands from individuals a true commitment to restorative sacrifice, acts that may entail personal cost, but ultimately serve to advance the equity agenda of a department. In this regard, naming and normalizing restorative sacrifice can be a valuable step towards equity of opportunity.

As academic environments diversify, so too must their culture and climate. An inclusive academic climate necessitates a shift from enculturation, which reinforces dominant norms and pressures individuals to assimilate into the existing status qu o, toward acculturation, which embraces pluralism and values diverse cultural contributions. True inclusion requires broadening institutional norms and expectations to accommodate identities and experiences that may diverge from established precedents. The extent to which these departures are acknowledged and supported critically shapes the potential for genuine acculturation. Moreover, consciousnessraising via explicit recognition of implicit cognitive bias (e.g., homophily; social stereotyping; implicit associations) can illustrate how existing norms can favor sameness and limit diversification.

Principle 3. Establishing the preconditions for internal alignment and cultural change

While universities are engines of innovation and intellectual progress, the pace of institutional change within academia can be notoriously slow. As astutely noted by Brian Rosenberg, 'Look at any mission statement for any college or university, and you will probably find a word like transformational or transformative. And look at the work of any faculty member in any discipline, and they will tell you that they're trying to push the boundaries of their

discipline and change things. But when it comes to the way these institutions operate, there is, in fact, a powerful resistance to, reluctance to, opposition to change' (Anderson, 2023). Rosenberg provides examples of how particular structures, such as shared governance and tenure systems, can have the effect, if not the intent, of incrementalizing change. In doing so, these systems can limit transformative change (Rosenberg, 2023). As a result, gradualist approaches to change based on a strong commitment to the status quo can take precedence over the kind of transformative change needed for cultural change. Cultural change is inherently complex and often elusive, owing to the intangible nature of culture itself. Institutional cultures, like human cultures, are a product of shared values, norms, and belief systems that shape daily practices and interactions, often operating beneath the level of conscious awareness or observable behavior. As such, changing culture presents a particularly challenging endeavor.

A substantial body of psychological research has focused on developing theories of cultural change at an evolutionary scale to explain how cultural patterns emerge, persist, and adapt within human societies. These theories not only describe cultural changes, but seek to explain why behaviors emerge, how they are transmitted, and how they become entrenched over time, even when they are maladaptive. Theories of cultural evolution account for how behaviors are reinforced or extinguished through subtle ecological and social selection pressures (Boyd &amp; Richerson, 2005; Lumsden &amp; Wilson, 1981; Varnum &amp; Grossmann, 2017). Understanding cultural evolution thus requires careful attention to the origins, persistence, and adaptability of behavior over time. Analogously, understanding organizational culture entails a similar analysis of how institutional norms are historically produced, socially reproduced, and adaptable to contextual pressure. The following section outlines three preconditions necessary to support inclusive and equity-oriented cultural transformation within academic departments.

A first precondition is cultivating a department-wide commitment to change . As organizational theorists note, transformational change requires broad-based recognition of both the need for change and strategies to overcome barriers to achieving it (Kotter, 1996). Within academic departments, however, such recognition is rarely uniform; disunity around change is to be expected. Resistance is especially pronounced when proposed reforms threaten the power, privilege, or autonomy of dominant actors-those who possess the most institutional capital, enjoy structural protections, and exert significant influence over decision-making processes (Kezar, 2014; Rosenberg, 2023). In particular, long-standing departmental members who have historically benefited from unexamined privilege often emerge as the most persistent opponents of cultural change that threatens their personal or professional interests, though their resistance is frequently tacit, operating subtly beneath the surface.

Academic systems have long demonstrated that cultural change is often resisted by those who benefit from the existing structures of power and privilege. A quintessential example of this is the introduction of external peer review, a process now regarded as a basic standard, but in its modern form, is a relatively recent development. Peer review was first introduced in 1665 by the Royal Society in London, intended to facilitate communication among a small, select group of elite scientists through letters, commentaries, and statements. Over time, this evolved into a system in which members of these self-organized scientific communities, serving as journal editors, would evaluate each other's scientific contributions and determine their suitability for publication. For example, Watson and Crick's famous 'double-helix' paper was deliberately withheld from external peer review because the Editor considered its correctness to be 'selfevident,' despite the authors themselves acknowledging that their model 'must be regarded as unproved until it has been checked against more exact results' (Watson &amp; Crick, 1953, p. 737).

As conceded by a later Editor of Nature, sending the submission for external peer review would have revealed that the foundational data belonged to a female colleague, Rosalind Franklin, whose critical contributions were minimized in this highly celebrated work (Baldwin, 2015a).

This exclusive model remained largely intact until the mid-20th century, when journals began issuing open calls for submissions and sending papers to external experts for peer review prior to publication. Notably, when prestigious journals adopted this new model, they faced considerable opposition and criticism from prominent scientists which protracted the time required to institutionalize this change (Baldwin, 2015b). Detractors of external peer review (which included Albert Einstein, who withdrew his own submission to Physical Review , objecting to the Editor's decision to send it outside experts for evaluation) argued that the existing system, which openly privileged those with influential personal and professional networks, was effective and thus did not warrant reform.

In today's world, such open and flagrant resistance to inequitable practices would likely be regarded as socially and professionally unacceptable. However, while the outward expression of opposition has shifted, the underlying drive to maintain inequitable practices can persist in more socially palatable forms. Today, common defenses of selective privilege within academic departments include appeals to institutional precedent (e.g. 'This is how things have always been""), concerns about destabilizing departmental operations, assertions of individual exceptionalism to justify special treatment, strategic delays, or situationist arguments that frame a faculty member's personal or professional circumstances as unique. Akin to detractors from external peer review, resistors are typically institutionally and structurally empowered beneficiaries of selective privilege in the academy, both historically and contemporarily.

In considering how to address subtle defenses of selective privilege, it is helpful to delineate the different types of inequity that give rise to selective privilege as this delineation has direct consequences for the ease and expediency with which inequities can be corrected. At one extreme, sources of structural inequity are particularly complex to correct as they often emerge as unintended consequences of longstanding institutional priorities and practices that reproduce systemic unfairness. These inequities are cumulative, diffuse, and can be difficult to define, rendering them resistant to rapid correction (Haslanger, 2023). By contrast, procedural and distributive inequities are typically more visible and measurable (Hartman et al., 1999), making them easier to expediently correct. These may include disparities in teaching assignments, variation in service workloads, or inconsistencies in the application of tenure and promotion criteria. Recognizing such inequities without taking immediate corrective action undermines departmental and institutional commitments to basic fairness and accountability.

Determining a department's commitment to change requires open engagement amongst its citizens. This necessitates truly inclusive channels of communication where the viewpoints of all members can be represented. Academic departments, like many organizations, often grapple with participatory inequity as it relates to dominance, wherein influence is disproportionately afforded to the most structurally empowered individuals. A routine expression of this at the departmental level is participation behaviors in faculty meetings. A simple audit of who speaks and who remains silent during faculty meetings reveals participation inequities that are often noticed, but seldom addressed through systematic intervention (Edmondson, 1999; Morley, 2013). As noted earlier, members of hierarchical institutions internalize beliefs about when and how it is safe to speak up (Detert &amp; Edmondson, 2011). These beliefs are unevenly distributed: marginalized faculty are more likely to perceive risks in expressing dissenting views, particularly

when advocating for equity-related change, which can be mistakenly construed as being driven by self-interest rather than by principle. Thus, examining participatory behaviors around discussions of change and committing intentional efforts to broaden participatory dialogue must also address the structural conditions (e.g. status hierarchies; norms of deference) that limit full engagement.

A second precondition is structural reinforcement of desired change . Culture change cannot be sustained without an alignment of systems of rewards, accountability mechanisms, and measurement of outcomes. Misaligned reward structures can incentivize behaviors that directly contradict institutional goals (Kerr &amp; Slocum, 1987). Within academia, individuals in positions of power may be tacitly rewarded for maintaining the status quo , regardless of the equity implications of their conduct (Fazackerley, 2025). This misalignment perpetuates maladaptive behaviors and shields those with privilege from scrutiny and accountability. For change to be effective, reward structures must be calibrated to goals, and systems of accountability must be introduced to support these new reward structures (Bushardt et al. 2011).

Connected to structural reinforcement is the need for an appraisal of the cost of change, both tangible and intangible. Organizational change often demands significant time, cognitive effort, and academic labor from those involved. Even ostensibly simple changes, such as transitioning to new digital platforms or introducing new operational procedures, can impose burdens that contribute to change fatigue or disengagement. Culture change, which involves addressing deeply held behaviors as they connect to values, carries an even greater risk of change fatigue. This fatigue can be particularly pronounced among those who are already overextended, such as marginalized faculty who are frequently called upon to do diversity labor without institutional recognition (Dancy &amp; Hodari, 2023). Successful transformation also therefore

requires the allocation of adequate resources-whether material, informational, or relational-to support those tasked with leading and enacting change.

Finally , mechanisms for measurement of change are a crucial, yet frequently underdeveloped, precondition for institutional transformation (Kezar, 2014; Eckel &amp; Kezar, 2003). Establishing clear, measurable goals with systems of accountability provides a shared reference point for assessing progress towards desired change (Bess &amp; Dee, 2008). Academic strategic plans often impose rigid timelines for change that fail to account for the unpredictable and fluid nature of culture change for which a chronology is often genuinely hard to predict. While some changes can be implemented relatively quickly (e.g., telework policies; committee structures), others, such as shifting departmental mindsets or building trust among colleagues, unfold over longer periods and require an ongoing commitment (Kezar, 2014). As such, evaluation frameworks must strike a balance between ambition and pragmatism. They should include both process indicators (e.g., participation patterns, qualitative feedback) and outcome indicators (e.g., retention patterns, diversity in faculty representation), while allowing for adaptation in response to new insights or barriers that emerge along the way (Kotter, 1996). Culture change is not a linear endeavor, and its success hinges on a shared implementable vision of what inclusive department life can and should look like.

Good intentions, uncertain outcomes: Navigating the complex terrain of equity implementation The aforementioned orienting principles for change offer a conceptual foundation for a more inclusive Psychology department (see Figure 1 for a graphical summary). However, there are challenges and threats that complicate the implementation and realization of these principles.

Figure 1. A graphical map of orienting principles towards inclusive excellence.

As is evident from the statistics on faculty hiring and retention in Psychology, the pace of change has been troublingly slow. A lack of change is not necessarily a product of apathy or negative intent; rather, it can reflect the structural and cultural complexities of academic institutions. Shifting departmental norms and re-establishing institutional priorities is profoundly challenging work. One contributing factor to gradualist change in academia is the nature of tenure, which is designed to support long-term employment and academic freedom. As a result, academic careers within departments are often significantly longer than those within other sectors. Moreover, the agency that tenured positions grant to faculty cuts both ways. On one hand, it empowers faculty to champion institutional change. On the other, it enables them to disengage entirely from almost any institutional agenda. This means that institutions rely heavily

on generational turnover, rather than more frequent cycles of hiring and leadership renewal, to produce systemic change. Ironically, those best positioned to implement change are those in administrative leadership. These individuals are structurally empowered, but for this very same reason, they may also be furthest removed from the lived experiences (and identities) of those most affected by inequities. This positional distance can limit their capacity to fully recognize or respond to specific needs for change.

Considering these structural constraints, one of the most promising avenues for long-term transformation lies in training the next generation of graduate students. As future faculty, researchers, teachers, and institutional leaders, graduate students shape the norms and values that will define the academic enterprise in the years to come. Equipping them with an understanding of structural power, privilege, and positionality fosters critical consciousness around these potent forces, encouraging them to examine how their own social identities influence both their scholarship and their roles within institutional hierarchies. Approaching this with intentionality can disrupt the reproduction of inequity by providing tools and strategies to identify and address systemic and structural disparities. Ultimately, this approach not only diversifies the future leadership pipeline, but also ensures that those who rise to positions of influence are empowered and equipped to challenge exclusionary practices and create more inclusive academic environments.

In general, the implementation of equity initiatives is rarely straightforward. Addressing structural inequities requires engaging with a deeply complex and multi-dimensional problem, one that spans every facet of academic life. Change therefore demands not only precise diagnostics, but also the design of actionable, well-sequenced interventions that are sustainable over time. Even with these supports, the most well-intentioned initiatives can falter. Ultimately,

acknowledging the inherent difficulty and slow pace of institutional transformation does not excuse inaction; rather, it underscores the need for sustained, strategic, and well-supported approaches to equity-driven reform.","Power,_Privilege,_an-with-image-refs","The chart lays out three interconnected approaches—alignment, awareness, and analysis—that guide departments from lofty equity goals to concrete practices by aligning policies, fostering an inclusive culture, and setting up conditions for change. Alignment ensures ideals, norms, and organizational policies reinforce each other, promising accountability and justice while warning of risks like unequal burdens on underrepresented faculty and skewed participation. Awareness cultivates an environment that values diverse cultural styles and voices, yet requires active interruption of hidden biases and resistance to shifting power. Analysis establishes clear commitments, reward structures, and measurable goals to sustain transformation, even as it anticipates pushback against losing entrenched privileges. Together, these pillars generate new equity-driven norms, a culture that celebrates variation, and a critically engaged community equipped for ongoing change.","The chart shows that embedding inclusive excellence in a department hinges on three linked practices—aligning ideals and policies, raising awareness of cultural dynamics, and analyzing the conditions for change. Each practice brings clear benefits (promises) like stronger accountability, a richer cultural climate, and measurable progress, alongside risks such as burnout, unconscious biases, and resistance to power shifts. By proactively addressing these threats—through policy reconciliation, targeted training, and transparent evaluation—departments can cultivate equity-driven norms, affirm diverse perspectives, and sustain transformational change. Ultimately, this integrated approach guides leaders to build accountable, inclusive cultures that respect variation and maintain momentum toward justice."
Strong_primary_cue_w-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/social_articles/processed_articles/Strong_primary_cue_w-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/social_articles/processed_articles/Strong_primary_cue_w-with-image-refs_artifacts/image_000010_09b1daea87105243f16895c9713d0841883c6b36ae792d925ae583304e0fc603.png,"## L1 English speakers

Figure  5. Scatterplots  displaying  the  relationship  between  weighting  of  the  primary  (left)  and secondary (right) cues to prosodic features, as compared with dimension-selective attention ability. Attention performance was collapsed across verbal and non-verbal subtests. L1 English speakers are displayed on the top of the graph, with L1 Mandarin speakers on the bottom. Points are colored red for primary and blue for secondary cues to match Figures 1 through 4.

One  interpretation  of  the  finding  that  performance  on  the  dimension-selective  attention  test correlates with primary cue weighting is that individuals who can consistently attend to the most useful cue  for  a  given  speech  feature  will  develop  perceptual  strategies  which  strongly  utilize  this  cue. However, an alternate interpretation is that the dimension-selective attention test draws on low-level auditory processing, which also facilitates development of optimal perceptual strategies. To attempt to distinguish between these two explanations, we used linear regression in R to determine the extent to which dimension-selective attention and auditory discrimination skill explain independent variance in primary cue weighting. We also included age and gender to ensure that these results hold even when accounting for demographic variables. Discrimination was averaged across all subtests (pitch and duration  discrimination).  The  regression  equation  for  both  groups  was  [primary\_weighting  ~ discrimination + dimension-selective attention + age + gender]. For the L1 English speakers, the model predicted 33.4% of the variance (F(4,49) = 6.16, p &lt; 0.001). Attention was the only significant predictor

(  = 0.64, F = 23.0, p &lt; 0.001): individuals with better dimension-selective attention showed greater weighting  of  the  primary  cue.  For  the  L1  Mandarin  speakers,  the  model  predicted  21.4%  of  the variance (F(4,52) = 3.53, p = 0.013). Attention was the only significant predictor (  = 0.39, F = 10.77, p =  0.002):  individuals  with  better  dimension-selective  attention  showed  greater  weighting  of  the primary cue.

To  illustrate  in  more  detail  how  categorization  differs  between  individuals  with  good  versus  poor dimension-selective attention, Figures 6 and 7 show how responses vary with pitch and duration level in  these two groups across the three speech perception tasks. Good and poor dimension-selective attention groups were defined as the top versus bottom terciles of performance. In both L1 English and Mandarin speakers, for word emphasis and stress categorization, the function relating responses to pitch level (the primary dimension for these features) is steeper in individuals with good dimensionselective attention. However, for phrase categorization, the function relating responses to duration level (the primary dimension for this feature) is steeper in individuals with good dimension-selective attention.

Figure  6 .  Categorization  responses  across  three  speech  features  in  L1  English  speakers.  Error  bars indicate one standard error of the mean. The good dimension-selective attention group consists of the third  of  the  participants  who  performed  best  across  all  attention  subtests,  while  the  poor  group consists of the bottom third.

L1 Mandarin speakers

Figure 7. Categorization responses across three speech features in L1 Mandarin speakers. Error bars indicate one standard error of the mean. The good dimension-selective attention group consists of the third  of  the  participants  who  performed  best  across  all  attention  subtests,  while  the  poor  group consists of the bottom third.",Strong_primary_cue_w-with-image-refs,"The chart reveals that native English listeners with strong dimension-selective attention exhibit a markedly steeper rise in choosing “study MUSIC” as duration increases, compared with listeners who have poorer selective attention. This steeper response function shows that high-attention individuals rely more heavily on the primary prosodic cue—duration—when identifying phrase boundaries. In contrast, the low-attention group’s shallower slope reflects less consistent weighting of duration differences. These divergent profiles underscore that dimension-selective attention ability directly shapes the development of effective perceptual strategies for prosodic feature weighting. Together, the data suggest that enhancing selective attention could help listeners tune into crucial acoustic cues more reliably.","The chart shows how the proportion of participants choosing to study music rises as the allotted duration increases, with two different conditions indicated by red and gray lines. In both cases, the share climbs from just under 50% at the shortest interval to just over 55% at the longest, revealing that giving people more time consistently encourages more music study. The red condition maintains a higher uptake than the gray at every duration, suggesting it’s more effective at promoting engagement. Even though the error bars overlap slightly at lower durations, the clear upward trend and persistent gap between lines indicate that both longer exposure and the factors represented by the red line reliably boost the likelihood of studying music."
The_Humble_Self-Conc-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/social_articles/processed_articles/The_Humble_Self-Conc-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/social_articles/processed_articles/The_Humble_Self-Conc-with-image-refs_artifacts/image_000001_83c58463113c9306c3366a7bb548b48ce15dcf84d05a3c9e9ddac11d99251af0.png,"## HSCM: Resilient Self-Belief Framework

who argue it conflates moral obligation with practical feasibility. For instance, Sinnott-Armstrong posits that obligations can exist in moral dilemmas even if impossible to fulfill, while empirical studies suggest laypeople sometimes endorse ""ought"" without ""can."" To avoid confusion between philosophical debates and psychological applications, HSCM employs OIC probabilistically and pragmatically in psychological contexts, where it empirically aligns with harm mitigation by emphasizing that individuals only have access to their conscious experience, not the unconscious processes shaping it, and cannot actively enhance their agency without a felt sense of safety in their internal self-relationship. This safety requires an always-accessible intrinsic self-worth, deserved baseline esteem, and justification for self-care and compassion. Thus, understanding OIC in this framing means that, in any given moment, especially within a seemingly deterministic reality, past states and human limitations are to be accepted without self-harm, treating self-correcting painful feelings (e.g., guilt or shame) as informative data for growth rather than allowing them to perpetuate conditioned cycles of internal harm, such as negative self-talk. This adoption is soundly premised on its utility for fostering self-forgiveness and resilience, remaining provisional and open to revision based on counter-evidence from psychological research.

- Principle 3: Universal Attempt as Virtuous Engine - Locates worth in the continuous, imperfect effort to persist and seek good, creating resilience, truth-seeking, and harm mitigation.

From these principles, the hypothesis derives that every human possesses unconditional worth as a meta-ethical foundation, independent of performance. This moral claim, that worth is intrinsic and tied to the universal attempt to persist, does not automatically confer psychological benefits; rather, it hypothesizes that endorsing this view fosters resilience through reduced shame and enhanced adaptability. This psychological link requires empirical testing, such as via mediators like shame reduction in RCTs. The conclusion: 'Every human is unconditionally worthy by virtue of existing in a state of continuous, imperfect effort, deserving steady self-esteem and full self-compassion' (Gopoian, 2023). This refutes conditional systems as incoherent and promotes a virtuous cycle: resilience enables truth-seeking, which mitigates harm. Individuals have access

only to their conscious experience, not the full unconscious processes shaping it, and cannot meaningfully enhance their agency without a secure internal self-relationship. This security demands an ever-present sense of intrinsic worth, baseline esteem, and justified self-compassion. Thus, self-forgiveness for human limitations and ignorance requires grasping that ""Ought implies can"" implies acceptance of past states, particularly in a seemingly deterministic reality, without self-inflicted harm, viewing corrective emotional signals as informative data rather than perpetuating conditioned cycles of internal harm, such as negative self-talk.

Figure 2: Guiding Philosophical Hypothesis of Unconditional Worth illustrates the axiomatic structure, with the prime axiom as the root, branching into principles, rules, and concluding hypothesis.

Guiding Philosophical Hypothesis of Unconditional Worth

Prime

Axiom

Humility emerges as the rational stance: Embracing fallibility without self-denigration. This axiomatic approach conceptually immunizes HSCM against relativism or nihilism, offering a meta-ethical rationale that integrates with the 10 steps (e.g., Step 2 operationalizes the proof as a daily mantra).",The_Humble_Self-Conc-with-image-refs,"The chart lays out how prioritizing harm reduction and long-term well-being gives rise to a theory of unconditional human worth. It starts from two core ethical considerations—respecting people’s real capacities and ensuring fair, equal treatment—and then rejects any system that ties worth to performance. By highlighting everyone’s ongoing, imperfect efforts as inherently valuable, it shows how resilience fuels honest reflection and lowers shame. These insights converge into the practical conclusion that steady self-compassion, grounded in intrinsic worth, forms a foundation for durable well-being. Adopting simple daily reminders of inherent value can break cycles of self-criticism and build sustainable inner strength.","The chart outlines a moral framework built on three pillars—recognizing human limits, treating like cases alike, and committing to an imperfect but persistent effort to do good—that together prioritize harm mitigation and long-term well-being. By rejecting conditional worth and extrinsic rewards, it shows how empathy and intrinsic motivation create a virtuous cycle of resilience and truth-seeking. This cycle feeds back into ever-stronger harm reduction and personal growth, leading to the hypothesis that every individual possesses unconditional worth. The framework’s actionable insight is clear: policies, cultures, and personal habits should emphasize fair treatment, intrinsic values, and ongoing virtuous effort to cultivate self-compassion, steady self-esteem, and humility as practical imperatives."
The_Myth_of_Publicat-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/social_articles/processed_articles/The_Myth_of_Publicat-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/social_articles/processed_articles/The_Myth_of_Publicat-with-image-refs_artifacts/image_000000_fb8e71eb5cbe2876ae94fecf1cd61289bdc680dea4b2e3eb0a0e6d25a1cca7b0.png,"## The File-Drawer Problem

Even a suggestively low publication bias does not prove the case for ESP, though it does mean the field of parapsychology is more open to reporting all outcomes. The file-drawer problem (explained in the Introduction) must therefore still be considered a key concern to the field . Critics may argue that sufficient numbers of non-significant studies may still remain unpublished, and if

they had been published and included in the meta-analyses, the significant overall results would shrink to non-significance. We argue that past research on this problem provides little evidence for the critic's argument:

96

Figure 1. Percentage of positive statistical outcomes, observed in non-preregistered experiments. In parenthesis the number of studies examined.

Figure 2. Percentage of positive statistical hypotheses observed in preregistered studies.",The_Myth_of_Publicat-with-image-refs,"The chart shows parapsychology publishes significant results only about one-third of the time (32.5%), while three mainstream psychology surveys report positive findings in roughly two-thirds to over 95% of published studies. This contrast suggests parapsychology is comparatively open to null outcomes, making it unlikely that a cache of unpublished negative ESP experiments could overturn its meta-analytic conclusions. Mainstream psychology’s consistently high positive rates point to stronger publication bias and underscore the value of preregistration and dedicated null-result outlets across disciplines. Overall, concerns about hidden nulls in parapsychology should shift toward strengthening methodological rigor rather than suspecting unseen negative evidence.","The chart reveals that studies in parapsychology report positive outcomes far less often—about one third of the time—than three major surveys of mainstream psychology, which range from roughly two-thirds to nearly all positive findings. This stark contrast suggests that mainstream psychology literature is heavily skewed toward publishing positive results. In contrast, parapsychology’s lower positive rate might reflect either a more balanced approach to reporting or a genuine scarcity of reproducible effects. Bridging this gap will require both fields to register research plans in advance, encourage the publication of null findings, and invest in replication efforts. By normalizing transparent reporting, researchers can build a more trustworthy evidence base across disciplines."
