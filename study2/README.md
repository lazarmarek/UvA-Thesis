# Study 2: Evaluating Local VLMs and Context-Enhanced Multimodal RAG

This study has two primary objectives:
1.  **Evaluating a Local VLM:** To assess the chart interpretation capabilities of a smaller, locally-deployable Vision Language Model (`InternVL3.5-1B`). This serves as a practical, cost-effective alternative to large, API-based models.
2.  **Context-Enhanced RAG:** To investigate whether chart interpretations generated *with* surrounding textual context improve retrieval performance in a multimodal RAG system compared to interpretations generated *without* context.

## Project Overview

This study **does not use the chart interpretations generated in Study 1**. Instead, it uses the dataset of images and their corresponding textual contexts from Study 1 as a starting point.

The pipeline is as follows:
1.  **Interpretation Generation:** Using a local VLM (`InternVL3.5-1B`), we generate two new sets of interpretations for each chart: one with its surrounding text context and one without.
3.  **Query Generation:**  For each chart, the researcher manually generated relevant questions based on the chart images and their respective contexts (`querymaker.py`).
2.  **Indexing:** Two separate ChromaDB vector store collections were created using these newly generated interpretations.
    *   **Collection A (without_context):** Indexes the "Image-only" interpretations from `InternVL3.5-1B`.
    *   **Collection B (with_context):** Indexes the "Image & Context" interpretations from `InternVL3.5-1B`.
4.  **Retrieval & Evaluation:** We run the generated queries against both vectorstores to retrieve the top-k relevant documents and compare the results to determine which set of descriptions performs better.

## File Tree

```
study2/
├── config/
│   └── dev_message.txt         # System prompt for the VLM
├── data/
│   ├── processed/
│   │   └── img-context-df.csv  # Input data from Study 1 (images, contexts, descriptions)
│   └── raw/
│       └── queries.json        # Manually generated queries via `querymaker.py`
├── notebooks/
│   └── demo.ipynb              # Jupyter notebook demonstrating the experiment & results
├── outputs/
│   └── all_responses.csv       # Responses generated by the VLM
├── src/
│   ├── __init__.py
│   ├── querymaker.py           # Script to generate display charts & contexts and save queries
│   ├── embeddings/
│   │   └── embeddings.py       # Logic for creating embeddings
│   ├── ingestion/
│   │   └── docloader.py        # (Optional) Scripts for loading and preparing documents for indexing 
│   └── retrieval/              # (Placeholder for retrieval and evaluation logic)
├── pyproject.toml              # Python project configuration file
└── requirements.txt            # Python package dependencies
```

## Setup

1.  **Create and activate a virtual environment:**
    ```bash
    python -m venv .venv
    source .venv/bin/activate
    ```

2.  **Install dependencies:**

    ```bash
    pip install -r requirements.txt
    ```

3.  **Set up environment variables:**
    Create a `.env` file and add any necessary API keys (e.g., `HF_API_KEY`)

## Usage

The primary workflow is demonstrated in `notebooks/demo.ipynb` and orchestrated using the scripts in the `src/` directory.

1.  **Generate Interpretations:** The first step is to use the local VLM (`InternVL-1B`) to generate new chart interpretations from the `data/processed/img-context-df.csv` file. This creates the core text data for the RAG system.
2.  **Generate Queries (Optional):** The `src/querymaker.py` script can be run to display chart images and queries to help create the queries manually, which are saved to `data/raw/queries.json`.
3.  **Create Embeddings & Index:** Logic from `src/embeddings/` is used to generate embeddings for the new text descriptions. This data is then indexed into two separate ChromaDB collections (`vector_db/`)—one for "with context" and one for "without context" descriptions.
4.  **Run Demo/Evaluation:** The `notebooks/demo.ipynb` notebook can be used to load the vectorstores, run queries from `data/raw/queries.json`, and perform a retrieval comparison between the two RAG setups.