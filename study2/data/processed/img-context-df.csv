article_name,md_path,image_path_1,image_context_1
2507.08637v1-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/arxiv_articles/processed_articles/2507.08637v1-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/arxiv_articles/processed_articles/2507.08637v1-with-image-refs_artifacts/image_000000_23c6d3101ca79aafafc9409dd2c6dda83fb6cdd6d7ec3e3324772485f94f9e49.png,"## 3.1 WERSA Algorithm

Let queries Q ∈ R n × d , keys K ∈ R n × d , and values V ∈ R n × d , the WERSA mechanism is as follows:

1. Linear Projection: Computes projected queries, keys, and values:

2. Multi-Head Splitting: Divides Q K ′ , ′ , and V ′ into h heads and reshape them to R n × × h d h , where d h = d /h k .
3. Wavelet Decomposition: Performs the wavelet transform (for simple implementation, it has been used the Haar wavelet transform see Appendix A) on Q ′ and K ′ for L levels:

where W generates a sequence of wavelet coefficients at various scales. It is important to state that the method is generic and thus Haar can be substituted with other types of wavelet decomposition such as Daubechies, symlets, etc.

4. Adaptive Wavelet Filtering: One of WERSA's innovations includes adaptive wavelet filtering. The average query representation for a specific head h can be computed as:

A neural network represented by g then maps Q ′ avg ,h to filter coefficients:

where σ denotes the sigmoid function,yielding coefficients in the [0 , 1] range. The coefficients are then applied element-wise to each respective wavelet coefficient acting like a filter. Additionally, each wavelet scale is modulated by a learnable parameter:

where ω i controls the importance of the i -th wavelet scale. Prior wavelet-based transformers [Zhuang et al., 2022] process all wavelet scales equally, which is suboptimal for sequences with heterogeneous frequency components. WERSA's gating mechanism ω i learns to suppress noisy high-frequency scales or enhance low-frequency global patterns, depending on the input.

5. Filtered Wavelet Representation: Multiplies the wavelet coefficients by the corresponding filter coefficients:

6. Unified Multi-Scale Reconstruction: WERSA efficiently reconstructs filtered signals through a single operation:

where W -1 represents the inverse wavelet transform of the filtered coefficients.

7. Random Feature Projection: To mitigate computational complexity, approximations with random features are utilized to approximate the softmax kernel. This is done by using a map ϕ x ( ) :

and specifically, a ReLU-based random feature map is used:

with R randomly drawn from R d h × m . The projection is enhanced with a trainable bandwidth parameter β :

where R can be initialized as orthogonal or Gaussian matrices. This yields the linear attention approximation:

8. Attention Computation: Compute attention with the filtered signals:

where ϵ is a small constant aiming at ensuring numerical stability.

9. Multi-Head Combination: Concatenates the outputs of all heads and project them:

A pictorial representation of the operations flow is depicted in Figure 1.

In order to increase transparency and reproducibility the WERSA pseudocode can be found in Appendix D while the Hugging Face Transformers compatible implementation will be released upon acceptance.

Figure 1: Wavelet Enhanced Random Feature Self-Attention (WERSA) architecture. From left to right: Input Processing (gray/blue), Wavelet Processing (orange), and Attention Mechanism (green)."
2507.08665v1-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/arxiv_articles/processed_articles/2507.08665v1-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/arxiv_articles/processed_articles/2507.08665v1-with-image-refs_artifacts/image_000001_468b541f690f83514b5020ab075d945df8c5dffd9287913fc2bb0b8612b156a0.png,"## 2.2. Autoformalization

Autoformalization constitutes a specialized machine translation task that transforms natural language statements into formal representations while preserving semantic content and complying with target syntax requirements. Initial investigations explored neural approaches like (Wang et al., 2018; Cunningham et al., 2023), demonstrating the feasibility of this paradigm.

Current research on LLM-based autoformalization primarily follows two dominant approaches: (1) few-shot incontext learning (Wu et al., 2022; Patel et al., 2023; Zhou et al., 2024), and (2) fine-tuning LLMs on NL-FL pairs (Lu et al., 2024a;b; Gao et al., 2025). While the latter has shown promising results with 96 % (pass@128) accuracy in MiniF2F (Zheng et al., 2021), performance drops to just 16 %(pass@128) on the College CoT benchmark, revealing the critical limitation of NL-FL data scarcity.

Aparallel research direction addresses the more challenging task of formal proof generation, where natural language proofs often diverge substantially from their formal counter-

Figure 2. Overview of Our Method. (a) Data Collection . We gather problems from various sources, including online resources and exercise sets, and construct an ontology library of relevant concepts and theorems. Through filtering and data synthesis strategies, we obtain a natural language (NL) corpus. (b) Semantic Parsing . We employ the KELPS model to perform semantic parsing, translating natural language problems into knowledge equations. The initial iteration of data is obtained through annotation. (c) Syntax Validation . The knowledge equations generated in (b) are validated by the AL Parser. Problems that pass validation are then converted into other formal languages via rule-based transformation. (d) Semantic Validation . Data that passes the compiler validation in the previous stage undergoes semantic review by both LLMs and human experts. Finally, the verified data is incorporated into the dataset, which is then used to continuously train the baseline model.

parts. Current approaches include: (1) proof decomposition into draft skeletons with subsequent completion (Jiang et al., 2022; Wang et al., 2023), and (2) direct neural translation of informal proofs (Wang et al., 2024; Shao et al., 2024). The first approach uses language models to complete proof steps within a structured framework, whereas the second aims for complete automated translation.

The alternative paradigm (Wu et al., 2024; Gao et al., 2025) initiates from formal language corpora, employing LLMs for backward translation to natural language. While valuable, this approach remains fundamentally constrained by the scope and completeness of existing formal libraries."
2507.08716v1-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/arxiv_articles/processed_articles/2507.08716v1-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/arxiv_articles/processed_articles/2507.08716v1-with-image-refs_artifacts/image_000003_4296db732d14ec712519c017680de3b4de17b6abd86a53cb8036b137372cf9cb.png,"## II. THE GREAT-X: ALL-IN-UE MULTIMODAL SIMULATOR

The constructed simulation platform reconstructs the electromagnetic process of the ray tracing channel simulation in the Unreal Engine, thereby enabling the implementation of the ray tracing algorithm based on ray tracing in high-fidelity scenarios. The following mainly introduces the reflection, refraction, and the scattering parts. The following formulas are referenced from SionnaRT [13].

During the propagation of electromagnetic waves, when a plane wave is incident on the planar interface between two different media, due to the difference in medium properties, part of the wave will be reflected at the interface, while the other part will pass through the interface and enter the second medium; that is, transmission or refraction occurs. For analysis, this paper assumes that both media are uniform and non-magnetic dielectrics and adopts the parameter definitions in ITU recommendations [14]. The complex amplitude vector of the incident wave's electric field, E i , can be represented by two arbitrary orthogonal polarization components:

Fig 3 illustrates the reflection and refraction phenomena that occur when electromagnetic waves are incident on a planar interface between two materials with relative dielectric constants of η 1 and η 2 . For ease of analysis, the coordinate system is chosen such that the wave vectors of the incident wave, reflected wave, and transmitted wave are all located in the same incident plane, which is defined as the x z -plane. At the same time, the normal vector ˆ n of the interface is oriented along the negative z axis. Due to the different polarization characteristics, the incident wave needs to be expanded in a set of specific bases, specifically expressed as two mutually orthogonal polarization components E i, ⊥ and E i, ∥ , that is:

Among them, the component perpendicular to the incident plane is called transverse electric (TE) polarization (on the left side of the figure). In contrast, the component parallel to the incident plane is called transverse magnetic (TM) polarization (on the right side of the figure). In the subsequent discussion, it is uniformly agreed that all the transverse vector components point outward from the plane of the figure. Based on the above definition, it can be directly verified that the following relationships must be satisfied:

The incident electric field phasor in terms of its TE and TM components can be expressed as:

The phasors of the reflected wave and the transmitted wave E r and E t are also expressed in a similar manner:

The corresponding unit vectors for the reflected and transmitted waves are defined as follows:

Fig. 3: Reflection and refraction of a plane wave at a plane interface between two materials [13].

Combining all the information, we obtain the following relationship among the incident wave, the reflected wave, and the transmitted wave:

In the phenomenon of scattering, when an electromagnetic wave is incident on a surface, some of the energy is reflected. At the same time, the other part penetrates the surface and enters the medium interior, forming a refracted wave. We usually classify reflection into two types: specular reflection and diffuse reflection. The former has been analyzed in the discussion of reflection and refraction, while this section will focus on the latter, namely diffuse reflection. Unlike specular reflection, when an electromagnetic wave is incident on a surface with diffuse reflection properties, the energy does not concentrate and reflect in a single direction, but is scattered in multiple directions outward. Considering that most actual surfaces will simultaneously produce specular reflection and diffuse reflection, we use S 2 to represent the energy of the diffuse reflection part, where S ∈ [0 , 1] is the scattering coefficient; correspondingly, R 2 represents the energy of the specular reflection part, where R ∈ [0 , 1] is the specular reflection attenuation factor. The relationship between the two is as follows:

Consider the scenario depicted in Fig 4, where at the scattering point q , there exists a plane incident wave with local linear polarization, and its electric field vector is E q i ( ) . Our focus is on the scattered field generated by this incident wave on the scattering direction ˆ k s , through an infinitesimal surface element dA . It should be noted that the surface normal vector ˆ n is in any direction in the global coordinate system, and the ( x, y, z ) coordinate axes indicated by the green dotted lines in the figure are used to assist in the explanation. The incident field vector can be expressed as two orthogonal polarization components perpendicular to the incident wave direction ˆ k i :"
2507.08723v1-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/arxiv_articles/processed_articles/2507.08723v1-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/arxiv_articles/processed_articles/2507.08723v1-with-image-refs_artifacts/image_000008_db51379aed090643ae5c5df8c474e036c1b620583c457f1b98fef5b2cd6229a6.png,"## 2. Water-Cherenkov Detectors

The Water-Cherenkov Detectors form the core of the Surface Detector array and have operated reliably since the beginning of Phase I. Each WCD consists of a cylindrical tank filled with 12 000 l of purified water, instrumented with three downward-facing large photomultiplier tubes (LPMTs) mounted at the top. These PMTs detect Cherenkov light from charged particles and provide signals over a wide dynamic range suitable for most shower geometries.

As already mentioned, as part of the AugerPrime upgrade, a fourth, small PMT was added to extend the dynamic range, and the original electronics were replaced by the Upgraded Unified Board. The SPMT allows accurate measurement of high particle densities near the shower axis, while the UUB provides enhanced digitization, timing, and calibration capabilities [3]. This extends the WCD dynamic range by more than an order of magnitude, from a few hundred VEM (vertical equivalent muon) with the large PMTs to nearly 20,000 VEM with the SPMT.

The stability of the WCD array is shown in Fig. 2, which presents the daily rate of high-quality events per active hexagon for the SD-1500, as an example. The indicated energy threshold is chosen to ensure full trigger efficiency. Blue triangles indicate Phase I data, and red circles correspond to Phase II data, while the open gray circles show the transition period between the two. Rates remain

Event rate above 3 EeV 1500m array

Figure 2: Daily rate of high-quality events per active hexagon for the SD-1500. The energy threshold ensures full trigger efficiency. Blue triangles indicate Phase I data, and red circles correspond to Phase II data, while the open gray circles show the transition period.

Figure 3: Evolution of SPMT-related parameters: number of active SPMTs (black), calibration factor 𝛽 (red), station temperature (blue), and SPMT current (magenta), from 2021 to 2025.

stable over time, demonstrating consistent array performance during and after the deployment.

The SPMT, a 1-inch Hamamatsu R8619, cannot be directly calibrated with atmospheric muons due to its small photocathode area. A cross-calibration procedure is used instead, converting the integrated ADC signal to VEM via a linear relation, 𝑆 VEM = 𝛽 𝑄 ADC counts, with a conversion factor 𝛽 determined to better than 2 5 % [6]. .

The long-term evolution of SPMT-related parameters can be seen in Fig. 3. The number of active SPMTs (black) increases until mid-2023 as deployment progresses. The calibration factor 𝛽 (red) shows a seasonal modulation of 8-10 %, consistent with the temperature dependence of SPMT gain. Temperature (blue), taken from sensors on the LPMTs, and SPMT current (magenta) reflect environmental trends and their effect on the hardware. In particular, the current stabilizes in mid-2023 following the activation of automatic high-voltage regulation.

The UUB digitizes waveforms from all detectors and provides unified acquisition across the upgraded Surface Detector [7]. It also maintains backward compatibility with Phase I triggers to support hybrid operation. Monitoring of voltage rails, temperature, and acquisition rates enables early issue detection and long-term stability tracking.

Figure 4: Daily averages of SSD MIP charge (red), PMT high voltage (blue), anode current (magenta), and the number of operational SSDs (black), grouped in 50-day intervals."
2507.08731v1-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/arxiv_articles/processed_articles/2507.08731v1-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/arxiv_articles/processed_articles/2507.08731v1-with-image-refs_artifacts/image_000006_6ff42391dda2dae179fb8fdebcce05e776a8320e9103ed5189152c3cc9d3bb50.png,"## 4.1. Spectral measurements

In this section, we present the results of our measurements. The values for the pEW and FWHM covering the first 40 days after the explosion are presented in Figure 6. The colour bars on the right side indicate the SN phase. The analysis across the full-time interval (0 - 40 days) does not reveal a distinct separation into two clusters; instead, it shows a continuum where SNe IIb tend to dominate at higher values.

We examined the spectral features across smaller time intervals to investigate whether the observed continuum is intrinsic or a consequence of the broad time range considered (0 - 40 days). The results for the earliest two intervals (0 - 10 and 10 - 20 days) are presented in Figure 7. Within these intervals, the pEW and FWHM parameters reveal clear trends: SNe IIb consistently exhibit stronger H 𝛼 and He I features throughout the entire early time range. Notably, an intermediate region is observed around 25 Å in the pEW of both H 𝛼 and He I, as well as around 125 Å for H 𝛼 and 200 Å for He I in the FWHM. This overlap between the two populations further supports the continuum observed in the full-time interval (0 - 40 days). However, we note that the overlap between the groups becomes more significant as time passes. Specifically, the later the analysed time range, the greater the overlap, as the SN II values grow stronger as time passes (see Figures A.3 and A.4 in the Appendix A). This highlights the important role that the epoch of the SN will have in our analysis and the development of our method. A summary of the pEW and FWHMvalues for each SN type at each time interval is presented in Appendix B (see Table B.4).

We also find that, although SNe 87A-like are spectroscopically similar at all phases to SNe II, they exhibit notably higher values, especially in the H 𝛼 absorption feature (Figures 6 and 7). Despite this, we excluded them from further analyses due to their low numbers.

To evaluate the statistical significance of our results, we applied the Kolmogorov-Smirnov test (KS test). This nonparametric method computes the maximum difference between the Cumulative Distribution Function (CDF) of two data sets under the null hypothesis that both groups are drawn from populations with identical distributions. The KS test results for the comparison of the pEW and FWHM of H 𝛼 and He I between SNe II and IIb are summarised in Table 2. The Table includes the KS statistic (greatest difference between the two CDFs) and the corresponding p-value (significance of the difference obtained) for each time interval. A p-value &lt; 0 05 indicates the rejection . of the null hypothesis, suggesting a statistically significant difference between the two groups.

From Table 2, we observe that most distributions have pvalues significantly below the 0.05 threshold, indicating a mean-

Fig. 6. Left panel: pEW of the H 𝛼 absorption profile versus the pEW of the He I 𝜆 5876 line measured across the full-time interval (0 - 40 days). The markers are established by SN type: SNe IIb are shown as blue triangles, SNe II as red circles, and SNe 87A-like as green squares. The colour bars on the right side indicate the SN phase. Right panel: Same as the left panel, but for the FWHM measurements.

Table 2. The KS-statistic run for our measurements. Separately for the pEW and FWHM results for the lines of H 𝛼 and He I.

|            |       | pEW          | pEW       | pEW          | pEW       | FWHM         | FWHM      | FWHM         | FWHM      |
|------------|-------|--------------|-----------|--------------|-----------|--------------|-----------|--------------|-----------|
| Time range | # Sp. | He I         | He I      | H 𝛼          | H 𝛼       | He I         | He I      | H 𝛼          | H 𝛼       |
|            |       | KS-statistic | p-value   | KS-statistic | p-value   | KS-statistic | p-value   | KS-statistic | p-value   |
| 0-10d      | 220   | 0.349        | 0.0002    | 0.281        | 0.005     | 0.303        | 0.002     | 0.205        | 0.085     |
| 10-20d     | 278   | 0.482        | 2.012e-08 | 0.439        | 5.374e-07 | 0.386        | 1.609e-05 | 0.445        | 3.048e-07 |
| 20-30d     | 233   | 0.348        | 1.494e-04 | 0.264        | 8.490e-03 | 0.340        | 2.366e-04 | 0.275        | 5.434e-03 |
| 30-40d     | 135   | 0.205        | 0.227     | 0.141        | 0.667     | 0.195        | 0.273     | 0.259        | 0.063     |
| 0-40d      | 866   | 0.732        | 7.914e-51 | 0.675        | 1.530e-42 | 0.635        | 3.131e-37 | 0.624        | 7.170e-36 |

The table shows the two values returned by the KS-test: the KS-statistic and the p-value. Values that are not statistically significant are shown in italics.

ingful separation between the SN II and IIb. The two samples exhibit statistically significant differences in the earliest 30 days post-explosion, with the most clear separation occurring within a 10 to 20-day interval. These findings support our hypothesis that SNe II and IIb display differences in their early spectral evolution, reinforcing the possibility of identifying them based on pEW and FWHM values.

The obtained KS test values suggest that while early-time observations can effectively differentiate between the two types based on pEW values, this distinction becomes less clear as SNe evolve. Interestingly, the greatest difference between SN II and IIb is observed during the 10 - 20-day interval rather than the earlier range (0 - 10 days). This is likely because both types are characterised by a blue featureless spectrum without prominent features soon after the explosion. Even as H lines emerge, they may still be weak, lacking sufficient signatures to differentiate between SNe II and IIb. Consistent with this, we find no significant difference in the H 𝛼 FWHMdistribution during the first 10 days post-explosion.

From30dayspost-explosion, classifying SNe II and IIb based on the analysed spectral features becomes increasingly challenging due to the significant overlap between the two populations. For the 30 - 40-day interval, the p-values exceed the significance threshold across all analysed distributions, indicating that the differences between the pEW and FWHM of the two SN types decrease as their spectra evolve. Although the reduced number of spectra may influence the results, the physical convergence of spectral features also contributes. Around 30 days after explosion, the H 𝛼 line in SNe II typically strengthens, reaching intensities comparable to those observed in SNe IIb. At the same time, the He I line is often blended with the emerging Na I feature, particularly for SNe II (Gutiérrez et al. 2017). Nevertheless, the distinction between the two types remains clear at this stage when considering additional spectral features beyond H 𝛼 and He I. Indeed, by ∼ 30 days post-explosion, the H 𝛼 P-Cygni profile in SNe IIb begins to exhibit distinct characteristics of this subtype. Notably, the He I 𝜆 6678 feature, located near the peak of H 𝛼 becomes more prominent, indicating an interference in the emission of the H feature. Additionally, He I lines emerge at

Fig. 7. pEW and FWHM values of H 𝛼 line and He I 𝜆 5876 for 0 - 10 days and 10 - 20 days. The markers are established by SN type: SNe IIb are shown as blue triangles, SNe II as red circles, and SNe 87A-like as green squares. The colour bars on the right side indicate the SN phase. The pEW measurements are in the top panels, and the FWHM measurements are in the bottom panels. In the left panels, we show the measurements within 0 - 10 days and in the right panels, between 10 - 20 days.

this phase, providing robust criteria for distinguishing between SNe II and IIb."
2507.08745v1-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/arxiv_articles/processed_articles/2507.08745v1-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/arxiv_articles/processed_articles/2507.08745v1-with-image-refs_artifacts/image_000003_ae9a6f20ef308ce874db6661c9a78a626e7dd78d52c1fe78f4c240bde76dbea7.png,"## 4.4 Results on Real-World Data

Data sets. We used four real-world data sets in our experiments. Some statistics about them are presented in Table 1. The Mammals dataset contains information about which mammal species inhabit which areas of the world on one side, and climate information on the other side [12]. The Dialect data contains features of spoken dialects of Finnish over different geographical regions [6, 7]. The 20Newsgroups data 4 is a corpus of 1000 posts from 20 different newsgroups, stemmed and with rare terms removed. The Abstracts data 5 is another corpus, this time of project abstracts. Terms are stemmed and rare ones are removed.

4 https://archive.ics.uci.edu/dataset/113/twenty+newsgroups 5 https://archive.ics.uci.edu/dataset/134/nsf+research+award+abstracts+1990+

2003

Table 1: Properties of real-world data sets

| Data         | Rows   | Columns   | Density ( % )   | Note           |
|--------------|--------|-----------|-----------------|----------------|
| Mammals      | 54 013 | 4 802     | -               | Numerical data |
| Dialect      | 1 334  | 506       | 16 . 14         | Numerical data |
| 20Newsgroups | 5 163  | 19 997    | 0 . 89          | Numerical data |
| Abstracts    | 4 894  | 12 841    | 0 . 90          | Numerical data |

Figure 5: Top row: Relative reconstruction error on the y -axis and number of tiles on the x -axis. (a) Tiles are redescriptions from the Mammals data. (b) Tiles are frequent itemsets from the Dialect data. (c) Tiles are rank-1 Boolean matrices for BMF from the Dialect data. Bottom row: Time (in seconds) on y -axis and number of tiles on x axis. (d)-(f) as (a)-(c).

Redescription mining. We used redescriptions mined using the fast redescription mining algorithm Fier [14]. These results contained 161 redescriptions, some of them very similar to each other. The results are shown in Fig. 5(a) and (d). The results show that HaPSi is able to find very fast a good set of approximately 50 redescriptions. It should be noted that this data can be fully covered using all 161 tiles, so all methods converge towards the end. Notice also that Greedy is very slow.

Tiling databases. We mine frequent itemsets from the Dialect dataset with minimum frequency of 15 % . This gave us 28 535 frequent itemsets. We used all of these as tiles, and searched for a set of maximum 1000 tiles. The results are in Fig. 5(b) and (e). We see that HaPSi is again very good up to about 50 itemsets, after which it has converged. It is again significantly faster than Greedy and significantly better than Naïve .

Boolean matrix factorization. We created the rank-1 Boolean matrices using the association matrix technique used by the Asso algorithm and the restarted random walks technique [19]. These rank-1 matrices can cover 0 s in the data, unlike in the previous cases. The results using the Dialect data are shown in Fig. 5(c) and (f). The results are similar to the other real-world use cases, except that in this case the Naïve algorithm increases its error consistently. But again HaPSi is competitive with Greedy , especially when we consider the significant advance it has on running time.

We also ran experiments with tiles found only by the association matrix technique using the two large corpus matrices. This created approximately 5000 tiles for 20Newsgroups and 12 000 for Abstracts . HaPSi took 850 s for Abstracts , while Greedy took 101 h . For 20Newsgroups HaPSi took 665 s and Greedy 74 h . This shows that HaPSi can find results efficiently with input sizes where Greedy is unable to produce the results in a reasonable amount of time."
2507.08747v1-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/arxiv_articles/processed_articles/2507.08747v1-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/arxiv_articles/processed_articles/2507.08747v1-with-image-refs_artifacts/image_000008_96611d5e31f451e5446bc6eda0e6cef728a387d83ea47a0f31ceca02be89bb6f.png,"## 3.3 Thunderstorms

The largest source of detector instabilities is lightning activity. Both in Phase I and Phase II, a spike in the T3 rate and a subsequent drop in the T3 purity are observed during and in the wake of thunderstorms, as shown in Figure 4. During these periods, the SD stations exhibit an elevated rate of T2 triggers. More importantly, T2 triggers from neighboring stations are highly correlated

Figure 4: a A large number of 3-fold T3s is recorded during periods with thunderstorms, as indicated by the arrows. b The corresponding 3-fold T3 purity rate measures a drop coincident with the weather period.

Figure 5: An erroneously triggered WCD trace (black) displays electronic noise in the form of a 10 MHz oscillation around the baseline (orange) between two sub-threshold muon pulses. The blue trace is the output of a trace-cleaning algorithm using the black trace as input. The effect of the electronic noise is mitigated.

in time, as they are likely caused by the same electric discharge. Consequently, the CDAS identifies many more T3 events than during nominal operation. Since the communication between the CDAS and SD stations is bandwidth-limited, only one to two T3 requests can be issued to the SD array every second. This represents a bottleneck that results in the buildup of a large T3 queue during thunderstorms. The CDAS naively iterates through the T3 queue and requests data from a station often hours after the event happened. 3 Such stale events block the acquisition of new, real air shower events and drop the T3 purity and SD uptime considerably. The estimated effect on the duty cycle of the UB array is ∼ 2 %. The UUB, with its higher resolution, is more susceptible to lightning-induced triggering. The drop in the SD duty cycle for Phase II reads 5%. The algorithm presented in Section 5 partially fixes this issue."
2507.08748v1-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/arxiv_articles/processed_articles/2507.08748v1-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/arxiv_articles/processed_articles/2507.08748v1-with-image-refs_artifacts/image_000006_94bcae196ac044a7e56f3bee2a6dfa0d40721f308fe3476fe7e23a1e5f809a4f.png,"## 4. Data quality monitoring

IceCube data-taking proceeds in periods called runs which have a typical duration of 8 hours. The condition of the detector during one run and resulting data quality are verified manually afterwards, with help of an array of monitored quantities and knowledge of past problems. This allows to define the set of runs usable for a selfconsistent archival data set. For FRA however, a heuristic is required that can assure data quality with a reasonable degree of confidence with lower latency. This has already been implemented to accommodate the GFU event selection [4]. This heuristic's starting point are the rates of intermediate event selection stages, measured in time intervals of typically 600 seconds. With this rate as 𝑋 and the exponentially weighted averages ⟨ 𝑋 ⟩ and ⟨ 𝑋 2 ⟩ , a Z-score is calculated for the deviation of 𝑋 -⟨ 𝑋 ⟩ relative to the standard deviation.

Summing this Z-score for several rates as well as the ratio between them results in the final instability score, which can be compared to a threshold. We present here

Figure 4: ROC curves comparing possible thresholds to be set on the three instability scores. The x-axis is a ""false positive"" rejection rate during good runs, while the y-axis is the ""true positive"" rejection during bad runs. The threshold of 10 is indicated by filled circles on the respective curves.

instability scores that are analogously derived for a cascade selection like DNN Cascades, and a DeepCore-focused selection like GRECO or ELOWEN. These might eventually prove to take complementary roles to the original, ensuring the quality of the specific event selections. √

In the new definitions, the Z-scores are also weighted proportionally to Δ 𝑡 in accordance with the expected variance. This suppresses the statistical fluctuations present for the GFU instability score during shorter-than-typical bins. Figure 4 shows that both new scores remain sensitive to

(a) Sky map of randomized events within a 1000second interval including one DNN Cascades event.

(b) Corresponding test statistic map in a point source analysis combining the three event selections.

Figure 5

conditions flagged by the run monitoring, and a common threshold or multi-variate cut can be optimized for a particular analysis."
2507.08753v1-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/arxiv_articles/processed_articles/2507.08753v1-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/arxiv_articles/processed_articles/2507.08753v1-with-image-refs_artifacts/image_000007_a3135b8346d49dc006d95f67eacb9f391e913a730dc410800cf6ebb06f62f43c.png,"## 3. Method

This is a template-based analysis where templates based on four different galactic emission models are being tested. The Fermi 𝜋 0 template [9], the KRA 5 𝛾 and KRA 50 𝛾 templates [10] and the CRINGE template [11]. The results of this analysis will include the measurement of the model normalization for each of the four model hypotheses, the rejection of the null hypothesis under each model assumption, and the rejection of the global null hypothesis by the most significant model hypothesis after trial correction. We use an unbinned maximum likelihood method that utilizes the reconstructed direction, energy, and angular uncertainty of each neutrino candidate in the ICEMAN dataset.

The signal considered is the excess of neutrinos along the galactic plane over the only declination-dependent background fluxes of atmospheric muons, atmospheric neutrinos, and diffuse

Figure 2: Per-flavor all-sky effective area in the ICEMAN sample.

astrophysical neutrinos.

To parameterize this excess prediction based on each model, templates of their respective prediction are folded with the effective area of each dataset. After normalizing, this creates a spatial probability density function (PDF) for each dataset and template combination. To account for the respective angular uncertainty of each event, each spatial PDF is smeared with a set of 2D Gaussian kernels corresponding to different angular uncertainties. They are then evaluated based on the uncertainty of the respective event. Figure 4 shows an example of the unsmeared PDF as well as an example of the smeared PDF for each sub-dataset.

To build the energy PDF, the MC is weighted with a single power law with a spectral index of -2.7 is used for the Fermi 𝜋 0 template. For the other three templates, the sky-averaged spectrum predicted by each template is used. The weighted MC is then binned in reconstructed energy. This way an energy PDF in reconstructed energy based on the predicted energy spectra is obtained. All nominal all-sky fluxes are shown in Figure 5.

To parameterize the declination-dependent backgrounds, a PDF of the density of neutrinos per declination is used. Since this background PDF is derived from data that contains partial contamination from the signal, a signal subtraction method is applied. The observed PDF from data ¯ , 𝐷 is characterized as the sum of the true isotropic background 𝐵 , and the average density of the signal per declination band ¯ 𝑆 𝛿 ( 𝑖 ) :

Using this, the likelihood of observing 𝑛 𝑠 neutrinos is defined as shown in equation 2:

Figure 3: Sketch showing which dataset overlapping events are kept in. The event numbers exclusive to the datasets and in all overlapping zones are shown. The colors of the shaded regions show which dataset the overlap will stay in.

Then, the test statistic (TS) is defined as the likelihood ratio of fitting 𝑛 𝑠 neutrinos over zero neutrinos,

Using this method, we can derive the sensitivity and discovery potential of this analysis using pseudo-experiments. Here, a number of Monte-Carlo events corresponding to an injected flux are

Table 1: Sensitivity and discovery potentials for different models. For Fermi 𝜋 0 , the per-flavor flux at 100 TeV is reported. For the other three models, sensitivity and discovery potential are reported in units of model flux.

| Quantity            |   Fermi 𝜋 0 [10 - 12 TeV cm - 2 s - 1 |   KRA 5 𝛾 |   KRA 50 𝛾 |   CRINGE |
|---------------------|---------------------------------------|-----------|------------|----------|
| Sensitivity         |                                  4.68 |      0.13 |        0.1 |     0.13 |
| Discovery Potential |                                 18.7  |      0.49 |        0.4 |     0.53 |

sampled from the spatial and energy distribution predicted by a chosen model. Data, with the right ascension coordinate randomized, is added to these signal events to account for the isotropic backgrounds. Table 1 shows the necessary flux for 50% of pseudo experiments to exceed the TS corresponding to a 5 𝜎 discovery for the four different templates. Since the flux measured in [3] is above the flux necessary to reach the 5 𝜎 threshold for the Fermi 𝜋 0 model, a 5 𝜎 pre-trial significance is a likely outcome of this analysis. In a set of pseudo-experiments, injecting the best fit flux of the previous measurement, we obtain a median local significance of 5.5 𝜎 for the Fermi 𝜋 0 model, as shown in Table 2.

Figure 4: Spatial signal PDF for the Fermi 𝜋 0 template. Shown in a) acceptance weighted with ESTES without smearing. In b) acceptance weighted with ESTES and smeared with a Gaussian kernel of 0.3 degrees width. In c) the template is acceptance weighted with DNN Cascades and smeared with a Gaussian kernel of 7 degrees width. In d) the template is acceptance weighted with the Northern Tracks and smeared with a Gaussian of 0.3 degree width. These smearing widths are chosen based on the median angular uncertainty of the signal events in the respective datasets.

Engergy [Gev]

Figure 5: Nominal per-flavor flux prediction for all four templates used in this analysis."
2507.08765v1-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/arxiv_articles/processed_articles/2507.08765v1-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/arxiv_articles/processed_articles/2507.08765v1-with-image-refs_artifacts/image_000000_7e924c9d86139a0f8f27c9686b10e4ee2f00c2e1d13bdea744f7c4eafdb1336f.png,"## I. INTRODUCTION

I N computer vision, Segment Anything Model (SAM) [1] has emerged as a dominant solution for a wide range of segmentation tasks. Extensive independent evaluations confirm that SAM could achieve high-quality segmentation masks through natural interactions, i.e. , clicks, bounding boxes, or text prompts. SAM's core objective is zero-shot segmentation, enabling segmentation of arbitrary objects without taskspecific training. Building on SAM's significant success, as Table I shows, numerous variants like Med-SAM [2] and SAMHQ [3] have been developed. They employ novel architectures and fine-tuning strategies to enhance efficacy in specific domains or boost segmentation performance further. SAM and its

Fenglei Fan (hitfanfenglei@gmail.com) is the corresponding author.

Juntong Fan and Feng-Lei Fan are with Frontier of Artificial Networks (FAN) Lab, Department of Data Science, City University of Hong Kong, Hong Kong, China SAR.

Zhiwei Hao and Jianqiang Shen are with the Data Storage Product Line, Huawei Technologies Co., Ltd. (haozhiwei@huawei.com; shenjianqiang@huawei.com)

Shang-Ling Jui is with Lagrange Mathematics and Computing Research Center (jui.shangling@huawei.com)

Yi Zhang is with the School of Cyber Science and Engineering, Sichuan University, Chengdu 610207, China, and also with the Key Laboratory of Data Protection and Intelligent Management, Ministry of Education, Sichuan University, Chengdu 610207, China (e-mail: yzhang@scu.edu.cn).

Jing-Xiao Liao is with Department of Industrial and Systems Engineering, The Hong Kong Polytechnic University, Hong Kong, SAR of China.

variants have been widely applied in diverse scenarios such as healthcare and intelligent manufacturing. However, SAM and many variants are notoriously large-scale, hindering efficient deployment for users with limited computational resources. For instance, MobileSAMv2 [4], a compact SAM variant, still comprises 641 million parameters and requires 2.37 GB of storage. Deploying such large models presents significant challenges in resource-constrained settings, such as autonomous vehicles and smartphones. Consequently, effectively compressing SAM and its variants becomes an increasingly pressing practical need.

TABLE I

A SUMMARY OF SEVERAL MAINSTREAM SAM VARIANTS

| Model                  | Pre-training Data   | File Size   | Parameters   | linear params(%)   |
|------------------------|---------------------|-------------|--------------|--------------------|
| SAM-B [1]              | SA-1B               | 357MB       | 94M          | 94.84%             |
| SAM-L [1]              | SA-1B               | 1.16GB      | 312M         | 97.96%             |
| SAM-H [1]              | SA-1B               | 2.38GB      | 641M         | 98.76%             |
| SAM-HQ-Tiny [3]        | SA-1B + HQSeg-47K   | 40.5MB      | 11M          | 86.12%             |
| SAM-HQ-B [3]           | SA-1B + HQSeg-47K   | 361MB       | 95M          | 93.92%             |
| SAM-HQ-L [3]           | SA-1B + HQSeg-47K   | 1.16GB      | 314M         | 97.58%             |
| SAM-HQ-H [3]           | SA-1B + HQSeg-47K   | 2.39GB      | 643M         | 98.53%             |
| SAM2-Tiny [5]          | SA-1B               | 148MB       | 39M          | 97.58%             |
| SAM2-Small [5]         | SA-1B               | 175MB       | 46M          | 97.91%             |
| SAM2-Base [5]          | SA-1B               | 308MB       | 81M          | 98.64%             |
| SAM2-Large [5]         | SA-1B               | 856MB       | 224M         | 99.36%             |
| MobileSAM [4]          | 1%SA-1B             | 38.8MB      | 10M          | 88.56%             |
| MobileSAMv2(ViT-H) [4] | 1%SA-1B             | 2.37GB      | 641M         | 98.76%             |
| EdgeSAM [6]            | 1%SA-1B             | 37.0MB      | 10M          | 41.19%             |
| EdgeSAM-RPN [6]        | 3%SA-1B             | 37.0MB      | 10M          | 41.19%             |
| EfficientSAM-Ti [7]    | SA-1B+IN            | 39.0MB      | 10M          | 90.72%             |
| EfficientSAM-S [7]     | SA-1B+IN            | 100MB       | 26M          | 95.41%             |
| TinySAM [8]            | SA-1B               | 38.8MB      | 10M          | 88.56%             |
| MedSAM [9]             | FLARE               | 358MB       | 94M          | 95.90%             |

We think that an ideal compression solution for SAMs should exhibit four key characteristics: versatility across model types, agility in model deployment, faithfulness to the original model, and compactness in model size. (i) The philosophy of SAMs lies in the universality, compression methods shall not be restricted to specific model architectures either. This universality enhances compression efficiency and facilitates the transfer of diverse SAM variants into massive users. (ii) The growing adoption of 'model factory' paradigms in cloud computing necessitates parsimonious model handling. Compression must therefore be sufficiently rapid to enable efficient batch processing, which can significantly boost the overall compression throughput. (iii) Compressed models shall closely preserve the original SAM's capabilities. Given the substantial costs involved in training foundational SAMs, a large performance loss is unacceptable. Consequently, compression should not require model retraining. While model open-sourcing is common, accessing the data engine, e.g. , as with Med-SAM, is often prohibited, and datasets prepared specifically for compression are typically limited. Retraining risks cumbersome procedures, catastrophic forgetting [10] (undermining SAM's core capabilities), and the potential introduction of unintended biases. (iv) Provided faithfulness is maintained, the compression solution shall achieve a high compression ratio to enable viable deployment in extremely

resource-constrained environments.

Four principal model compression techniques are commonly employed: pruning, quantization, knowledge distillation, and low-rank decomposition. The compression ratios by pruning and quantization methods are hard to scale. Studies have shown that under the constraint of model fidelity, pruning can typically achieve only a compression ratio of 2 to 4 times [11]. The theoretical lower bound for quantization is 1-bit representation, but subject to the severe perturbation to model parameters and the difficulty of fine-tuning. To the best of our knowledge, achieving a reliable and performant lower-than-INT4 quantization has been a challenging task for industry and academia so far. Therefore, low-bit quantization remains impractical for models like SAM. Moreover, lowrank decomposition approximates weight matrices via lowrank representations but faces critical limitations. First, the method necessitates extensive tuning for convergence due to the non-convex optimization landscape. Second, low-rank decomposition struggles with standard convolution kernels which are small, limiting its applicability as a universal model compression technique [12]. As for knowledge distillation, it involves extensive training; therefore, we exclude it from our discussion.

Fig. 1. The basic principle of the Hyper-Compression, where x ∈ R N denotes the target high-dimensional point. Some transformation T and the initial point x 0 establish the existence of a scalar k ∈ N + for an arbitrary x such that x can be approximately represented as T k ( x 0 ) . Thus, we can compress x into k . This figure is reproduced based on Figure 2 in [13]. .

Earlier, our group proposed Hyper-Compression, which is a novel model compression technique that leverages trajectory density to transform the high-dimensional parameter compression into the representation of a low-dimensional dynamic system. As illustrated in Figure 1, there exists a dynamic system such that the one-dimensional trajectory induced by some initial point can fill the high-dimensional space. Here, 'filling the high-dimensional space' means that the trajectory can always arrive in the arbitrarily small neighborhood of any point. Thus, Hyper-Compression establishes a deterministic mapping between the one-dimensional trajectory parameter k (composition times or trajectory length) and the highdimensional target vector x ∈ R N : x ≈ T k ( x 0 ) . This potentially enables the compression of x into the scalar k : x → k , while decompressing x from k follows the compo- sition of transformation: T k ( x 0 ) ≈ x . As Table II shows, Hyper-Compression enjoys a novel mechanism for model compression that is fundamentally different from quantization, pruning, and low-rank decomposition."
2507.08771v1-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/arxiv_articles/processed_articles/2507.08771v1-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/arxiv_articles/processed_articles/2507.08771v1-with-image-refs_artifacts/image_000003_ff9df14cc5c6595ac9125e1d9738fd8503be8034b376904a6bf9e0045f292161.png,"## 4.1.1 Overall Results

To demonstrate the rationality of our architecture, we conduct experiments by comparing BlockFFN with multiple sparsely-activated architectures: Vanilla TopK MoE, DeepSeekMoE (DSMoE) (Dai et al., 2024), GRIN (Liu et al., 2024c), and ReMoE (Wang et al., 2024b) (see Appendix C). To ensure fairness, we keep consistent settings for attention layers and MoE experts (i.e., the number and intermediate dimension of experts) throughout baselines and BlockFFN. Besides, all settings (within each scale) have close parameter numbers, training token numbers, and token-level sparsity. We involve four parameter scales: Small (0.1B), Medium (0.5B), Large (0.8B), and XLarge (1.2B). See Appendix D for model settings.

We adopt two comparison metrics: perplexity (PPL) on validation datasets and evaluation scores on benchmarks. Benchmarks include two groups: commonsense reasoning (C.R.) and reading comprehension (R.C.). See Appendix E for details about data and benchmarks.

The PPL and evaluation scores are shown in Table 2 and 3, respectively. The training curves of the 'XLarge' settings are drawn in Figure 1b. We can draw the following observations:

(1) Performance : Under close parameter numbers, all the settings (except for Small ReMoE and BlockFFN) cannot match the 'Dense' setting, due to the performance compromise of sparsification. However, under close TLS values (i.e., identical average FLOPs for each token), BlockFFN outperforms other MoE baselines in terms of validation PPL, train loss, and scores on downstream tasks , showing less performance compromise.

Figure 3: For each token in vocabulary, we calculate its frequencies and average ratios of activated experts, which show a bimodal distribution of expert allocation.

Figure 4: The layer-wise distributions of average activation magnitudes on the 'Small' settings. While BlockFFN uses CLS-aware objectives, ReMoE adopts L1 regularization.

(2) Sparsity : Under close TLS values, BlockFFN always has considerably higher CLS values than other baselines . Attributed to CLS-oriented training objectives, this property makes BlockFFN more friendly for acceleration."
2507.08776v1-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/arxiv_articles/processed_articles/2507.08776v1-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/arxiv_articles/processed_articles/2507.08776v1-with-image-refs_artifacts/image_000003_400fb125b741a72f6db786d8b5c16a6d271a2451b7ee8fa05f806bc438fcda25.png,"## 4.2 Main Results

Figure 2 and Figure 4 show the main evaluation results on the RealEstate10K and DL3DV datasets. In the plots, the x-axis represents the data size of the scene representation: Storage CLiFTs for our method, decoder input tokens for LVSM, and splats for MVSplat and DepthSplat. The y-axis reports PSNR; due to space constraints, the LPIPS and SSIM plots are deferred to the Appendix, where they show consistent trends.

None of the baselines support controlling the data size and require a separately trained model for each data point. Many baseline points are missing from the plots because we either use publicly available checkpoints (for MVSplat and DepthSplat) or train a new model (§4.1). In contrast, our method trains a single model per dataset and supports fine-grained control over both the storage data size and the render data size via the numbers of Storage ( N s ) and Render ( N r ) CLiFT tokens.

The plots show that CLiFT achieves comparable PSNR with approximately 5-7 × less data size than MVSplat and DepthSplat, and about 1.8 × less than LVSM, highlighting the effectiveness of our compressed tokens and overall scene representation. CLiFT also attains the highest overall PSNR with significantly lower data usage. Qualitative results in Figure 4 support these findings: our method preserves sharp appearance details that are closer to the ground truth and maintains high visual fidelity even under strong compression, with only minor loss in high-frequency content.

Figure 3: Ablation studies on our individual components, in particular, latent K-means and neural condensation. The plots compare three variants of our system by dropping latent K-means and neural condensation one by one from the system, while varying the data size. Specifically, the x-axis is the size of the scene representation. The y-axis is rendering quality (PSNR, LPIPS, and SSIM), rendering speed (FPS), or rendering cost (FLOPs), measured on an NVIDIA RTX A6000 GPU.

Table 1: We fix the number of storage CLiFTs to represent a scene ( N s =4096), then vary the number of render CLiFTS (how many tokens to use for rendering) on-the-fly and measure rendering quality (PSNR), rendering speed (PSNR), and rendering cost (theoretical number as GFLOPs).

| Metrics   |   Render CLiFTs | Render CLiFTs   | Render CLiFTs   | Render CLiFTs   | Render CLiFTs   |
|-----------|-----------------|-----------------|-----------------|-----------------|-----------------|
| Metrics   |         4096    | 3072            | 2048            | 1024            | 512             |
| PSNR      |           26.72 | 26.71 (-0.01)   | 26.56 (-0.16)   | 25.75 (-0.97)   | 23.89 (-2.83)   |
| GFLOPs    |           70.6  | 63.3 (-10%)     | 56.1 (-21%)     | 48.9 (-31%)     | 45.23 (-36%)    |
| FPS       |           54.3  | 53.89 (-1%)     | 66.44 (+22%)    | 80.77 (+49%)    | 90.15 (+66%)    |"
2507.08784v1-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/arxiv_articles/processed_articles/2507.08784v1-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/arxiv_articles/processed_articles/2507.08784v1-with-image-refs_artifacts/image_000005_94edd31b0621700b447074ce346f0cef4c4f83d6d4ecee89edf2a8fe26e48ad2.png,"## D. Memory Analysis

Figure 5 reports the peak GPU memory of GreedyLore during pre-training of LLaMA models. For models with up to 350 M parameters, we use a per-GPU batch size of 128. For the 1B-parameter model, we reduce the batch size to 64 due to memory constraints. We do not employ gradient accumulation here, even though it enables larger effective batch sizes.

The memory overhead of GreedyLore comprises two components. First, the error-feedback mechanism requires additional storage proportional to the total number of model parameters. However, activation tensors dominate memory usage during pre-training, rendering this overhead negligible. Second, storing the projection matrix U ∈ R min( m,n ) × min( m,n ) incurs storage of size min( m,n ) 2 by selecting either a leftor right-sided projection (see Appendix D). Consequently, the overall memory overhead of GreedyLore remains negligible, as confirmed by our profiling results.

Fig. 5: Peak memory of pre-training of LLaMA models."
"""Did_I_buy_that_just-with-image-refs","/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/""Did_I_buy_that_just-with-image-refs.md","/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/""Did_I_buy_that_just-with-image-refs_artifacts/image_000006_baa0d407f0fbba5355dafa833b7d7f1e3e331f706135b2da22e050080103059e.png","## 4 Results

- *Descriptive Statistics: Sample*

After the exclusion of outliers (reported in section 3.5), a total of n=290 participants (n=118 male, n=171 female, n=1 non-specified) were analyzed. Their mean age was 33.27 years (SD = 10.34, Range: 18-71). Table 1 summarizes demographic and other characteristics of the participants. The sample is non-representative and follows the known pattern of Prolific’s panel to be younger, more female and more educated than the general population (Prolific, 2024). Net household incomes in the sample were distributed around the average net household incomes in Germany (€3174), Austria (€3205), and Switzerland (€7132) (Eurostat, 2024). Compared to the population in these countries, the sample also contains a higher share of people with another country of birth (41 % in the sample vs. 17.3 % in Germany and 19.3 % in Austria, Statistisches Bundesamt, 2023). 32 % of the sample (94 out of 290) were enrolled as students at the time of the study.

| Variable                                | n =290   | % sample share   |
|-----------------------------------------|----------|------------------|
| Country of residence                    |          |                  |
| Germany                                 | 253      | 87 %             |
| Austria                                 | 27       | 9 %              |
| Switzerland                             | 10       | 3 %              |
| Country of birth                        |          |                  |
| Germany/Austria/Switzerland             | 172      | 59 %             |
| Other                                   | 118      | 41%              |
| Highest education                       |          |                  |
| Secondary school or lower               | 19       | 7 %              |
| Highschool or trade school              | 79       | 27 %             |
| University degree                       | 184      | 63 %             |
| PhD                                     | 8        | 3%               |
| Net household income                    |          |                  |
| Less than €1000                         | 26       | 9 %              |
| €1000 to €1999                          | 51       | 18 %             |
| €2000 to €2999                          | 70       | 24 %             |
| €3000 to €5000                          | 82       | 28 %             |
| More than €5000                         | 48       | 17 %             |
| Unspecified                             | 13       | 4 %              |
| Familiarity with online (food) shopping |          |                  |
| No experience with online shopping      | 1        | 0 %              |
| Shops non-food products online          | 77       | 27 %             |
| Tried online food shopping once         | 138      | 48 %             |
| Regularly shops food online             | 74       | 26 %             |
| Dietary preferences                     |          |                  |
| No restrictions                         | 195      | 67 %             |
| Flexitarian (i.e. mostly vegetarian)    | 54       | 19 %             |
| Vegetarian                              | 27       | 9%               |
| Vegan                                   | 9        | 3%               |
| Other                                   | 5        | 2%               |
| Allergies/Intolerances                  |          |                  |
| No allergies                            | 240      | 83 %             |
| At least one allergy /food intolerance  | 50       | 17 %             |

Table 1: Characteristics of the sample (n = 290)

- *Descriptive Statistics: Shopping Behavior*

Table 2 contains summary statistics for indicators of shopping behavior. Older participants tended to take longer during the shopping task (Correlation between age and shopping time: *r* = *.* 31 *, p &lt; .* 05). Beverages were excluded from relevant metrics, as there was high variance in the spending in this category, and it was not of relevance to any of our analyses. Even after this adjustment, there was a large range for money spent. However, the median amount of spent (excl. beverages) was reasonably close to the German average of € 70 for a 2-person household (Statistisches Bundesamt, 2021).

| Variable                                          |     M |    SD |   Median | Range          |
|---------------------------------------------------|-------|-------|----------|----------------|
| Total shopping time (in mins)                     | 13.65 |  8.59 |    11.41 | 1.18 – 53.12   |
| Number of individual items chosen                 | 32.7  | 15.02 |    30    | 5.00 – 91.00   |
| Avg. time spent deliberating each item (in secs)  | 26.25 | 14.63 |    23.41 | 6.08 – 116.43  |
| Money spent in € (excl. beverages)                | 91.72 | 52.09 |    81.86 | 10.42 – 394.69 |
| Avg. price per chosen item in € (excl. beverages) |  2.15 |  0.61 |     2.09 | 0.81 – 7.68    |

Table 2: Summary statistics of shopping behavior. Avg, time spent deliberating each item is calculated as ""Total shopping time"" divided by the ""Number of individual items chosen"".

To estimate the validity of participants’ demonstrated shopping behavior, we asked them how regularly they buy products of the top-level categories in their daily lives. Figure 4 shows the average values for each of the top-level categories. Participants’ behavior was considered consistent if their choices in the shopping task included products from categories that they reported buying ”at least somewhat regularly” or more often (5 or higher on the 7-point Likert scale). Conversely, their shopping was also considered consistent if they reported buying from a category ”now and then” or rarer (4 or lower on the 7-point Likert scale), and then did not include any products of that category in their selection.

Figure 4: Mean values of habituality for each top-level category. Error bars represent standard deviations. Base: n=290.

On average (standard deviation), their behavior was consistent in 6.90 (1.50) out of 9 categories. Outliers (n=6) with inconsistent behavior in most (6 or more) categories were further inspected in terms of instruction compliance, but their behavior in the shopping task or the survey was not noticeable in any other way. Therefore, we did not exclude them from the analyses. It is conceivable that the one-time nature of our assessment failed to accurately capture the dietary habits of these participants. This is a limitation that dietary recall studies also acknowledge (Jonnalagadda et al., 2000). In general, fulfilling the shopping task seems to have been at least somewhat representative of participants’ usual purchasing habits.

- *Accuracy of Self-Reports*

In shopping self-reports, two types of errors were possible: Underreporting (false negative) errors refer to participants choosing products of this category during the shopping task, but then not indicating this in the self-reports, whereas overreporting (false positive) errors represent reported product choices from a category they did not actually choose any products from. This was possible for all three types of recall: TBR at top-level, IBR at sub-level, and CR.

Self-reports contained, in total, an average (standard deviation) of 3.81 (2.20) errors for a total of 29 self-report categories. The range of errors was between 0 and 13; the median was 4. Figure 5 shows how errors were distributed in the sample, and that 6 participants gave error-free self-reports. Conversely, 98 % of the sample had made at least one error, and most of the participants (86 %) had made more than one. None of the participants made use of the option ”I don’t remember” for any of the categories.

Figure 6 shows all categories included in the self-reports, and how many errors occurred for each of them. Further, it differentiates occurrences of underreporting and overreporting per category.

Figure 5: Frequencies of absolute error counts. Base: n=290.

*Figure 6: Frequencies of error counts, by category and error type. The dashed horizontal lines represent the separation of self-report types, from top to bottom: TBR, IBR, CR. Base: n=290.*

Generally, underreporting was more common than overreporting. Underreporting errors mostly occurred in cued recall (CR). In Figure 6, it is especially apparent how CR was the most common source of error. Participants reported their purchases more accurately when they were presented as clickable options (TBR and IBR) compared to having no specific cue to help them to remember.

Noticeably, top-level-categories, meat and sausage products, were more often overreported than underreported. A striking number of participants (n=95) overreported sausage-type products. Product categories that the sample reported buying frequently, such as vegetables (as can be seen in Figure 4), were less often falsely reported (see Figure 6). However, while this is inherently plausible, implications should be interpreted with caution. If fewer participants choose to shop for a category, there is also a lower chance for it to be reported inaccurately.

The measure of sensitivity *d′* reflects participants’ ability to differentiate signals from one another and is routinely used to compare performances in recall or detection tasks between subjects or conditions (Macmillan &amp; Creelman, 2005). Its calculation is detailed in section 3.5. The total average (standard deviation) self-reporting sensitivity *d′* was 2.18 (0.63). Table 3 contains more error measures for the three types of self-reports. Paired t-tests confirmed that the average accuracy was significantly different between self-report types ( *p &lt; .* 05, all significant), with participants showing the highest accuracy in text-based recognition (TBR, 89 %), followed by image-based recognition (IBR, 78 %) and cued recall (CR, 44 %).

|                          | Text-based recognition (TBR / “top-level”)   | Text-based recognition (TBR / “top-level”)   | Image-based recognition (IBR / “sub-level”)   | Image-based recognition (IBR / “sub-level”)   | Cued recall (CR / “other”)   | Cued recall (CR / “other”)   |
|--------------------------|----------------------------------------------|----------------------------------------------|-----------------------------------------------|-----------------------------------------------|------------------------------|------------------------------|
|                          | M                                            | SD                                           | M                                             | SD                                            | M                            | SD                           |
| Absolute error count     | 1.00                                         | 0.95                                         | 1.48                                          | 1.45                                          | 1.33                         | 0.95                         |
| Relative accuracy (in %) | 88.89                                        | 10.58                                        | 77.60                                         | 7.24                                          | 44.37                        | 31.75                        |
| Sensitivity d’           | 1.91                                         | 0.62                                         | 2.15                                          | 0.59                                          | Cannot be computed           | Cannot be computed           |

Table 3: Accuracy metrics for different types of shopping self-reports. Sensitivity d' cannot be computed for CR as there were only three instances (""other fruits/vegetables/types of meat"").

- *Exploratory Regression*

In addition to analyzing the occurrence of errors in shopping self-reports, we conducted a linear regression to explore which factors predicted self-report accuracy (expressed as sensitivity *d′* ). Table 4 shows the items that were used to capture the measured variables and summarizes their descriptive statistics. A more detailed documentation of how the questionnaire was administered is contained in the Appendix. The results of the linear model are shown in Table 5. Data from n=17 participants were not included in the model, as they had missing data points for survey questions. Two variables went into the regression model as aggregated indices with Cronbach’s ɑ = 0.83 (health-related identity) and ɑ = 0.82 (UTAUT effort subscale).

The regression model was overall significant at *p &lt; .* 05, explaining 8.18 % of the total variance in sensitivity (Adjusted *R* 2). The model revealed the following variables as significant ( *p &lt; .* 05) predictors of sensitivity *d′* : perceived mental load, average time spent deliberating each item, and dietary preferences (participants with a flexitarian, vegetarian or vegan lifestyle reported more accurately than omnivores). All these effects withstood robustness checks which are documented in Table 7, found in the Appendix.

Self-reported degree of deliberation (”While shopping in the online supermarket, I very consciously thought about my choice of products.”), gender (male participants reported more accurately than female participants) and household size (participants with a household size of 2 or more reported more accurately than participants in a single-household) emerged as borderline significant at *p &lt; .* 10.

| Variable                                                                                                                                                |     M |    SD |   Median | Scale Range   |
|---------------------------------------------------------------------------------------------------------------------------------------------------------|-------|-------|----------|---------------|
| Social desirability: sum score                                                                                                                          |  2.47 |  1.17 |     2.5  | 0 – 5         |
| UTAUT effort 1: ”I found it easy to learn how to use the online supermarket.”                                                                           |  6.34 |  0.94 |     7    | 1 – 7         |
| UTAUT effort 2: ”My interaction with the online supermarket was clear and understandable.”                                                              |  6.23 |  0.93 |     6    | 1 – 7         |
| UTAUT effort 3: ”I found the online supermarket easy to use.”                                                                                           |  6.02 |  1.22 |     6    | 1 – 7         |
| UTAUT_effort (aggregated)                                                                                                                               |  6.2  |  0.89 |     6.33 | 1 – 7         |
| TLX mental Load: ”How did you feel about the mental demands of the shopping task?”                                                                      | 38.9  | 26.57 |    35    | 0 – 100       |
| TLX effort: ”In your opinion, how much effort was required for the shopping task?”                                                                      | 37.64 | 25.87 |    34.5  | 0 – 100       |
| Attitude towards online food shopping: ”How do you feel about doing at least some of your grocery shopping online?”                                     |  3.75 |  0.96 |     4    | 1 – 5         |
| Self-reported degree of deliberation: ”While shopping in the online supermarket just now, I have thought very consciously about my choice of products.” |  3.94 |  0.82 |     4    | 1 – 5         |
| Health identity 1: ”I often think about my health.”                                                                                                     |  5.46 |  1.16 |     6    | 1 – 7         |
| Health identity 2: ”My health is an important part of my identity.”                                                                                     |  5.16 |  1.36 |     5    | 1 – 7         |
| Health identity 3: ”Being a healthy person is an important part of how I see myself.”                                                                   |  5.12 |  1.38 |     5    | 1 – 7         |
| Health_identity (aggregated)                                                                                                                            |  5.25 |  1.12 |     5.33 | 1 – 7         |
| Environmental identity: ”I act in an environmentally conscious manner even if it involves considerable costs and effort.”                               |  4.24 |  1.46 |     4    | 1 – 7         |
| Household size                                                                                                                                          |  2.21 |  1.07 |     2    | 1 – 10        |

*Table 4: Descriptive statistics of measured variables. Base: n=290.*

| Variable                                                                    | Estimate  (Std. Error)   | p-value   |
|-----------------------------------------------------------------------------|--------------------------|-----------|
| (Intercept)                                                                 | 2.32 (0.41)              | <0.000    |
| Social desirability: sum score                                              | -0.02 (0.03)             | 0.462     |
| Cognitive Factors                                                           |                          |           |
| UTAUT_effort                                                                | -0.01 (0.04)             | 0.839     |
| TLX_mentalLoad                                                              | -0.01 (0.00)             | 0.003     |
| TLX_effort                                                                  | 0.00 (0.00)              | 0.130     |
| Experience with online food shopping (vs. no experience)                    | 0.03 (0.09)              | 0.749     |
| Positive Attitude towards online grocery shopping (vs. neutral to negative) | 0.00 (0.09)              | 0.980     |
| Conscious Food Choice Deliberation                                          |                          |           |
| Self-reported degree of deliberation                                        | 0.09 (0.05)              | 0.079     |
| Avg. time spent deliberating each item                                      | 0.01 (0.00)              | 0.012     |
| Money spent (excl. beverages)                                               | 0.00 (0.00)              | 0.863     |
| No dietary restrictions (vs. flexitarian, vegetarian or vegan)              | -0.17 (0.09)             | 0.046     |
| Allergies (vs. no allergies)                                                | -0.13 (0.10)             | 0.193     |
| Health-related identity                                                     | -0.03 (0.04)             | 0.382     |
| Strong environmental identity (vs. neutral to negative)                     | -0.06 (0.08)             | 0.425     |
| Sociodemographic Characteristics                                            |                          |           |
| Education                                                                   | 0.02 (0.03)              | 0.544     |
| Income                                                                      | 0.00 (0.02)              | 0.868     |
| Female gender (vs. male)                                                    | -0.15 (0.08)             | 0.068     |
| Age                                                                         | -0.01 (0.00)             | 0.181     |
| Single household (vs. family)                                               | -0.18 (0.10)             | 0.067     |
| Born in Germany / Austria / Switzerland  (vs. another country)              | -0.10 (0.08)             | 0.217     |

Table 5: Results of linear model with a sample of n=273, after excluding participants with missing values. Dependent variable: Sensitivity d' of shopping self-reports. Multiple R²: 0.146, Adjusted R²: 0,0818, F(19,253)=2.28, p &lt; .05.

- *Additional Analyses: Share of Organic Products*

As discussed in section 3.2, there is a striking discrepancy between shoppers self-reported shares of organic purchases and their actual, noticeably lower market shares. Participants in our study were asked to estimate the organic share in their shopping baskets. On average (standard deviation), their estimations were off by 14.88 (13.88) percentage points, with the absolute error ranging between 0 and 65 percentage points. More participants overestimated (n=125) than underestimated (n=102) their organic share. Participants who overestimated vs. underestimated their organic share were compared in terms of their general self-report accuracy in a post-hoc t-test, but no significant difference was found between groups.

From the sample that gave an estimation of their organic share, for about half of them (n=120, or 51.72 %), their estimation was off by more than 10 percentage points. A few participants estimated their share perfectly (N=7), but most of them (N=5) had an organic product share of 0 %. The distribution of estimation errors is shown in Figure 7.

In line with expectations, participants who scored high (i.e., higher than 5 on the 7-point Likert scale) on environmental identity chose a significantly higher share of organic products ( *p &lt; .* 05). This corresponded with a higher degree of error in their estimations that was borderline significant ( *p &lt; .* 10).

To improve our understanding of factors influencing organic estimation accuracy, we again employed a linear model, with participants’ absolute estimation error as dependent variable. 68 participants were not included in the model due to missing values in survey items or because they ticked ”I don’t know” upon being asked for an estimation of their organic share. Table 6 contains the results for this model.

Figure 7: Histogram of the degree of deviation of participants' estimation from their observed organic share. Note: A deviation of exactly 0 only occurred for n=7 participants. Base: n=236 who gave an estimate of their share of organic products.

The model was overall significant at *p &lt; .* 05, explaining 16.32 % of the total variance (Adjusted *R* 2) in the absolute deviation between participants' estimation and their observed organic share. Perceived mental load in the NASA-TLX emerged as a significant ( *p &lt; .* 05) predictor in the model, just as it did for the prediction of overall self-report accuracy. Among indicators for conscious food choice deliberation, self-reported degree of deliberation and health-related identity emerged as a significant ( *p &lt; .* 05) predictors for higher absolute estimation errors, while average deliberation time per item was borderline significant (p &lt; .10) for predicting lower estimation errors.

Age was also revealed as a significant predictor, i.e., younger participants were better at estimating their organic share. Another significant ( *p &lt; .* 05)  predictor was the country of birth, i.e., people who were born in a different country from the one they are living in more accurately estimated their organic share.  However, these effects should be interpreted with caution, as age also significantly positively correlated with the chosen share of organic products (Age: r = .16, p &lt; .05). Similarly, non-immigrants had a significantly higher organic share than people with a different country of birth. A higher organic share results in an increased potential for estimation errors, which may have contributed to this observation.

In addition, the regression model revealed the presence of allergies as a significant predictor, i.e., participants without allergies estimated their share more accurately. A post-hoc t-test revealed a significant  ( *p &lt; .* 05)  distinction in estimation error between individuals with allergies and those without. However, this is based off of only n=38 of participants who gave an estimation and also reported to have food allergies (vs. n=198 without allergies).

| Variable                                                                     | Estimate  (Std. Error)   | p-value   |
|------------------------------------------------------------------------------|--------------------------|-----------|
| (Intercept)                                                                  | 6.41 (10.07)             | 0.525     |
| Social desirability sum score                                                | -0.24 (0.83)             | 0.771     |
| Cognitive Factors                                                            |                          |           |
| UTAUT_effort                                                                 | -0.88 (1.03)             | 0.398     |
| TLX_mentalLoad                                                               | 0.16 (0.05)              | 0.001     |
| TLX_effort                                                                   | -0.07 (0.05)             | 0.132     |
| Experience with online food shopping (vs. no experience)                     | 1.93 (2.19)              | 0.378     |
| Positive Attitude towards online grocery shopping  (vs. neutral to negative) | 0.89 (2.16)              | 0.682     |
| Conscious Food Choice Deliberation                                           |                          |           |
| Self-reported degree of deliberation                                         | 2.56 (1.22)              | 0.037     |
| Avg. time spent deliberating each item                                       | -0.12 (0.07)             | 0.090     |
| Money spent (excl. beverages)                                                | -0.01 (0.02)             | 0.420     |
| No dietary restrictions (vs. flexitarian, vegetarian or vegan)               | -1.49 (2.09)             | 0.478     |
| Allergies (vs. no allergies)                                                 | 9.56 (2.38)              | <0.001    |
| Health-related identity                                                      | 2.32 (0.90)              | 0.010     |
| Strong environmental identity (vs. neutral to negative)                      | -0.07 (1.95)             | 0.972     |
| Sociodemographic Characteristics                                             |                          |           |
| Education                                                                    | -0.11 (0.67)             | 0.874     |
| Income                                                                       | -0.74 (0.58)             | 0.205     |
| Female Gender (vs. male)                                                     | -0.09 (1.96)             | 0.963     |
| Age                                                                          | -0.19 (0.09)             | 0.038     |
| Single-household (vs. family)                                                | -0.17 (2.38)             | 0.945     |
| Born in Germany / Austria / Switzerland  (vs. another country)               | 4.66 (2.08)              | 0.026     |

Table 6: Results of linear model with a reduced sample of n=222, after excluding participants with missing values. Dependent variable: Absolute deviation between the estimated and the observed share of chosen organic products. Multiple R²:0.2352, Adjusted R²:0.1632, F(19,202)=3.268, p &lt; .05."
1_UNDERSTANDING_AND_-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/1_UNDERSTANDING_AND_-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/1_UNDERSTANDING_AND_-with-image-refs_artifacts/image_000006_b261b90770fd59ea5cb50519db71ed7c81f8da27b61eddaee730475e1d6fead3.png,"## Key Insights from Household Habit Shifting:

The Time of Use (ToU) tariff is presented as an opportunity for savings, contingent on the consumer's ability to shift electricity usage to off-peak hours when rates are lower. However, a critical analysis of the simulation results reveals that this structure may disproportionately penalize vulnerable ""Stay at home"" households, for whom shifting energy consumption is not a simple choice but a significant burden that can compromise well-being. The simulation compares bills for two family types, demonstrating the severe financial consequences of not being able to shift consumption patterns. While the ""Out of home family"" is less affected, the ""Stay at home family"" faces a different reality. With their original habits, this family faces a significant financial penalty, with their ToU bill simulated at RM 536.96 a cost increase of RM 97.11 compared to the General Tariff.

Tariff Bill Comparison

Figure 7: Simulated Monthly Bills: Original vs. Shifted Habits for Domestic Users

Description: This bar chart directly compares the estimated monthly bill under the Time of Use (ToU) tariff for each domestic family type, showing both their 'Original' consumption habits and their 'Shifted' habits (where usage is moved to offpeak times).

The simulation suggests that only by actively shifting habits can this family mitigate the penalty, lowering their bill to RM 423.82 to achieve a modest saving of just RM 16.03. The stark difference between a nearly RM 100 penalty and a RM 16 saving represents the immense financial pressure placed on these households to alter essential daily routines. This inadvertently highlights a penalty on essential lifestyles, as the higher initial ToU bill is a direct consequence of their need to use electricity at home during the day. The premise that households with higher peak-hour consumption have the ""most to gain' is therefore flawed, as it overlooks that these are often the households with the least flexibility. For them, ""shifting habits"" is not an optimization but a compromise on essential needs. Ultimately, the model's requirement of ""active management and shifting of energy-intensive activities' places an undue expectation on caregivers, the elderly, or work-from-home professionals, turning a supposed benefit into a source of potential increase in efforts.

Weekday Hourly (kWh/day)

Out of home family (Shifted)

Weekday Hourly (kWh/day)

Figure 8: Visualizing Original vs. Shifted Hourly Consumption Profiles for Domestic Families"
Algorithmic_Fairness-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/Algorithmic_Fairness-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/Algorithmic_Fairness-with-image-refs_artifacts/image_000000_1f1ddc421b65e44fabb784895bf0b683a2dc6cf19413b38289689f98a5ec1383.png,"## Legal and Operational Requirements

The legal and operational requirements affected the fairness perceptions of the participants because they limited the opportunities to implement a shift schedule they perceived as fair. Legal requirements reduced

the flexibility to adapt shift schedules to the healthcare workersÕ preferences because of mandatory rest periods between shifts or the number of allowed shift type rotations that had to be satisfied: Ò The labor law, [...], I do not find it fair. [...] It could have been scheduled differently, but you are not allowed to do so. For example, due to the rest periods Ó (I10). Similarly, personnel shortages and staff turnovers were recognized as a source of perceived unfairness since a shift schedule satisfying operational requirements could consider fewer preferences in this case. The addition of this theme marked the most notable enhancement compared to the OJT. On the one hand, in times of personnel shortages in the entire healthcare sector (OECD, 2023), it  is  not  surprising  that  the  resulting  inflexibility  to  adjust  shift  schedules  is  recognized  by  healthcare workers. On the other hand, legal requirements for shift schedules are mainly perceived as unfair. We argue that  legal  requirements  may  proactively  avert  situations  that  are  perceived  as  unfair,  such  as  ignoring contractual agreements. Still, employees may only notice instances where legal requirements impede their fairness perceptions and subconsciously ignore others where these requirements prevent unfair situations.

Figure 1. Thematic Map"
Analyzing_Income_Ine-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/Analyzing_Income_Ine-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/Analyzing_Income_Ine-with-image-refs_artifacts/image_000002_fb1cf71533bf0d6714a279468b4c1f08f6a304b645afc7097bc6e8450fb24c37.png,"## 4. Inequality of Income Across Italian Regions

Inequality across the Italian regions between 2004 and 2020. The  analysis  of net  income inequality (S80/S20)  across  Italian  regions  from  2004  to  2020  reveals  significant  disparities  in  how  income distribution has evolved over time. While some regions have successfully reduced inequality, others have experienced  notable  increases,  particularly  in  the  South,  exacerbating  the  existing  economic  divide between northern and southern Italy. There is a slight fall in income inequality in Piemonte and Valle d'Aosta with absolute falls of -0.1 and a relative fall of -2.08% and -2.44%, respectively. This indicates a fairly stable economic scenario in the two regions with minimal deviation of the richest and the poorer part of the population. Trentino-Alto Adige and Emilia-Romagna also record slight falls in the S80/S20 ratio with a fall of -0.2 each. The falls imply that economic interventions or territorial dynamics stabilized the income distribution. Marche and Basilicata record the greatest falls in the S80/S20 ratio with falls of -9.76%  and  -12.24%,  respectively.  The  improvements  might  be  linked  with  social  interventions, economic  diversification,  or  demographic  factors  that  contributed  toward  expanding  the  wealth distribution  base.  But  a  few  of  the  areas  registered  notable  increases  in  income  disparities.  Liguria registered an 11.54% improvement in S80/S20 ratio that signifies a widening of the wealth and poverty gulf. The 4.08%, 9.76%, and 7.14% respective percentage variation rates increases also take place in the areas of Lombardia, Veneto, and of the state of Lazio. The direction of the increases in the latter areas signifies  that  although  they  rank  among  the  richest  in  Italy,  the  economic  development  may  not necessarily be equally distributed. The same direction of the increases also follows in the states of the Friuli-Venezia Giulia and of the states of the state of the Umbria and of the state of the Molise, although the increases of the latter states are smaller in scale. The strongest trends follow in the southern half of Italy with the top- and lower-income brackets' gulf widening enormously. The highest 20.97% growth of the S80/S20 ratio has the state of the Campania with an estimate of 7.5 in 2020. This dramatic leap marks the entrenched economic deprivations of the state of the highest employment rates and limited economic opportunity. The large increases of 17.65% and 17.31% also follow in the states of the state of the Puglia and of the state of the Sardegna and the smaller increases of 4.92% and 4.35% also follow in the states of the state of the Calabria and of the state of the Sicily, respectively. The widening disparities of the latter four states reveal underlying structural factors such as lower-quality labor markets and lower rates  of  industrialization  and  entrenched  socio-economic  deprivations  that  disproportionately  hit  the

lower-income  deciles.  The  evidence  also  highlights  the  widely-documented  north-south  economic cleavage of Italy. The north and center of Italy has experienced stable or slightly expanding disparities and southern Italy has experienced expanding disparities. Underpinning explanations of the trends will be the varied patterns of the development of the industry, employment prospects, government investment and social policies. Policy interventions against the disparities will be necessitated including investment in the education system, employment generation and development of the infrastructure with the intention of catalyzing the economic development of the South (Figure 2). Not adopting such interventions will further widen the disparities of southern Italy and entrench economic disparities at the expense of social mobility and economic prospects of future generations (Kocurová and Hampel, 2020; Daniele, 2021; Guzzardi et al., 2024; Rossi et al., 2024; Sbardella et al., 2021).

Figure 2. Inequality across the Italian regions between 2004 and 2020.

2020

Inequality across the Italian macro-regions between 2004 and 2020. The net income disparities statistics of Italy's macro-regions during the period 2004-2020 demonstrate a widening and permanent economic distance between the South and the North. The income disparities have increased throughout the entire macro-region set except that the relative growth has been highest in the Mezzogiorno and the South

specifically. The income disparities growth has been slight in the North. The general S80/S20 ratio of the macro-region of the north has gone up from 4.7 in 2004 to 4.9 in 2020 with a 4.3% increase. Within this macro-region, the increment has been slightly higher at 6.1% with the ratio going up from 4.9 to 5.2. This signifies that economic growth in the industrious areas has been good and that wealth has been slightly  unevenly  distributed.  The  macro-region  of  the  North-East  with  the  territories  specializing  in powerful small and middle-size firms had a slight increment of 2.3%, and the ratio had gone up to 4.85.2. This signifies that income disparities in this macro-region had been higher and had not changed that dramatically compared with the macro-region of the North-West. The income disparities had gone up in the center of Italy from 4.8 to 5.2 with an 8.3% increment. This signifies that economic disparities had been widening in this macro-region that has the major cities of Rome and Florence among them. The economic disparities may be explained by the workforce transformation in the employment markets and the rural-urban  disparities and  economic  reforms  that  had  benefited  the  higher-income  classes disproportionately. The highest and the most disquieting tendency has been the widening disparities of the Mezzogiorno macro-region with a general increment of the ratio of 6.0 in 2004 to 6.5 in 2020 with an 8.3% increment. In the Mezzogiorno, the South (Sud) posted the steepest growth at 6.4 and 5.7 and a 12.3% relative growth-the highest relative growth among the macro-areas. This reflects an widening gap between the richest and the poorest households in the South and possibly the effect of underlying economic  structural  issues  and  the  influence  of  high  rates  of  unemployment  and  limited  economic prospects. The islands (Sicily and Sardinia) also posted a notable growth in the S80/S20 ratio with the latter advancing at 7.0 and an associated 6.1% growth. This further reflects economic hardships in the two regions with heavy dependence on seasonal activities such as agriculture and the tourism sector that leads to income volatility and economic disparities. Nationally, Italy's S80/S20 macro area also posted an  increase  at  5.9  and  5.6  and  reflects  a  5.4%  growth  in  income  disparities  during  the  16  years  of observation. This development emphasizes the necessity of the efforts of the country toward equitable economic development. The north has been able to keep the problem of inequality at bay at least to a certain limit, the South and the islands posted widening disparities that reinforce Italy's old economic dualism. The statistics further imply that unless the government intervenes with specific interventions such as investment in the South's infrastructure and education and the provision of employment-these disparities between the two areas will be hard to stem and widen further in the future leading to further social and economic cleavages (Figure 3). The widening disparities in the center and north also imply the necessity of economic development interventions that encourage inclusive economic development and that the wealth that ensues will be dispersed among a larger population and not restricted among the higher-income earners (Sbardella et al., 2021; Culotta,  2021; Aresu et al., 2023).

Figure 3. Inequality across the Italian macro-regions between 2004 and 2020.

2020"
Artificial_Intellige-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/Artificial_Intellige-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/Artificial_Intellige-with-image-refs_artifacts/image_000000_eca8b8d652b31d34c335984eb670c6e71ec6ecb28785906aa95e52054744f065.png,"## 3.2 AI in Account-Based and Customer-Based marketing

Account-Based Marketing  (ABM) is a strategic  marketing strategy that targets specific high-value accounts instead of a general audience. It is a personalized marketing effort based on the needs and interests of each account to ensure increased engagement  and  conversion  rates.  AI  supports  ABM  by analyzing data automatically, facilitating predictive targeting, and personalizing content. Machine  learning techniques  assist  in  detecting  the  best  potential  accounts, whereas AI-powered automation supports real-time interactions  in  the  form  of  chatbots,  dynamic  content,  and tailored messages.

AI-powered ABM has been implemented effectively by some firms through the utilization of AI-driven analytics platforms that  enable  them  to  discover  crucial  decision-makers  at targeted accounts and build highly personalized campaigns to enhance conversion rates. ABM uses AI to boost efficiency, enhance  lead  scoring  precision,  and  give  insights  about customers.  Nonetheless,  issues  like  data  privacy  issues, integration challenges, and excessive costs of implementation need to be tackled for successful implementation.

Fig 1 - AI-Driven Account-Based Marketing (ABM) Workflow

Customer-based marketing (CBM)  is  an  approach  that emphasizes individual  customer  behavior,  preferences,  and engagement patterns to provide highly individualized marketing experiences. In contrast to Account-Based Marketing (ABM), which personalizes campaigns for individual high-value accounts, CBM  aims to serve individual  consumers  in  multiple  segments.  AI-powered CBM  leverages  sophisticated  data  analytics  and  machine learning to forecast customer behavior, automate individualized  recommendations,  and  optimize  customer journeys.

For instance, online retail websites like Amazon utilize AI to browse  through history and past purchases to suggest relevant  products,  whereas  streaming  services  like  Netflix utilize AI-based  recommendation  algorithms  to  provide personalized  content  to  each  user.  Similarly,  ride-sharing companies  like  Uber  and  Lyft  utilize  AI  to  implement dynamic pricing models depending on demand, user location, and past patterns of rides. By analyzing customers' interactions  in  real-time,  AI  enhances  CBM  by  offering customized promotions, optimizing email marketing campaigns, and facilitating more efficient customer interactions."
Automated_Video_Anal-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/Automated_Video_Anal-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/Automated_Video_Anal-with-image-refs_artifacts/image_000003_6fdfe755f8701b05ee3f5c863a82743eedc6b98d59d9c8b63b72014486c42528.png,"## Search Query

TS=(video* OR clip* OR spots OR livestream* OR ""live-streaming"" OR trailer*) AND (SO=('Journal of Marketing' OR 'Journal of Marketing Research' OR 'Journal of Consumer Research' OR 'Journal of Consumer Psychology' OR 'Journal of the Academy of Marketing Science' OR 'Marketing Science' OR 'Management Science' OR 'International Journal of Research in Marketing' OR 'Journal of Retailing')) AND (PY=(2000-2024))

Note. TS = topic, which means title, abstract, keywords plus, and author keywords of publications are searched for these words.

Study Selection. We identified studies in the selected journals by searching the WoS database using the search query outlined in Table 1, which initially returned 255 studies. These 255 studies were screened in two stages based on the inclusion and exclusion criteria outlined in Table 2 (see the flow diagram in Figure 3).

In the first stage, the author manually screened the identified studies' titles, abstracts, and keywords, excluding 211 papers. These were removed for reasons such as being conceptual studies, being unrelated to marketing, and not conducting any video analysis but examining contexts like the video game industry, movies, or video streaming platforms, among others. These exclusions ensured that only studies relevant to the research objectives advanced to the second stage of the screening.

In the second stage, the same author reviewed the full manuscripts of the remaining 44 studies, informed by the inclusion and exclusion criteria. An additional 19 studies were excluded in which humans conducted the video analysis manually or the study lacked sufficient methodological details. This process resulted in a final set of 25 publications for further analysis.

Table 2. Inclusion and Exclusion Criteria

| Criterion          | Inclusion                                                                                                                                                                                                                                                                   | Exclusion                                                                                                                                                                                                                                                                                                                                              |
|--------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Study Type         | Empirical Studies                                                                                                                                                                                                                                                           | Conceptual or Non-empirical Studies                                                                                                                                                                                                                                                                                                                    |
| Method             | Quantitative Research                                                                                                                                                                                                                                                       | Qualitative Research                                                                                                                                                                                                                                                                                                                                   |
| Language           | English                                                                                                                                                                                                                                                                     | Non-English                                                                                                                                                                                                                                                                                                                                            |
| Publication        | Published Work                                                                                                                                                                                                                                                              | Unpublished Work (e.g., working papers, preprints)                                                                                                                                                                                                                                                                                                     |
| Publication Titles | International Journal of Research in Marketing, Journal of Consumer Psychology, Journal of Consumer Research, Journal of Marketing, Journal of Marketing Research, Journal of Retailing, Journal of the Academy of Marketing Science, Management Science, Marketing Science | All other journals, except the list of journals on the left                                                                                                                                                                                                                                                                                            |
| Article Type       | Full-text Articles, Early Access, Online First                                                                                                                                                                                                                              | Abstracts, Proceedings/Conference Articles                                                                                                                                                                                                                                                                                                             |
| Context            | Marketing                                                                                                                                                                                                                                                                   | Non-Marketing                                                                                                                                                                                                                                                                                                                                          |
| Date               | 2000-2024                                                                                                                                                                                                                                                                   | Any study published before 2000 or in 2025                                                                                                                                                                                                                                                                                                             |
| Relevance          | /                                                                                                                                                                                                                                                                           | • Studies that do not contain video analysis (e.g., research context is the video game industry) • Studies that involve video analysis related to non- marketing contexts (e.g., security surveillance) • Studies which focus on non-automated video analysis or manual coding of video features • Studies that lack sufficient methodological details |

Figure 3. Flow Diagram of the Screening Procedure

Data Extraction and Synthesis. We extracted and synthesized the relevant data to address our research questions: (1) Which information have scholars automatically extracted from video data, and (2) which methods, software, tools, and techniques have they employed? We manually screened each manuscript, concentrating on the methodological and empirical sections. Where papers did not include sufficient details about the video analysis, we consulted the supplementary Web Appendix for the necessary information.

We structured the information on the automated video analysis in the papers into three categories, following Schwenzow et al. (2021), Li et al. (2019), Wang et al. (2024), and Zhou

et al. (2021): (1) video data characteristics , (2) technical video analysis approach , and (3) extracted video features . For each category, we defined subcategories to allow for a more indepth interpretation of the information in the papers. Figure 4 shows this framework for automated video analytics, including the categories, subcategories, and examples of the information retrieved from the 25 studies.

Figure 4. Framework for Automated Video Analytics: Key Dimensions and Approaches

For instance, video data characteristics include the subcategories of the modality of video data (i.e., audio, visual, text; Grewal et al., 2022), the extraction level (i.e., video, scene, or frame-level), and the frame rate when applicable. The modality of video data refers to the different ways information is communicated, such as through numerical indicators (e.g., number of likes or star ratings), text, audio, or visual content (Grewal et al., 2022). The extraction level specifically refers to the granularity at which features are extracted from a video, encompassing frame-level (individual frames), segment-level (series of frames), and video-level (entire sequence) analyses (Xu et al., 2024). The frame rate is the number of individual frames or images displayed per second in a video, typically measured in fps (Tekalp, 2015).

The technical video analysis approach category covers six subcategories. The first is the video analysis task : what should be analyzed in a video, e.g., object detection or audio classification. The second subcategory is the algorithm family , which refers to a broad category of algorithms grouped based on their underlying principles, methodologies, or intended tasks (Goodfellow et al., 2016). For example, neural networks and traditional CV are different algorithm families, each with distinct approaches to problem-solving. The third subcategory is the algorithm type , which specifies a subset within an algorithm family detailing the architecture, structure, or mechanism used to solve a task (Vaswani et al., 2017). For example, CNNs, recurrent neural networks (RNNs), and transformer-based models are types within the neural network family. Fourth, the specific models or algorithms subcategory refers to implementations of an algorithm type designed for a specific task or domain (Simonyan &amp; Zisserman, 2014). Examples include VGG (Visual Geometry Group; a CNN for image classification), OpenCV (a library for CV tasks), and GPT (Generative Pre-trained Transformer; a transformer-based model for natural language processing). The fifth subcategory is the type of solution , meaning if the software or tool is open-source (e.g., OpenCV) or proprietary (e.g., Google Vision, Microsoft Azure). The last subcategory, model training , categorizes if the model was pre-trained or was customized for the use case in the respective paper. In video analytics, model training is the process of optimizing a machine learning model's parameters using labeled or unlabeled video data to enable it to recognize patterns, extract features, and perform tasks such as object detection or action recognition, either through pre-trained or custom approaches (Goodfellow et al., 2016; Karpathy et al., 2014).

We separate extracted video features into the subcategories of content features (e.g., facial emotions, body language) and structural features (e.g., color brightness, scene cuts, duration) (Schwenzow et al., 2021; Lang et al., 1993). Structural features are lower-level technical aspects of videos that shape the video's presentation (Schwenzow et al., 2021).

While less interesting, structural features are objective and therefore important for analysis control. In contrast, content features focus on what is presented in the video, including visual elements like faces and objects, as well as viewer perceptions, such as emotions, providing higher-level insights (Schwenzow et al., 2021).

From this systematic literature review, we developed a coding scheme to systematically classify video analysis methods in the reviewed papers, reducing complexity and standardizing diverse information. For example, we categorized tasks involving detecting hands, objects, or people under 'object detection.' If researchers used ChatGPT for text analysis, we classified it as a transformer-based algorithm within the neural network family. This structured approach allows for more precise interpretation and synthesis of data across key dimensions, helping us answer the first two research questions. The detailed coding scheme is available in Web Appendix A."
Confessions_of_a_Gre-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/Confessions_of_a_Gre-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/Confessions_of_a_Gre-with-image-refs_artifacts/image_000000_2f1139537d93c2f65c31e3372103c1e4c692f528ef0b9ccfca4252a6b7fc1ac6.png,"## Conceptual Framework

To investigate the complex pathways through which religiosity influences green purchase intentions, this  study  proposes a  conceptual framework that integrates motivational and social influences. The model positions conspicuous virtue signaling, both self-oriented and other-oriented, as key mediators linking intrinsic and extrinsic religiosity to sustainable consumer behavior

Fig. 1 Conceptual framework"
Data-Driven_Innovati-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/Data-Driven_Innovati-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/Data-Driven_Innovati-with-image-refs_artifacts/image_000003_e3dad1a0685b22e9d0df826d87150d2c12f572f48c2d547de04986ba042db88a.png,"## 5.1. Existing Frameworks

Schemes have been proposed by governmental initiatives to categorise the risks posed by AI-centric artefacts. The AI Act xxiv agreed upon by the EU Parliament and EU Council classifies the risk associated with AI systems as prohibited, high, limited, and minimal. This classification can also be understood in accordance with Maslow's Hierarchy of Needs (Mcleod, 2007), as depicted in Figure 5.1. Prohibited are the livelihood and security-threatening systems, e.g., social scoring, biometric categorisation, and manipulative systems, e.g., artificially inferring emotions for profitable and malicious intentions.

Maslow's Needs Classification

Al Act Risk Classification

Figure 5.1 : The EU AI Act classification of risk associated with AI systems understood in accordance with Maslow's Hierarchy of Needs.

| Physiological Needs air; water; food, shelter; clothing, sleep_                                                         | Prohibited Risk behaviour manipulation, biometric categorisation, social scoring; emotion inference                          |
|-------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------|
| Safetyand Security health, security, employment; property-_                                                             | High Risk Public infrastructure and services, education, safety; employment; law, immigration, and justice                   |
| Love and belonging friendship, intimacy; family, connection _ Self-esteem respect; recognition, individuality, freedom_ | Limited risk General purpose Al systems requiring less transparency obligations, e.g., chatbots, deepfakes, content creation |
| Self-actualization morality, creativity; purpose, inner potential___                                                    | Minimal risk Unregulated systems such as Al video games, product recommendations, grammar check, photo editors               |

Under the high-risk category, the AI Act includes public infrastructure and services such as education, employment, law, migration, and justice etc. For the developers of high-risk AI, the AI Act mandates a risk management system, data governance, technical documentation, record keeping, usage instructions, human oversight, performance, and quality control. For systems with limited risk, e.g., chatbots, content creators,  etc.,  the  AI  Act  mandates  technical  documentation,  copyright  compliance,  and  usage instructions, while being transparent with the users that they are interacting with AI. The systems with minimal risk include self-development, creativity-enhancing, entertainment tools, etc.

In  literature,  Alfrink et  al. (2024,  p.  55)  classify  child  protection,  public  housing,  health,  social protection, security, taxation, etc., under public AI and built environment and mobility solutions like electric vehicle charging, parking systems, etc., under urban AI. Fernández-Llorca and Gómez (2023, p.  32)  review  state-of-the-art  autonomous  vehicles  in  accordance  with  the  recommendations  for trustworthy AI (see Table 2.1). They find that 'Technical Robustness &amp; Safety' is high ly mature, while other recommendations have low and medium levels of maturity."
Decentralized_Distru-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/Decentralized_Distru-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/Decentralized_Distru-with-image-refs_artifacts/image_000002_9a8bd8ef481eb799a2143b091186a8de750a6d830122d03c3388c0f0869dc26a.png,"## Results

Perceived Payment Method Risk . As predicted, a one-way ANOVA revealed a significant main  effect  of  the  payment  method  ( F (1,  595)  =  235, p &lt;  .001).  The  results  confirmed  that cryptocurrencies  were  perceived  as  a  significantly  riskier  payment  method  compared  to  more traditional payment methods (M Cryptocurrencies = 4.13, SD Cryptocurrencies = 1.74 vs. M CreditCards = 2.20, SD CreditCards = 1.32, p &lt; .001).

Consumer  Sentiment . We  observed  significantly  more  negative  aspect  sentiment  for cryptocurrencies compared to credit cards as a payment method (see Figure 2:A; M Cryptocurrencies -.24, SD Cryptocurrencies = .60 vs. M CreditCards = .29, SD CreditCards = .54, p &lt; .001). Specifically, our study revealed that consumers have concerns regarding the security and usability of cryptocurrencies as a payment method (see Figure 2:B; M Cryptocurrencies = -.15, SD Cryptocurrencies = .64 vs. M CreditCards = .37, SD CreditCards =  .49,  p  &lt;  .001).  They  are  particularly  dissatisfied  with  the  payment  options (M Cryptocurrencies = -.15, SD Cryptocurrencies = .63 vs. M CreditCards = .24, SD CreditCards = .53, p &lt; .001) and worried about the associated purchase currencies and fees (M Cryptocurrencies = -.36, SD Cryptocurrencies = .54 vs. M CreditCards = .12, SD CreditCards = .63, p &lt; .001).

Mediation .  To  test  whether  perceived  payment  method  risk  mediates  the  relationship between the  payment  method  and  consumers'  firm  attributions  (i.e.,  consumer  sentiment),  we applied  PROCESS  Model  4  (Hayes,  2022)  using  10,000  bootstrapped  samples  (credit  cards dummy-coded  as  0  and  cryptocurrencies  as  1).  As  predicted,  the  negative  direct  effect  of cryptocurrencies as a payment method on consumers' firm attributions (b Direct = -.216, 95% CI Direct : [-.295; -.138]) was significantly mediated via first enhancing the perceived payment method risk (b PaymentMethod = 1.945, t(594) = 15.4, p &lt; .001), which in turn reduced consumers' firm attributions (b PaymentMethodRisk =  -.147,  t(593)  =  -13.4, p &lt;  .001).  The  indirect  effect  through  the  perceived

payment method risk was significant with the 95% confidence interval excluding zero (b Indirect = -.286, 95% CI Indirect : [-.342; -.237]).

Topic

Figure 2: Consumers express negative sentiment towards cryptocurrencies (vs. credit cards)"
Deucalion__A_dataset-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/Deucalion__A_dataset-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/Deucalion__A_dataset-with-image-refs_artifacts/image_000000_9bf6e8c0f52945b21ae1f693d071d8a792da05daf565da766a71fc554866e834.png,"***Table 1:*** *flood classes*

| Deucalion’s classes as of June 2025   | Deucalion’s classes as of June 2025   |
|---------------------------------------|---------------------------------------|
| 1. flooded                            | 8. people                             |
| 2. vehicle                            | 9. pets                               |
| 3. destroyed trees                    | 10. wet surfaces                      |
| 4. vegetation                         | 11. sea                               |
| 5. other damage                       | 12. rain                              |
| 6. rocks and mud                      | 13. other meteo                       |
| 7. ruins                              | 14. river                             |
| 15. pool                              |                                       |"
Forbidden_Fruit_and_-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/Forbidden_Fruit_and_-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/Forbidden_Fruit_and_-with-image-refs_artifacts/image_000000_deb33a6a82013c3f69c68588de0019ff6829c4720a3e8d28efd17d72e9083996.png,"**4.0 Model Construction**

The present framework proposes a dual-stage theoretical model to explain how migrants initiate and then escalate engagement in an addictive consumption behavior after moving from a prohibitive to a permissive environment. We label the stages **Stage 1: The Forbidden Fruit Effect** (behavior initiation, transitioning from 0→1) and **Stage 2: The Rebound Effect** (behavior escalation, from 1→n). In Stage 1, drawing on **psychological reactance theory** (Brehm, 1966) and **commodity theory** (Brock, 1968), we argue that stringent prohibition in the country of origin imbues the forbidden behavior with heightened allure, leading to first-time adoption once the individual is in a freer context. In Stage 2, informed by **rebound effect theory** (an economic concept of over-consumption after efficiency gains or constraints removal) and coping dynamics from psychology, we explain how initial adoption can spiral into excessive or compulsive consumption. **Figure 1** illustrates the conceptual model, highlighting key constructs in each stage and their relationships.

*Figure 1: A dual-stage conceptual model of addictive consumption initiation and escalation among migrants. Stage 1 (Forbidden Fruit Effect) depicts how strong* ***perceived prohibition*** *in the origin context fosters* ***symbolic desire*** *and a* ***latent craving*** *for the forbidden behavior, culminating in* ***first use*** *upon migration. Stage 2 (Rebound Effect) shows how the* ***constraint release*** *of migration triggers an* ***opportunity shock*** *(initial surge in consumption), which—especially under* ***stress reinforcement*** *(using the behavior to cope with post-migration stress)—leads to* ***compulsive escalation*** *of the behavior.*"
Moderating_Tamil_Con-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/Moderating_Tamil_Con-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/Moderating_Tamil_Con-with-image-refs_artifacts/image_000011_593471f7983fe787471624165cd2e5752ca64e7bae7c368f79a4575010c4c759.png,"## /gid00024/gid00035/gid00052/gid00001/gid00031/gid00042/gid00001/gid00052/gid00042/gid00048/gid00001/gid00047/gid00035/gid00036/gid00041/gid00038/gid00001/gid00047/gid00035/gid00032/gid00001/gid00046/gid00042/gid00030/gid00036/gid00028/gid00039/gid00001/gid00040/gid00032/gid00031/gid00036/gid00028/gid00001/gid00043/gid00039/gid00028/gid00047 /gid00033/gid00042/gid00045/gid00040/gid00001/gid00045/gid00032/gid00040/gid00042/gid00049/gid00032/gid00031/gid00001/gid00052/gid00042/gid00048/gid00045/gid00001/gid00030/gid00042/gid00041/gid00047/gid00032/gid00041/gid00047/gid00169

- Figure 3. Reasons participants identified as to why social media platforms removed or restricted their content (number of survey participants who stated that they have experienced content removals or restrictions, n= 83). Source CDT's online survey (NovemberDecember 2024).

takedowns and actions perceived to suppress the reach of their posts were taken to 'silence' their voices online. These suspicions were pervasive and were repeated in one-on-one interviews with Tamil creators, digital rights advocates, and content moderators.

One longstanding Tamil computing expert and former moderator noted the following when asked about his own experience facing moderation:

'It doesn't matter if it's in Tamil or English, when you are talking against the government the same thing happens. It's not the language, it's the bias. The post will be reported as spam and taken down as violating the community guidelines [...] Community guidelines are so vague. It's not like someone is sitting there and responding to us to understand what exactly went wrong. They'll just say 'You violated the community guidelines' so we read all the guidelines. We don't understand, what have we violated? I think it's for their own legal safety.' (Former moderator, October 2024, India)

Another interview participant suspected politically-driven moderation because they posted about Tamils fleeing persecution from Sri Lanka during the civil war. Multiple interview participants pointed to research and human rights advocacy that has long argued that over-moderation in the Sri Lankan context stifles social movements and 'obstructs

accountability' (Amarasingam &amp; Nandakumar, 2021; Ethirajan, 2021). One interview participant said that even when people posted images of shopfronts containing the very common name of 'Prabhakaran,' which is also the name of the slain leader of the L TTE, their posts would be taken down. One interview participant said that symbols that seemed to be associated with the Tamil nationalist movement were detected and taken down on social media, such as the gloriosa lily. 11 They were not notified whether their post was taken down due to a state request for takedown or proactive moderation, although Sri Lankan and Western media reporting suggest frequent requests for takedowns coming from the government (Constine, 2018; Tamil Guardian, 2024; Mallwarachi, 2024):

'There was a point during COVID, when there were no mass events. There were a few online activations for November 27th and people were sharing their respect online. This was a period where people weren't posting the [LTTE] flag or actual pictures of members or uniforms, but were still having content being taken down. The gloriosa lily, I had a friend post a picture with her friend holding lilies saying 'We remember and resist' and the post was taken down. It was shocking. How can a flower be violent? How did they find this?' (Sri Lankan Tamil journalist and digital rights advocate, January 2025, United Kingdom)

A few interview participants also suspected that they experienced opaque moderation, more commonly known as 'shadowbanning,' when they used certain terms in Tamil or another language. Shadowbanning is a colloquial term for a practice that encompasses a broad range of undisclosed content moderation actions, such as hiding a user's posts from other users, removing a user's handle or posts from search, or ranking users' content so low in a recommendation system that it is less discoverable (Nicholas, 2022). Prior research showed that, without clear disclosure from the platform about the circumstances under which it might moderate a user's content without informing them, users develop their own rationale for why their posts face a reduced reach and find ways to circumvent this type of intervention (Savolainen, 2022; Nicholas, 2022). A Tamil political commentator we interviewed emphasized that some of their political content gets less engagement:

11 The gloriosa lily is seen as a symbol and the national flower of the Tamil Nationalist movement, which has become synonymous with the Liberation Tigers of Tamil Eelam (LTTE) because it contains all the colours contained in the Tamil Eelam national flag.

'Especially when I write a lot about the RSS 12 , it goes to the lower feed. I have 54,000 followers but my reach is very limited. Certain posts where you use the word Gaza or Palestine or RSS or BJP, you know you'll only get…I can close my eyes and say in 10 minutes I'll get three likes. That's for sure. That's how I know it's been moved to the lower feed.' (Tamil journalist and digital rights advocate, November 2024, India)

We found that platforms do rely on government input to shape content moderation and often did not disclose this to users. Based on our interviews with former and current platform representatives, platforms often rely on government guidance to shape content moderation in Tamil in the following ways.

First, Indian and Western social media platforms often comply with state requests for takedown or user information with little notice to users. A moderator described that:

'[Platform] was pretty clear on if a request to takedown a specific content is going to come from a government of a country that states that it is legally obliged to do so, then perhaps [platform] was obliged to remove those content.' (Moderator on social media platform, December 2024, India)

Second, Indian and Sri Lankan laws related to intermediaries are often interpreted by Indian and Western social media platforms in overbroad and subjective ways resulting in takedown of Tamil content. Laws like the IT Act in India and the Online Safety Act, No. 9 in Sri Lanka require online services hosting user-generated content in the respective jurisdictions to take down illegal or harmful content, with differing degrees of specificity. These requirements have been interpreted in different ways by online services, and often are burdensome to smaller, local intermediaries (Kumar et. al, 2022). This has resulted in Indian platforms, and even Western ones, directly incorporating them into their community guidelines and guidance to moderators, as a head of a policy team at an Indian social media company indicated:

'The general principle was what came down from the IT Law… You cannot make comments that would create public disharmony

12 The Rashtriya Swayamsevak Singh or RSS is a Hindu nationalist organisation and the parent organization of the Bharatiya Janata Party (BJP), which is a political party, the party which is currently in power federally in India (Britannica, n.d.).

or discord which means you can't make comments about gods… For instance, Indian law does not allow you to denigrate the national flag, so you can't post content that would denigrate the national flag. Our community standards were largely a subset of that… If something happens on a platform and the police would reach out. If you posted it, you started a riot, the police will ask us for your details. And we'll provide your details…Similarly, there have been various other instances where they have asked for companies to take down things or keep an eye out for things…we'll be a little more proactive about it.' (Head of policy, Indian social media company, October 2024, India)

These laws result in vast takedowns of content, resulting in distrust in platform moderation that causes users to circumvent appeals processes and even content moderation processes altogether. A moderator of an online forum said that disclosure to users was essential to maintaining trust and navigating a tricky balance between compliance and user trust:

'I may be wrong, but if I'm removing content based on a government's request, and I keep on doing that, the users won't feel trusted that this is a safe space to come and talk. So if I am removing content, I would say to them, 'Because the government said I need to remove it.' If it's the second time, I should at least have a substantive policy to say, 'Hey, look this is some sort of government rule. These are the policies that you need to make sure you don't violate.' You also don't want the government to come back to you again and again and say hey remove this, and this.' (Moderator on a small online forum, November 2024, India)

/gid00001

One employee at a U.S.-based social media company said that distrust in platform moderation results in part from some social media users' perception that companies were aligning themselves with government actors, something that has long been documented by academics, civil society, and media (Sombatpoonsiri &amp; Mahapatra, 2024; Horowitz &amp; Purnell, 2020; Mirza, 2023).

Finally, Western social media platforms often increased their investments into content moderation capacity only after pressure from government actors. In interviews with Trust &amp; Safety staff, there was a sense that teams do not listen to anyone except for the government. For instance, in the aftermath of the Easter Sunday attacks in 2019, where the Sri Lankan government restricted access to social media platforms

under a state of emergency measure, one former policy employee noted that their company did pull out 'breakglass measures' to moderate Tamil content:

'There are breakglass measures, for example, in the aftermath of the Easter Sunday attacks, the prominent narrative was that the company was blindsided because it didn't have classifiers in Tamil or Sinhala and there was all this hate speech going unchecked. Which was definitely the case. But at some point, internally, the company did deploy something called 'Green Lantern,' which I think is a hate speech classifier, and which was able to contain hate speech in those local languages. There was a lot of secrecy around it because I think companies don't want it to be known externally that they have these capabilities. So there's a little bit of ambiguity on whether or not there is an actual resource gap, or whether it was the company's unwillingness or it doesn't suit their strategic needs to deploy resources where they might have them.' (Policy Lead at a U.S.-based social media company, December 2024, India)

In 2018, a year before those breakglass measures were used, companies were accused of failing to moderate instances of incitement of violence amongst Sri Lankan users which led to widespread riots in Sri Lanka. In response to those riots, the Sri Lankan government blocked access to social media services under an emergency order. In 2020, after an investigation commissioned by Meta, one of the companies whose services that order blocked, the company stated that it 'recogni[zed] and apologi[zed] for the very real human rights impacts that resulted' (Brustein, 2020). The company committed to hiring more staff who spoke Sinhalese and Tamil, and to using detection technology in Sinhalese to protect vulnerable groups including Muslims and Tamils (Facebook Sri Lanka Human Rights Impact Assessment, 2020). 13

Ultimately, interview participants argued that the real danger is in the growing self-censorship that threatens freedom of speech and the political environment in the region.

'You have to think a lot before you write and speak. People will automatically self-censor.  If the platforms don't want to censor them, they'll censor themselves. If one platform is not good, you

13 Meta also conducted a human rights impact assessment for India but never published it, despite calls for disclosure from international civil society (Access Now, 2022).

can find another platform. Or you can have your own website or your own server. The platforms taking you down is actually a matter of inconvenience. The real danger is the democratic climate of the country. We are putting ourselves [at] risk. More than the platforms themselves, the real risk is the climate, the democratic climate of the country that you're operating in.' (Former moderator, October 2024, India)"
Quiet_Quitting_-_Per-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/Quiet_Quitting_-_Per-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/Quiet_Quitting_-_Per-with-image-refs_artifacts/image_000003_fbcf3e540f920ab536876d862d894717998bf10f0cc9e5a6dbcf27c271ea9bfb.png,"## 3.4. Quiet Quitting Across Generational Groups

Quiet quitting is not just restricted to younger generations, such as Generation Y and Z. Recent studies (Alisha Johar et al., 2023; Hamouche et al., 2023; Serenko, 2024) show that workers from other previous generations can also manifest quiet quitting behaviors. Therefore, quiet quitting is not only prevalent among iGen workers and Millennials, but also similarly preeminent among

workers from other (older) generational groups, who are dissatisfied with ineffective management within organizations (Mahand &amp; Caldwell, 2023).

This fact has been corroborated by the data available in the Gallup Report (2023), regarding employee engagement with the organization's goals and mission. The Gallup Report tracks employee engagement in thousands of organizations around the world, measuring employees' perspectives on the most crucial elements of workplace culture. In 2023, the report highlights that the majority of workers worldwide fall into the quiet quitting category, with 59% of workers ""not engaged"". Figure 3 shows the percentage of ""not engaged"" workers for the different regions of the world, following the Gallup Report (2023).

Figure 3 - Quiet Quitting by Region, worldwide, according to the Gallup Report (2023)

Looking at Figure 3 reveals a worrying trend of disengagement in the workplace among workers in different parts of the globe. Most regions have a significant proportion of ""not engaged"" workers, with Europe leading the way (72%), followed by Southeast Asia (68%). This data is relevant to understanding the phenomenon of quiet quitting, as worker disengagement can have a negative impact on the work environment and, consequently, lead to high costs in lost productivity. In this regard, Johnson (2023) introduces the concept of a ""safety zone"" to explain why workers may choose to do only the minimum necessary in the workplace, i.e. quiet quitting. This zone is influenced by various factors (e.g. organizational culture, leadership, reward, and recognition

policies). In addition, the author incorporates an economic approach to analyze workers' behavior within the ""safety zone"". According to this approach, workers respond to incentives and opportunity costs in their work environment. If the benefits of committing more at work (e.g. recognition, promotions or salary increases) are not perceived as significant in relation to the costs (e.g. stress, additional effort, lack of recognition), workers may choose to remain in the ""safety zone"" and do only the bare minimum.

Based on these conclusions regarding Johnson's ""safety zone"" (2023), the authors of this article have drawn up a chart, in every way like a Control Chart, which illustrates the concept of the ""safety zone"" in the workplace, where the profile of workers is mirrored in relation to what is expected as a result of their productivity (See Figure 4). The upper threshold refers to the performance that the individual or group can achieve. The lower threshold refers to the performance deemed acceptable by the supervisor. The irregular line in the middle represents actual performance over time, reflecting the real productivity of an individual or group.

Figure 4 - Productivity variation and the ""safety zone"" in the workplace

The elements in Figure 4 help to visually illustrate the variation in productivity over time, highlighting the established performance limits and the actual performance of workers within the ""safety zone"".

The ""safety zone"" represents a state in the work environment in which workers feel comfortable performing the minimum necessary to keep their jobs, without pushing themselves beyond it.

Within this ""zone"", workers avoid attracting attention, either positively or negatively, and tend to avoid additional risks and efforts, opting to fulfill only the minimum expectations in order to avoid problems or conflicts.

Figure 4 also shows worker profiles, which illustrate how workers inside and outside the ""safety zone"" can position themselves in relation to the established performance limits and how their attitudes and behaviors can influence their productivity and engagement at work. Therefore, while ""Middle Workers"" and ""Accommodators"" fall within the safety zone, ""High Performers"",

""Careerists"" and ""Wage Criminals"" tend to operate outside these limits, for different reasons related to their behaviors and goals in the workplace.

Thus, ""High Performers"" are workers who constantly seek to exceed the established upper limit of productivity, actively seeking success and recognition through exceptional performance. These workers show a high level of engagement with their tasks and responsibilities and consistently exceed expectations. They are intrinsically motivated, proactively demonstrate autonomy and initiative at work and are resilient in the face of challenges and setbacks. As well as excelling individually, 'High Performers' are also able to collaborate effectively with teammates and lead projects or initiatives when necessary. They value constructive feedback and always look for opportunities to learn and develop. They are recognized as leaders and role models within the organization, in line with the conclusions of the studies by Hajra and Jayalakshmi (2024) and Pandey and Chauhan (2021).

'Careerists' are workers who also tend to operate above the safety zone, as they are focused on their professional career, working hard, and setting clear goals to achieve professional success. These workers show high professional ambition, with high self-confidence in their abilities. They are proactive in seeking opportunities for growth and development in order to progress professionally, and they value networking and building professional relationships. Although they are highly dedicated to their careers, these workers also value work-life balance, corroborating the studies by Fan and Sheng (2023) and Järlström et al. (2020)

'Middle workers' maintain consistent productivity that does not exceed the upper limit but does not fall below the lower limit either.  These workers try to avoid standing out to their superiors and colleagues, staying within a range of performance considered acceptable (in the ""safety zone""), to avoid problems and drawing attention to themselves. They are reliable and competent workers but may not actively seek opportunities for growth or promotion, as they value stability and work-life balance, corroborating the conclusions of the study by Farivar et al. (2023).

'Accommodators"" are workers who adopt a passive, adaptable and conformist stance in the workplace, following instructions and established norms without questioning or challenging the status quo. They prefer to stay within the established boundaries (""safety zone"") and avoid situations that could result in conflict or friction in the workplace, opting to maintain harmony and stability, even if this means not expressing their opinions assertively or disagreeing with decisions. These workers tend to avoid drawing attention to themselves, preferring to remain relatively ""invisible"" in the workplace, carrying out their tasks discreetly and without seeking recognition or prominence. They value work-life balance, seeking to maintain a clear separation between their professional responsibilities and their personal needs, avoiding overloading themselves with excess work, according to studies by Man et al. (2020) e Raval (2021).

'Wage Criminals' are workers who operate below the safety zone, violating workplace rules and regulations and failing to meet minimum expected performance standards. They represent a challenge, acting in a harmful and dishonest way, as they tend to perform only the minimum necessary to fulfill their responsibilities at work, showing disinterest, lack of motivation towards their tasks and the work environment in general, resulting in low engagement and productivity. They avoid taking on complementary responsibilities and extra tasks that may require additional effort, as they tend to do as little as possible to avoid work overload. This approach can lead to conflicts with coworkers or superiors, especially as it has a negative impact on the work environment and the team's productivity. Although they try to go unnoticed, these workers run the

risk of being identified as uncommitted and ineffective in their roles, which can result in dismissal, corroborating the studies by Fan and Sheng (2023) e Järlström et al. (2020).

The ""safety zone"" can be seen as a state of equilibrium for workers, where they try to avoid situations of stress or emotional exhaustion, opting for a more passive approach to work. However, by remaining in this zone, workers may display disengagement behaviors, such as quiet quitting, in which they disconnect emotionally and reduce their productivity. Therefore, understanding the dynamics of the ""safety zone"" is essential for organizations wishing to promote a healthy work environment and encourage workers to leave this comfort zone and become more meaningfully involved in their activities, promoting a more productive and satisfying work environment for everyone involved. It is therefore important to understand what organizational strategies should be adopted with workers to prevent the practice of quiet quitting."
The_Limited_Role_of_-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/The_Limited_Role_of_-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/The_Limited_Role_of_-with-image-refs_artifacts/image_000001_9a95880ca9def823f29d381e103af1cd16f95f716fdbfc00f23aab00d30b5d2b.png,"## Cheating

In all rounds, participants reported, on average, more than the six expected correct predictions in the AFTER version of the task (see Figure 2), showing that some of them cheated. It is not possible to determine precisely the proportion of participants who cheated due to variations in the degree of cheating. However, the difference between the expected and observed proportion of participants reporting a given number of correct predictions allows for a conservative estimate of 36.5% of participants who cheated when they first played the AFTER version of the task. The true proportion is likely higher, given that this estimate assumes a minimum number of cheaters and maximum possible cheating consistent with the observed data. In reality, it is likely that a higher number of participants cheated somewhat less. A similar conservative estimate for participants who played the AFTER version of the task is appreciably higher for later rounds, where participants had to pay to play the AFTER version of the task, and it ranged from 69.6% in the third round to the maximum of 85.8% in the fifth round.

1 Even though we pre-registered that the personality measures will be standardized, we opted to keep the natural scale at the end. Given that standardization and centering differ only in scaling, this difference in analysis can influence only the reported regression coefficients, but not their significance.

Figure 2. The distribution of correct predictions per round . The figure shows the distribution of the number of reported correct predictions in the baseline measure of cheating separately for the two loss conditions and for participants who played the cheating-enabling (AFTER) version of the task in later rounds after payment. Observed means and their 95% confidence intervals are also displayed. The crosses show the binomial distribution expected if all participants reported their predictions honestly."
VISTA__Verifiable_In-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/VISTA__Verifiable_In-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/business_articles/processed_articles/VISTA__Verifiable_In-with-image-refs_artifacts/image_000000_00f9d2b044adfd2119124914e6bac76ab63804a1d016cf1e7a25d75b64e72564.png,"## 4. Agent Lifecycle Management in VISTA

VISTA formalizes agent behavior as a lifecycle embedded within an information architecture. This lifecycle is not just a technical protocol-it is a structural commitment to reflexivity, traceability, and adaptive governance. Inspired by biological and legal constructs, the lifecycle enforces checkpoints that ensure agent actions remain verifiable and aligned with institutional norms:

- Genesis - The agent is instantiated and issued a decentralized identifier (DID) and verifiable credentials (VCs), binding its existence to a cryptographically accountable identity.
- Active - The agent performs tasks within a trusted execution environment, maintaining real-time logs and enforcing logic constraints informed by organizational policy.
- Escalation - When decisions deviate from policy or present ambiguity, control is automatically routed to human or institutional oversight-a critical design for reflexivity in IA contexts.
- Sunset - Upon task completion or breach, the agent is revoked or decommissioned. Sunset records are preserved as part of the permanent audit trail, enabling institutional memory.

This lifecycle acts as a temporal skeleton within the broader information architecture-embedding feedback, escalation, and ethical alignment into the substrate of agent behavior.

Figure 1 illustrates this process: a user request is routed through the agent's lifecycle checkpoints

Figure 1 : VISTA Agent Lifecycle and Request Flow.

This diagram illustrates the processing of a user request across the modular VISTA layers. Each stage reflects a checkpoint in the agent's lifecycle-beginning with identity verification (Genesis), through secure execution and policy compliance (Active), potential escalation to human oversight (Escalation), and ultimately resolution or shutdown (Sunset). VISTA ensures cryptographic traceability and ethical accountability at every stage."
A_MODULAR_SOFTWARE_F-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/A_MODULAR_SOFTWARE_F-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/A_MODULAR_SOFTWARE_F-with-image-refs_artifacts/image_000008_c7e3824421c6b0de1bf5cc907773351f9bebd499b208f52b8b68b7dbac03f588.png,"## 5.1 Single-Agent Experiments

This section presents the results obtained in the single-agent experimental settings, as introduced in section 3.3. Each Self-Modeling Agent configuration combines one policy module (BasePolicy, HybridPolicy, RLPolicy, or AdvancedPolicy) with one SelfModel variant (Simple or Advanced). The primary goal is to evaluate how different combinations influence agent behavior, meta-cognitive dynamics, and adaptive performance.

The evolution of key internal variables-confidence, fatigue, and agent mode-over time for each configuration is analyzed below. The corresponding scientific metrics extracted from the experiment logs are summarized in Table 2.

Dummy Policy + Simple/Advanced SelfModel DummyPolicy configurations serve as a baseline, with no learningdriven policy adaptation. The confidence trajectories remain relatively flat, while fatigue gradually increases. The AdvancedSelfModel introduces additional predictive monitoring (predicted confidence and mode), which yields slightly more stable confidence profiles and marginally improved switching dynamics. However, in the absence of active policy learning, the impact of self-modeling remains limited (see Figure 2 and Figure 3).

Hybrid Policy + Simple/Advanced SelfModel HybridPolicy configurations (Section 3.3) balance PPO-driven policy actions with stochastic exploration. This policy enables more dynamic confidence profiles and stronger fatigue modulation. The AdvancedSelfModel further enhances stability and predictive self-consistency, resulting in more adaptive mode switching between exploration and exploitation phases (see Figure 4 and Figure 5).

RL Policy + Simple/Advanced SelfModel Configurations with RLPolicy (Section 3.3) demonstrate the most sophisticated adaptive behavior. RLPolicy agents exhibit confidence trajectories tightly coupled to task performance, with pronounced and interpretable fatigue cycles. AdvancedSelfModel agents maintain more consistent internal predictions and leverage meta-cognitive feedback to modulate exploration-exploitation balance effectively (see Figure 6 and Figure 7).

Advanced Policy + Simple/Advanced SelfModel The AdvancedPolicy configurations (Section 3.3) represent the most integrated meta-cognitive design, dynamically modulating exploration and exploitation based on the agent's internal state. AdvancedPolicy + SimpleSelfModel already yields adaptive behavior with clear confidence cycles and interpretable fatigue dynamics. AdvancedPolicy + AdvancedSelfModel achieves the highest level of dynamic self-regulation, with optimized switching between exploration and exploitation and a well-balanced fatigue profile (see Figure 8 and Figure 9).

Quantitative Summary Table 2 summarizes the core scientific metrics across all single-agent configurations. Notably, AdvancedSelfModel consistently yields lower switching rates (indicating more stable behavioral modes), higher average confidence, and more adaptive fatigue regulation compared to SimpleSelfModel. AdvancedPolicy + AdvancedSelfModel emerges as the most balanced and self-aware configuration.

Table 2: Scientific Metrics Summary - Single-Agent Configurations

| Configuration       |   Switching Rate |   Avg. Confidence |   Avg. Fatigue | Time in Exploitation   | Time in Exploration   |
|---------------------|------------------|-------------------|----------------|------------------------|-----------------------|
| Dummy + Simple      |           0.9854 |            0.9962 |         0.7712 | 50.1%                  | 49.9%                 |
| Dummy + Advanced    |           0.914  |            0.9893 |         0.7511 | 53.3%                  | 46.7%                 |
| Hybrid + Simple     |           0.9948 |            0.9965 |         0.6865 | 52.1%                  | 47.9%                 |
| Hybrid + Advanced   |           0.9045 |            0.9905 |         0.6678 | 55.0%                  | 45.0%                 |
| RL + Simple         |           0.9999 |            0.9998 |         0.5927 | 50.9%                  | 49.1%                 |
| RL + Advanced       |           0.8806 |            0.9827 |         0.5721 | 55.9%                  | 44.1%                 |
| Advanced + Simple   |           0.8992 |            0.9881 |         0.565  | 55.7%                  | 44.3%                 |
| Advanced + Advanced |           0.8503 |            0.9805 |         0.5618 | 56.4%                  | 43.6%                 |

Discussion These results demonstrate the benefits of integrating meta-cognitive modeling within the agent architecture:

- AdvancedSelfModel consistently improves stability and internal state coherence, enabling more deliberate behavioral modulation.
- RLPolicy and AdvancedPolicy agents capitalize most effectively on self-model feedback, achieving dynamic yet controlled exploration-exploitation cycles.
- HybridPolicy configurations offer a robust compromise, benefiting from PPO guidance while retaining exploratory flexibility modulated by meta-cognitive signals.
- DummyPolicy agents, as expected, exhibit limited adaptive capacity, underscoring the value of learned policies coupled with self-modeling.

Overall, these findings validate that explicit self-modeling enriches agent behavior in single-agent settings, paving the way for more advanced meta-cognitive capabilities in multi-agent and lifelong learning scenarios, as explored in subsequent sections, even in relatively simple Gridworld environments, highlighting the generality of the approach.

Step

Step

Step

Step"
Adversarial-Resistan-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/Adversarial-Resistan-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/Adversarial-Resistan-with-image-refs_artifacts/image_000000_56d9cad6625947993039654d324cc3023d3cc284d88120fed50b29c3da75abd0.png,"## 3.1 Dual-LLM Architecture Overview

Our proposed system employs a dual-LLM framework consisting of a primary reasoning component and a specialized response validation module that operate in architectural isolation to prevent manipulation inheritance  [3][5] . The primary component utilizes DeepSeek Coder 7B or similar coding-CS specialized LLM, fine-tuned on curated CS50 educational materials and integrated with retrieval-augmented generation (RAG) using course-specific vector embeddings  [2] . The validation module employs another 7B or smaller parameter (let's suppose 1B) model trained specifically on adversarial examples to perform binary classification of response appropriateness, operating independently from the primary model's reasoning context  [4][5] .

The architectural separation principle ensures that emotional manipulation or adversarial prompts affecting the primary model's reasoning process cannot influence the validation component's assessment of pedagogical appropriateness  [4][5] . This design addresses fundamental vulnerabilities identified in single-LLM approaches where adversarial context can compromise both content generation and safety evaluation simultaneously  [4] .

Ensure conditions

Figure 1: Dual-LLM Architecture Overview showing architectural separation between reasoning (1st LLM) and validation (2nd LLM) components, with behavioral design elements including prompt processing and response trimming mechanisms"
Assessing_Data_Imbal-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/Assessing_Data_Imbal-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/Assessing_Data_Imbal-with-image-refs_artifacts/image_000014_9d576863b223cc632b0b9d3d396a3a01349d3e3f43a59c7911ead4a368dc4d61.png,"## 4.2 Model interpretation

Given that parameter interpretation of the penalised regression models was not possible due to the regularisation process, the standard logistic regression models were taken forward for interpretation. The model fitted with uncorrected data provided improved performance and thus was used for model interpretation. Ht was predicted to significantly increase collision probability ( 𝛽𝐻𝑡 = 0.811, [95 % CI: 0.079, 1.542], z = 2.173, p = 0.029) (see Table 2). For average levels of Hs and pupil diameter, a one standard deviation increase in Ht (~4 percentage points of normalised Ht ) resulted in an increase in the probability of a collision by 7 percentage points. Exponentiation of the standardised coefficient ( 𝑒 𝛽𝐻 𝑡 = 2.250) revealed that this was equivalent to a medium effect size, as defined by Rosenthal (1995).  This result implies that if the spatial distribution of gaze is at an average level and drivers have average levels of MWL (as indexed by mean pupil diameter), then an increase in the randomness of gaze transitions is predicted to increase the probability of a collision during critical transitions of control (see Figure 9).

Table 2: Model parameter estimates from uncorrected data GLM

| Predictors      |   Estimate |    SE |   z-value | p-value   |
|-----------------|------------|-------|-----------|-----------|
| 𝜷 𝟎             |     -2.664 | 0.339 |    -7.851 | <0.001    |
| 𝜷 𝑯 𝒕           |      0.811 | 0.373 |     2.173 | 0.029     |
| 𝛽 𝐻 𝑠           |     -0.064 | 0.267 |    -0.241 | 0.809     |
| 𝛽 𝐷 𝑚           |      0.05  | 0.345 |     0.146 | 0.883     |
| 𝜷 𝑯 𝒕 :𝑯 𝒔      |      0.517 | 0.243 |     2.123 | 0.033     |
| 𝛽 𝐻 𝑡 :𝐷 𝑚      |      0.231 | 0.356 |     0.65  | 0.515     |
| 𝛽 𝐻 𝑠 :𝐷 𝑚      |     -0.063 | 0.255 |    -0.249 | 0.802     |
| 𝛽 𝐻 𝑡 :𝐻 𝑠 :𝐷 𝑚 |      0.177 | 0.165 |     1.072 | 0.283     |

Figure 9: Relationship between standardised Ht and the probability of a collision for average levels of Hs and pupil diameter. As standardised Ht increases (i.e., as transitions of gaze become more random) the probability of a collision occurring during a critical takeover increase.

The model also highlighted a significant interaction between Ht and Hs ( 𝛽𝐻𝑡:𝐻𝑠 =  0.517, [95 % CI: 0.039, 0.994], z = 2.123, p = 0.033). For average pupil diameter levels, increasing Hs amplified the effect of Ht on increasing collision probability by a further 4 percentage points. Exponentiation of the standardised coefficient ( 𝑒 𝛽 𝐻 𝑡 :𝐻 𝑠 = 1.677) revealed that this was also equivalent to a medium effect size, albeit smaller than the effect of Ht individually. This result implies that when the spatial distribution of gaze is higher, and the fixations are highly random, there is a higher probability of a collision (see Figure 10).

Figure 10: Relationship between standardised Ht , Hs , and the probability of a collision for average levels of pupil diameter. When standardised Hs is lower than average (i.e., the spatial distribution of gaze is constrained), increasing standardised Ht appears to have minimal effects on the probability of a collision. However, when standardised Hs is over average (i.e., the spatial distribution of gaze is dispersed), an increase in the randomness of gaze transitions results in higher probability of a collision during a critical takeover."
AutoML__A_Tertiary_S-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/AutoML__A_Tertiary_S-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/AutoML__A_Tertiary_S-with-image-refs_artifacts/image_000000_ae26ea8dd64ded7a6250ce6fba5f2bdaf720e39d43b1ab35d66a11ca9d53e4b5.png,"## 3 Study Design

We conducted a tertiary review [21] to investigate the current state of AutoML research, synthesizing evidence from existing systematic and multivocal literature reviews, which are treated here as primary sources. To efficiently explore and identify appropriate methods and tools for practitioner use cases, we adopted a rapid review methodology [16], involving a domain practitioner (second author) to ensure relevance and accuracy. The complete research process, along with the number of papers retained at each stage, is illustrated in Fig. 1.

Fig. 1: Overview of the research process"
Autonomous_identity--with-image-refs,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/Autonomous_identity--with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/Autonomous_identity--with-image-refs_artifacts/image_000003_9b1a95eac2345792bf0b7cfd791ff913dcf54d60e393c7e4351ca753cf25b112.png,"## Zero-Trust Architecture

Fig. 1. Overview of the trust-based access control model.

- f i (x) : Normalized function representing the observed metric for factor iii .
- n: Total number of factors considered.

Thresholds ( T ) can then be set to trigger responses:

If R &gt; T , then isolate or restrict identity."
Bias_and_Fairness_in-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/Bias_and_Fairness_in-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/Bias_and_Fairness_in-with-image-refs_artifacts/image_000000_cf1a945d28634505596de274c2d81507c9fb08465b98eb5d486d2bab03064018.png,"## 2.2 Bias and Fairness in Med LLMs

While following responsible principles of development and deployment is important in any application of LLMs, Med LLMs must especially adhere to ethical constraints, including patient consent, privacy, compliance with regulations (such as HIPAA in the US and GDPR in Europe [35, 120]), as well as unbiased and fair treatment of individuals. Fig. 1 illustrates the various ethical aspects that should be addressed in Med LLMs.

Bias and fairness are key considerations in the deployment of Med LLMs, as these systems increasingly influence clinical decision-making and patient care [41, 50, 127]. Bias in the context of LLM refers to disparities in outcomes or representations, sourced from data and algorithms [45]. Fairness is related to parity in outcomes or opportunities

Fig. 1. Overview of Med LLM characteristics and ethical deployment considerations.

with respect to patient attributes, such as socioeconomic status and comorbidities [132]. This way, fairness is often considered as the absence of imparity between groups or individuals [30].

Med LLMs operate in safety-critical environments where incorrect or biased outputs can disproportionately impact underrepresented populations, exacerbating healthcare disparities [8, 104]. To tackle such challenges, Med LLMs require robust bias mitigation techniques, interdisciplinary collaboration with clinicians, explainability [56], and comprehensive validation frameworks to ensure their trustworthiness and alignment with evidence-based medical knowledge and guidelines. Accordingly, a dedicated review of studies related to this domain seems essential."
Bioenergy_with_carbo-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/Bioenergy_with_carbo-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/Bioenergy_with_carbo-with-image-refs_artifacts/image_000002_64f61a1d6bc4bfc9d63ca2e4436f796a2fb242d6e20bcf6777d7f0ca15c3f6eb.png,"### 3.1. Emission reduction measures

The resulting trajectories of emission reduction for each decarbonization scenario (S1-S4) are shown in **Figure 4** . By 2100, S1 resulted in 572 Mt CO2 reduction while S2, S3, and S4 lead to 613, 719, and 830 Mt CO2 reduction from the baseline scenario (BL) of 683 Mt CO2. Across S1-S4, most of emission reduction are generated from increasing renewable energy (RE). About 536-548 Mt of CO2 emissions can be reduced in 2100 by expanding renewable power capacity. Meanwhile, emission reduction from BECCS varies across different decarbonization scenarios. BECCS contribution is not significantly required to achieve less stringent emission targets (S1 and S2). There is no BECCS required under S1 and insignificant amount of BECCS is required under S2 (5 Mt CO2, 0.8% of total emission reduction in 2100). However, BECCS contribution increased significantly as more stringent targets are considered (S3 and S4). Under S3 and S4, BECCS contribution increased to 105 and 265 Mt CO2 (15% and 32% total emission reduction in 2100). Emission reduction from BECCS comprised of both avoided fossil emission (BECCS-ZERO) and carbon dioxide removal (BECCS-CDR). Besides transitioning to non-fossil, reduction of fossil power emission also came from shifting to lower emitting fuels, i.e., shifting from coal to natural gas input for power generation. In addition, emission reduction via nuclear power is only selected under S3 and S4—with S3 resulting in larger nuclear capacity than S4.

Figure 4 | Development of emission reduction contribution (in Mt CO2 y-1) by technologies from 2020 to 2100 in different decarbonization scenarios (S1-S4)."
Continuous_Real-Time-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/Continuous_Real-Time-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/Continuous_Real-Time-with-image-refs_artifacts/image_000006_b24a088c19a49172abf751b13873162cd246204008534a88f523c567c0db7484.png,"## 3.1 Gesture decoding

The implemented neural network architecture proved to perform well in decoding hand gestures. We visualized the decoding performance using a circular plot (Figure 6) which showed predicted direction in polar coordinates. In this plot, 12 colors represented 12 target locations and the dotted position represented the cursor position. It can be noticed that our network architecture decoded the direction well, and errors occurred when the decoder attributed the direction to a neighboring sector. For example, the decoder confused 0 and 150 degrees for participant RK07. Despite these errors, the measurements of MSE showed that the trajectories were decoded well. The MSE for the trained model was compared to the MSE after a random shuffle of the decoded trajectories, and the difference was 0 293 . ± 0 03 for 100,000 shuffles. .

Figure 6: The scatter plots showing how direction was decoded from the OMG data with the multilayer perceptron. Each dot corresponds to one sample of the OMG data. Polar coordinates represent the decoded cosine and sine values of the direction. Each dot's color corresponds to the instructed direction, and its distance from the center represents the decoding result. The box plot (bottom, right) quantifies the prediction of click gesture.

The box plots shown in Figure 7 depict the predictions of clicks. In the box plot pairs, the box shown on the right corresponds to the probability of click detection for the fist clench gesture, and the box shown on the left corresponds to the probability for any other gesture. It is clearly seen that the model distinguished fist clinch from the other gestures. To avoid false detection of the click, a click threshold value was set for each subject individually.

Figure 7: Accuracy of click decoding represented as box plots. The box on the right corresponds to the decoding of fist clench. The box on the left corresponds to false detection of click from the gestures different from fist clench."
Decoding_community_p-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/Decoding_community_p-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/Decoding_community_p-with-image-refs_artifacts/image_000005_4322d483f7304b335a3949f511b75c481106d8c625e9574153d98932cbd69b4e.png,"## 4.1.4 Named entity recognition

Using the GLiNER model, we identified named entities (Person, Location, Event, Organization, Issues) in both local and national news. Figure 4 illustrates the distribution of top named entities for each category. Local news focuses on local personalities and places, for example featuring names like Lucas Vuitel or cities such as Neuchâtel , La Chaux-de-Fonds , and the canton of Valais . National news highlights more widely recognized figures and broader locations, such as Donald Trump or Suisse , Genève , États-Unis .

538

539

540

541

542

543

544

545

546

547

548

549

550

(a) Local News - Person

(c) Local News - Location

(e) Local News - Organization

(g) Local News - Event

- (i) Local News - Issue

(j)

National News - Issue

(b) National News - Person

(d) National News - Location

(f) National News - Organization

(h) National News - Event

Fig 4. Comparison of Top NERs in local news vs national news across the 5 Categories.

The prominence of local personalities, places, and organizations in local news, contrasted with the broader and more nationally or internationally recognized entities in national news, underscores the distinct audience orientation of each media tier. Local news' focus on hyper-local entities reinforces its role in constructing and maintaining community identity, while national news' broader scope reflects its function as a mediator of national discourse. This finding supports the theoretical expectation that proximity is not just geographic, but also discursive and symbolic [59]."
Delivering_Tactile_S-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/Delivering_Tactile_S-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/Delivering_Tactile_S-with-image-refs_artifacts/image_000004_af1c5c4d0d0a8e7ba3b26f12779414b65cea2169a4d60b1b4284d58a9bd559fd.png,"## Results

Figure  5.  Onset  lag  and  asynchrony  for  trimodal  stimuli.  a) Onset  lag  of  visual, auditory and haptic stimuli where 0 ms represents the stimulus onset trigger (websocket signal). b) Asynchrony of auditory and vibration stimuli compared to the visual stimulus onset (set to 0 ms). Dashed lines represent means of the distributions, in subplot b the solid line represents perfect synchrony.

Table 2. Stimulus Onset (Lag). Time in ms of measures from photodiode (light), auditory input (sound) and microphone input (vibration) measured via the RTbox. Timings are relative to expected stimulus onset of 0ms. Measurements are based on 987 measured events, mean and standard deviation of onset lags are shown, percentages indicate raw data percentiles.

| Event     |   mean |   std dev |   min |   25% |   50% |   75% |    max |
|-----------|--------|-----------|-------|-------|-------|-------|--------|
| light     |  35.91 |      4.28 | 17.83 | 33.16 | 35.79 | 38.5  |  51.49 |
| sound     |  40.91 |      3.44 | 23.26 | 38.82 | 41.03 | 43.19 |  52.46 |
| vibration |  78.64 |      5.05 | 56.08 | 75.54 | 78.5  | 81.6  | 111.65 |

Table 3. Asynchrony. Time in ms of sound and vibration measurements relative to the visual stimulus. Measurements are based on 987 measured events. Percentages indicate raw data percentiles.

| Event     |   mean |   std dev |   min |   25% |   50% |   75% |   max |
|-----------|--------|-----------|-------|-------|-------|-------|-------|
| sound     |   5    |      3.24 | -8.4  |  2.91 |  5.33 |  7.3  | 14.46 |
| vibration |  42.73 |      5.41 | 24.87 | 39.13 | 42.85 | 46.02 | 82.3  |

Relative  to  the  websocket  trigger  signal,  the  visual  stimulus  was  presented  with  a  lag  of 35.34 ms and a precision of 7.35 ms (Fig 5a). Similarly, the auditory stimulus was presented with  a  lag  of  38.72  ms  and  a  precision  of  4.49  ms.  Tactile  stimuli  were  presented  after  a longer  lag  of  84.20  ms  at  a  precision  of  6.07  ms.  Of  note,  the  larger  lag  in  the  tactile stimulation  may  be  attributable  to  the  ramp  period  in  vibration,  and  the  microphone  not detecting  vibration  stimuli  until  a  certain  amplitude  was  surpassed.  Since  the  microphone used  to  detect  the  vibrations  was  also  recording  during  data  collection,  we  were  able  to inspect  the  waveform  of  the  recorded  vibrations  (Figure 6a). On average, an initial ramp period before the triggering of the RT Box in these waveforms lasted 30.46 ms (SD: 5.28 ms; Figure 6b). We conducted a simple linear regression to examine whether this ramp period predicts the asynchrony from the visual onset (Figure 6c). The overall model was significant, F(1, 978) = 135.3, p &lt; .001, and explained 12.2% of the variance in asynchrony (R² = .122). The slope for ramp time was positive and significant (b 1 = 0.342, SE = 0.029, t(978) = 11.63, p &lt; .001, 95% CI [0.284, 0.400]) and so was the intercept (b0 = 32.23, SE = 0.91, t(978) = 35.46,  p  &lt;  .001,  95%  CI  [30.45,  34.01]).  This  intercept  means  that  the  model  predicts  an asynchrony of 32.23 ms even when the ramp period is at 0 ms, meaning there still exists an approximate 30 ms asynchrony when accounting for ramping of the vibration stimulus. This ramping  is  an  inherent  feature  of  vibration  stimulus  presentation,  and  is  therefore  an important factor for researchers to consider when designing experiments involving vibration. For example, one strategy might be to cue the vibration slightly earlier to ensure that its peak amplitude aligns more precisely with a visual stimulus.

b)

c)

0.10

0.05

0.00

10

Mean = 30.51 ms

20

30

40

50

60

R2 = 0.12, p &lt; .001

20

30

40

50

Marker Lag (ms)

Marker Lag (ms)

Figure 6. Analysis  of  ramp  onset  for  vibration  stimuli. a) Example  waveform illustrating the recorded vibration stimulus (blue), with onsets identified by a convolutional neural network (red) and event markers recorded by the RT Box (black). The marker lag (grey shaded area) between these two onset measurements quantifies the marker's timing

3

2

60

50

40

30

accuracy.  The  envelope  of  the  waveform,  calculated  using  root  mean  square  (RMS, yellow),  filters  out  spurious low-amplitude onsets to improve onset detection reliability. b) Distribution of marker  lag  values,  indicating  an  average  lag  of  30.51  ms  from  the neural-network-defined  onset. c) Scatterplot  demonstrating  a  positive  correlation  (R²  = 0.12, p &lt; .001) between marker lag and visual-vibration asynchrony, showing that marker timing variability partly explains differences in multimodal synchrony.

When  conducting  studies  where  response  time  relative  to  a  stimulus  is  the  dependent variable.  A  lag  in  stimulus  onset  can  often  be  accounted  for  by  subtracting a constant lag value from response times (where response times are calculated relative to a trigger signal), a  low  precision  of  onsets  makes this  calculation  less  reliable.  Browser  precision  is  worse compared  to  lab-based  alternatives  where  visual  and  auditory  onset  precision  have previously  been  recorded  at  0.35  ms  and  0.96  ms,  respectively  (Bridges  et  al.,  2020). Nevertheless,  even  in  browsers  this  appears  to  be a relatively good degree of precision. Standard deviations under 10 ms for both lags and asynchronies create a relatively stable experimental environment, as long as the expected effect size is large enough to stand out from the noise in signal onset. To assess how onset variability affects statistical sensitivity in typical  RT  studies,  we  used  our  Monte Carlo framework to determine the effect size (Δμ) required for 80 % power. When no onset noise was added, a drift rate increment of Δμ = -0.0421 (≈ 6.6 ms in RT space) was sufficient to reach 80 % power. Introducing 10 ms of onset variability  raised  the  required  effect  size  only  marginally  to  Δμ  =  0.0424  (≈ 6.7 ms). These findings indicate that the natural trial -to trial -variability in decision times overwhelmingly dominates the impact of a 10 ms uncertainty in stimulus onset.

Additionally, we can interpret the timing as the asynchrony between the stimuli. In this case we can ignore the websocket trigger and set our latencies relative to the onset of the visual stimulus  (captured  using  a  photodiode).  Previous  browser  based  audiovisual  synchrony recorded on a Windows 10 device using Chrome and PsychoPy was reported as 65.32 ms with  a  precision  of  3.01  ms  (Bridges  et  al.,  2020).  That  is,  auditory  stimuli  were  detected approximately 65 ms after the visual stimuli. Interestingly, the audiovisual synchrony on the mobile device was substantially better at 3.37 ms, but precision was worse at 6.49 ms (Fig 1b). The vibration stimulus lagged the visual onset by a larger 48.85 ms with a precision of 7.44 ms."
Implementing_Reliabi-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/Implementing_Reliabi-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/Implementing_Reliabi-with-image-refs_artifacts/image_000001_d5ec5773cf3ec85fd122d800e6addd2c80e8565e234d942d533b549b831ce84f.png,"## 2 Evolution of Maintenance Strategies

Since the mid-1930s, the evolution of maintenance can be described in terms of three generations of development, as illustrated chronologically in Figure 2.

Figure 2: Historical evolution of maintenance strategies."
Incorporating_Partic-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/Incorporating_Partic-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/Incorporating_Partic-with-image-refs_artifacts/image_000000_4d72b1ae76ef84fb8108c4726da438cc7fd0a5eec5697fc3d9a8fdb99e8ce3f9.png,"## Data collection

We performed semi-structured, qualitative interviews with the DMR participants to understand what outcomes they experienced before introducing them to any frameworks (Figure 2). The interviews consisted of three parts, with each subsequent part being less inductive and focusing more on the frameworks: (1) opening question and responses; (2) framework introduction and specifying outcomes; and (3) mapping outcomes on the framework. An interview guide was created to structure questions or reminders according to the interview schedule (Supplemental file 1: Interview Guide). This three-stage approach to the interviews helped to collect a variety of comments on the outcomes and frameworks from the interviewees, but was treated as one interview during analysis.

Figure 2 : Visual overview of the research methods, showing how quotes from the three parts of the interview were coded in two phases, resulting in the final codebook.

All 105 participants in the 2021 edition of the DMR project were invited by email to take part in these interviews. A few interviewees (n=2) replied via email but the majority (n=14) was recruited by calling a randomised list of the remaining participants. We stopped recruitment at a feasible number of interviewees given our resources. The first interview was held with a participant who was a professional in CS and water management at the TU Delft, which was used as a test interview. The interview duration ranged from 25 to 75 minutes, depending on the conciseness of interviewees, how many outcomes they identified, and their willingness to enter into a longer discussion of their experiences. All interviews were conducted by the first author in Dutch, except for one interview with a participant who could understand Dutch but was more comfortable in English.

After an introduction to cover data-ethics best practices and establish rapport, part 1 started with the open question: 'Could you tell me in what ways participating in DMR has been meaningful for you, in terms of the outcomes or results that you experienced from the project?'. Probing questions were asked to help interviewees elaborate on their answers, and reminders of key aspects of the project (such as the measuring task, or information received via newsletters and webinars) were only given by the interviewer after the first top-of-mind answers were noted.

For part 2 of the interviews, a physical two-sided board was created that displayed the frameworks used in this study (Supplemental file 2: Framework board). It was used to introduce the interviewees to the concept of evaluation frameworks by explaining the outcomes and impacts one by one. As soon as the interviewees were familiar with the frameworks, they were asked whether they experienced any of these outcomes. After any clarification and structuring of the discussed outcomes, the final list of outcomes was agreed upon before continuing to part 3.

In part 3, the interviewees were involved in the process of classification of the outcomes. The interviewees were asked to score the outcomes that they experienced within the frameworks presented in part 2, which provided an opportunity to also share their view about the frameworks themselves. We explored this method to map the alignment of the outcomes to the frameworks, but did not design more formal methods for quantitative analysis due to resource constraints. Quotes from all three parts of the interview were used in the analysis, but the scores of part 3 were discarded."
Interpretability_req-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/Interpretability_req-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/Interpretability_req-with-image-refs_artifacts/image_000000_6d98e21efcbfcb6784e049b55d15e9457519cea07be1b53360ccea15ab32cf0d.png,"## 2 Current state of theory and experiment

Without loss of generality, we discuss examples of interpretability work on input attribution and component attribution (Figure 1). For each, we cover complexity-theoretic and experimental efforts.

Figure 1: Interpretability research illustrated through two types of questions it attempts to answer: which input features and which components of the model explain various tasks of interest?"
JusticeNetBD__A_Retr-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/JusticeNetBD__A_Retr-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/JusticeNetBD__A_Retr-with-image-refs_artifacts/image_000004_eec45bb42b8644b5e6e07b2d45ff823fb2f708f9d53def55b2e9e1fdfb9ab3f8.png,"## Model Performance Comparison

Figure 5: Model Performance Comparison

General-purpose state-of-the-art large language models such as DeepSeek V3, Gemini,2.5 Flash, and ChatGPT-4o underperform in this domain due to their lack of access to domain-specific legal statutes relevant to Bangladesh, as shown in Figure 5. As a result, these models tend to produce generic legal advice or reference incorrect statutory sections, which adversely affects lexical overlap metrics (e.g., ROUGE-L) and, to a lesser extent, semantic similarity measures such as BERTScore. Additionally, their responses often include hedging phrases (e.g., 'laws may vary...') rather than directly citing precise punitive clauses.

Incorporating exact statutory text segments into the prompt allows the RAG model to:

- Mitigate hallucination : ensuring that every generated claim can be traced to retrieved source text.
- Enhance lexical fidelity : resulting in improved ROUGE-L scores due to accurate legal phrasing.
- Preserve linguistic fluency : maintaining BERTScore values comparable to human-generated responses.

However, despite strong retrieval performance, approximately 10% of queries fail to retrieve the gold-standard statutory chunk. Future work could explore approaches such as query rewriting or hybrid sparse-dense retrieval methods to further improve recall towards 1.0. In the context of providing high-stakes legal advice, the grounded RAG system substantially outperforms closed-book state-of-the-art LLMs. The pipeline not only enhances factual accuracy but also offers transparent evidence retrieval, thereby providing a safer and more reliable tool for supporting women's rights initiatives in Bangladesh."
Learning_to_operate_-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/Learning_to_operate_-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/Learning_to_operate_-with-image-refs_artifacts/image_000003_72900c0e39b599e56f813b7bf70856e716cd54602a728de3e78304f3a55a8229.png,"## Statistical tests

To  statistically  validate  our  results,  we  analyzed  data  from  healthy  participants  and  the amputee separately. For each healthy subject, we compared measurements from stimulation and non-stimulation trials. Normality of each dataset was assessed using the Shapiro-Wilk test. If normality was  confirmed, we  applied an independent  t-test; otherwise, the non-parametric  Mann-Whitney  test  was  used.  False discovery rate (FDR) correction was applied within each subject, separately for behavioral and visuomotor measurements.

A  similar  procedure  was  followed  for  the  amputee.  Measurements  from  stimulation  and non-stimulation  trials  were  compared across three experimental sessions. FDR correction was  applied  across  all  comparisons  from  the  three  days.  Comparisons  with  a  corrected p -value below 0.1 were considered statistically significant.

Figure  3.  Different  phases  of  the  recordings.  Red  line represents normalized hall-effect sensor data, while the green line shows its gradient. When the wrist is opened, the sensor's data is equal to 1, if closed - then equal to 0. Threshold equal to ± 0.025 for signal velocity allowed to acquire best accuracy for correct trial separation into phases. It allows us to get information on the position of positive and negative peaks of the signal"
Machine_Learning-Bas-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/Machine_Learning-Bas-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/engineering_articles/processed_articles/Machine_Learning-Bas-with-image-refs_artifacts/image_000000_a0cb01ae387bc9e5bae1be9606fddc51090d254d813ff395050ca959da8bd69a.png,"## 4.2 Random Forest

- Accuracy: 98.3%
- Precision: 99.1%
- Recall: 97.6%
- ROC-AUC: 0.995

Figure 1: Top features by importance from the Random Forest classifier. Variance, entropy, and energy were among the strongest predictors."
Beyond_the_Exponenti-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/life_articles/processed_articles/Beyond_the_Exponenti-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/life_articles/processed_articles/Beyond_the_Exponenti-with-image-refs_artifacts/image_000003_db7b6f050cd07cbb9fa07a51821340c2a200c115cc680ee838c9e4986e8b2911.png,"## 5 General Discussion

These results suggest that the rational function family (hyperbolic and its generalizations) is not only descriptively valid but also normatively grounded in physical principles. The brain, as an energy-constrained prediction machine, naturally discounts outcomes that are temporally or informationally distant. This bridges psychological, economic, and thermodynamic models of behavior, offering a powerful interdisciplinary synthesis (Callender, 2021; Friston, 2010).

Thermodynamic Metaphor for Delay Discounting

Figure 3: Thermodynamic metaphor for delay discounting: entropy cost accumulates over time while value decays.

General Forms of Discounting

Figure 4: General discounting curves across domains: time, probability, effort, and cognitive cost."
Oral_Language_Outcom-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/life_articles/processed_articles/Oral_Language_Outcom-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/life_articles/processed_articles/Oral_Language_Outcom-with-image-refs_artifacts/image_000000_9d456dd074f3e6056dcaed5efa793175e329021217b820cdc124457761fa03b9.png,"**Language Macrostructure**

Table 3 and Figure 1 show the average language macrostructure outcomes for the retelling and personal narratives during baseline, intervention, and maintenance phases in English and Spanish interventions. Additionally, we provide a detailed summary table of the retelling result indicators in Appendix B.

| Table 3.                                             | Table 3.                                             | Table 3.                                             | Table 3.                                             |
|------------------------------------------------------|------------------------------------------------------|------------------------------------------------------|------------------------------------------------------|
| Average macrostructure outcomes for all participants | Average macrostructure outcomes for all participants | Average macrostructure outcomes for all participants | Average macrostructure outcomes for all participants |
| Variables                                            | Baseline                                             | Intervention                                         | Maintenance                                          |
| Participant 1                                        |                                                      |                                                      |                                                      |
| English                                              |                                                      |                                                      |                                                      |
| Total retelling                                      | 5.5 (3)                                              | 13.83 (0.40)                                         | 13.5 (2.12)                                          |
| Story grammar                                        | 4.25 (0.50)                                          | 8.50 (0.83)                                          | 8 (0)                                                |
| Language complexity                                  | 0 (0)                                                | 0.33 (0.51)                                          | 1.5 (0.70)                                           |
| Episode                                              | 1.25 (2.50)                                          | 5 (0)                                                | 4 (1.41)                                             |
| Total personal story                                 | 0.75 (0.95)                                          | NA                                                   | 3 (2.12)                                             |
| Story grammar                                        | 0.75 (0.95)                                          | NA                                                   | 3 (2.12)                                             |
| Language complexity                                  | 0 (0)                                                | NA                                                   | 0 (0)                                                |
| Episode                                              | 0 (0)                                                | NA                                                   | 0 (0)                                                |
| Spanish                                              |                                                      |                                                      |                                                      |
| Total retelling                                      | 14 (0.81)                                            | 14 (2.75)                                            | 15.5 (0.70)                                          |
| Story grammar                                        | 8.75 (0.95)                                          | 7.16 (1.72)                                          | 8.5 (0.70)                                           |
| Language complexity                                  | 0.25 (0.50)                                          | 1.83 (1.69)                                          | 2 (0)                                                |
| Episode                                              | 5 (0)                                                | 5 (0)                                                | 5 (0)                                                |
| Total personal story                                 | 7.25 (2.97)                                          | NA                                                   | 11 (7.77)                                            |
| Story grammar                                        | 4.25 (1.25)                                          | NA                                                   | 6 (4.24)                                             |
| Language complexity                                  | 0.25 (0.50)                                          | NA                                                   | 0 (0)                                                |
| Episode                                              | 2.75 (1.06)                                          | NA                                                   | 5 (3.53)                                             |
| Participant 2                                        |                                                      |                                                      |                                                      |
| English                                              |                                                      |                                                      |                                                      |
| Total retelling                                      | 12.5 (1.73)                                          | 12.16 (2.32)                                         | 10 (2.82)                                            |
| Story grammar                                        | 8 (0.81)                                             | 7 (1.54)                                             | 6 (1.41)                                             |
| Language complexity                                  | 0 (0)                                                | 0.50 (0.54)                                          | 0 (0)                                                |
| Episode                                              | 4.5 (1)                                              | 4.66 (0.81)                                          | 4 (1.41)                                             |
| Total personal story                                 | 1.75 (3.5)                                           | NA                                                   | 1.5 (2.12)                                           |
| Story grammar                                        | 1 (2)                                                | NA                                                   | 1.5 (2.12)                                           |
| Language complexity                                  | 0 (0)                                                | NA                                                   | 0 (0)                                                |
| Episode                                              | 0.75 (1.5)                                           | NA                                                   | 0 (0)                                                |
| Spanish                                              |                                                      |                                                      |                                                      |
| Total retelling                                      | 5 (3.53)                                             | 9.83 (1.32)                                          | 7.5 (0.70)                                           |
| Story grammar                                        | 5 (1.41)                                             | 5.16 (0.98)                                          | 4.5 (0.70)                                           |
| Language complexity                                  | 0 (0)                                                | 0 (0)                                                | 0 (0)                                                |
| Episode                                              | 1.5 (2.12)                                           | 4.66 (0.81)                                          | 3 (0)                                                |
| Total personal story                                 | 0 (0)                                                | NA                                                   | 0 (0)                                                |
| Story grammar                                        | 0 (0)                                                | NA                                                   | 0 (0)                                                |
| Language complexity                                  | 0 (0)                                                | NA                                                   | 0 (0)                                                |
| Episode                                              | 0 (0)                                                | NA                                                   | 0 (0)                                                |
| Participant 3                                        |                                                      |                                                      |                                                      |
| English                                              |                                                      |                                                      |                                                      |
| Total retelling                                      | 20.5 (0.57)                                          | 22.16 (1.83)                                         | 22.5 (0.60)                                          |
| Story grammar                                        | 14.25 (1.25)                                         | 15.33 (2.06)                                         | 15.5 (0.70)                                          |
| Language complexity                                  | 1.25 (0.95)                                          | 1.83 (0.75)                                          | 2 (1.41)                                             |
| Episode                                              | 5 (0)                                                | 5 (0)                                                | 5 (0)                                                |
| Total personal story                                 | 8.75 (9.42)                                          | NA                                                   | 12 (0)                                               |
| Story grammar                                        | 6.25 (7.08)                                          | NA                                                   | 6.5 (0.70)                                           |
| Language complexity                                  | 0.50 (0.57)                                          | NA                                                   | 0.50 (0.70)                                          |
| Episode                                              | 2 (2.44)                                             | NA                                                   | 5 (0)                                                |
| Spanish                                              |                                                      |                                                      |                                                      |
| Total retelling                                      | 12.25 (1.50)                                         | 15 (0.89)                                            | 11 (1.41)                                            |
| Story grammar                                        | 8.5 (1.73)                                           | 9.66 (1.03)                                          | 9 (1.41)                                             |
| Language complexity                                  | 0.25 (0.50)                                          | 1 (0.63)                                             | 0.50 (0.70)                                          |
| Episode                                              | 3.5 (1)                                              | 4.33 (1.03)                                          | 1.5 (2.12)                                           |
| Total personal story                                 | 3.25 (1.70)                                          | NA                                                   | 8 (5.65)                                             |
| Story grammar                                        | 2.25 (1.50)                                          | NA                                                   | 4 (2.82)                                             |
| Language complexity                                  | 1 (1.41)                                             | NA                                                   | 1 (0.70)                                             |
| Episode                                              | 0 (0)                                                | NA                                                   | 3 (1.12)                                             |

*Note.* For Participants 1 and 2, total scores could be a maximum of 25 points; story grammar was a maximum of 10 points, language complexity 10 points, and episode 5 points. For Participant 3, total scores could be a maximum of 32 points; story grammar was a maximum of 17 points, language complexity 10 points, and episode 5 points.

Participant 1's baseline retelling scores in English showed some variability ranging from 4 to 10. Spanish baseline scores were higher than English and stable, ranging from 13 to 15. This suggests that at the beginning of the study, Participant 1 had better narrative skills in Spanish than in English. The visual analysis revealed a clear effect of the intervention with no overlap between baseline and intervention scores in English. The intervention line was flat and stable above baseline from session one. In Spanish, we observed an upward trend in the intervention in session four, but scores dropped to baseline levels in session six. Only two out of six intervention points were above 14, the highest baseline point. The change from baseline to intervention was 8.33 points in English and 0 in Spanish. These results were confirmed by the effect size calculations where we observed a Tau-U of .95 ( *p* = .01) for English and .04 ( *p* = .91) for Spanish, indicating that the intervention was more effective for English than Spanish narrative skills. The positive effect of the intervention in English scores was maintained a week and a month after the intervention. We observed increases in personal stories where Participant 1 increased 2.25 points in English and 3.75 points in Spanish from baseline to maintenance, indicating a possible generalization of story grammar elements from the intervention.

Progresses in Participant 1’s narrative abilities were primarily related to the story grammar and episode measures in both language interventions. For story grammar, Participant 1 consistently used the specific name of the character, the problem, the action, and a clear ending. However, Participant 1 frequently used general terms such as “bad” or “happy” instead of more precise emotional descriptors for the feeling. Participant 1 consistently employed a complete narrative structure for the episode (problem, action, and ending), indicating good planning and sequencing skills. However, challenges were observed in language complexity. For example, in English, Participant 1 used only basic temporal connectors such as “then” and “before” during the intervention.

Participant 2’s baseline scores in English were stable between 10 to 14 points. Spanish baseline scores were lower than English, at 4 and 9 points, although only two data points were available. The visual analysis showed variability in English intervention scores, with only two intervention points above the highest baseline point. In Spanish, we observed an intervention line initially higher than baseline but with a downward trend. Out of the six intervention points, four were above 9, the highest baseline point. The change from baseline to intervention was -.34 points in English and 4.83 in Spanish. The effect size calculations showed that the intervention was neither effective for English (Tau-U = -.04, *p* = .91) nor Spanish (Tau-U = .83, *p* = .09). Participant 2 exhibited difficulty using connectors and constructing basic (problem, ending) or complete (problem, action, and ending) narrative episodes. No intervention effect was observed for personal narratives in either language (baseline to maintenance).

Participant 3 showed higher retelling scores during baseline in English (20 and 21 points) than in Spanish (11 to 14 points). Baseline was stable for both English and Spanish, although an upward trend was observed in the last point of the Spanish baseline. The visual analysis revealed a slight intervention effect for English and Spanish, with four out of the six intervention points above baseline in both language interventions. The intervention line showed some variability and an upward trend in sessions four to six in English, and it was flat in Spanish. The change from baseline to intervention was 1.16 points in English and 2.75 in Spanish. The effect size calculations showed that the intervention was effective for increasing Spanish retelling scores (Tau-U = .79, *p* = .04), but it did not reach significance for English (Tau-U = .66, *p* = .08). However, this increase in Spanish scores was not maintained a week and a month after the intervention. We observed increases in personal stories. Participant 3 increased 3.25 points in English and 4.75 points in Spanish from baseline to maintenance, indicating a possible generalization of story grammar elements from the intervention.

Participant 3's improvements in retelling and personal narratives were influenced by the addition of more story elements, such as a specific ending and feelings. Additionally, Participant 3 used more connectors while retelling the story in English compared to Spanish, such as causal (""then"" and ""because""), consequence (""because""), and temporal (""when"") connectors. However, Participant 3 showed some difficulty when expressing the ""plan"" and ""specific consequence"" of the stories.

Figure 1.

*Total retelling scores for all participants*

*Note.* For Participants 1 and 2, total scores could be a maximum of 25 points; story grammar was a maximum of 10 points, language complexity 10 points, and episode 5 points. For Participant 3, total scores could be a maximum of 32 points; story grammar was a maximum of 17 points, language complexity 10 points, and episode 5 points.

Overall, the intervention was effective for increasing retelling scores in Participants 1 and 3’s weakest language (English and Spanish, respectively), but no significant effect was observed in the other language. This effect seemed to generalize to personal narratives in English and Spanish for both Participants 1 and 3. No significant intervention effect was observed for Participant 2 in English or Spanish. All participants achieved higher scores in the retellings than in personal narratives in both English and Spanish."
The_Promise_of_Maste-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/life_articles/processed_articles/The_Promise_of_Maste-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/life_articles/processed_articles/The_Promise_of_Maste-with-image-refs_artifacts/image_000002_a15a2a99eb3134bdd94c4ca37e0e43b09731c759fcb7823973c81eea25f3b6ce.png,"## 3.1 Struggling Students Were More Likely to Repeat Tests

The relationship between (a) the first-attempt scores for all students on all mastery tests, and (b) whether students chose to repeat each test is displayed in Figure 3. As predicted, students who struggled on their first attempt were much more likely to repeat the test: a one standarddeviation decrease in first attempt scores was associated with a more than 3x increase in the odds of a student repeating the test at least once, p &lt; .001. Students were likely to repeat unit tests at least once if they attained lower than an 80% on their first attempt (with 81% of students doing so), but only 21% of students who attained a score greater than 80% chose to retake that test. A horizontal line has been added to Figure 3 to emphasize this criterion that students appeared to impose on themselves.

Figure 3. First attempt scores vs. the proportion of students repeating each unit test.

3.2 Use Of The Testing System Was Associated With Students Returning To Practice Course Resources From Previous Units

Figure 4. Use of the mastery testing system vs. online chemistry problems completed in all units before (4A) and after (4B) the first corresponding mastery-based test. Plotted lines show predicted values from the regression models used to test the relationships. Error envelopes represent +/- 1 standard error.

On average, students chose to repeat approximately six tests over the duration of the course. Figure 4 shows the relationship between the number of repeated tests that each student took and the number of problems that they completed in relevant units of the online courseware, both before (4A) and after (4B) attempting each test for the first time. Although usage of the mastery testing system was unrelated to the number of problems that students completed before their first attempts,  (232) = 0.506, t p = .613, each additional repeated test was associated with a student circling back to a previous unit and completing an additional 95.4 practice problems after their first test attempt,  (232) = 8.75, t p &lt; .001, an approximately 10% increase in total studying per repeated attempt. This pattern of results is consistent with the mastery testing system encouraging students to return to course materials to prepare themselves for subsequent mastery attempts, increasing the total studying students completed.

3.3 Studying Between Mastery Attempts Was Associated With Improved Performance On Repeated Tests

Figure 5. Problems completed between repeated tests, compared with change in performance between attempts. Each point represents one unit test for each student. The plotted lines show predicted values from the linear mixed-effects models used to test the relationship for submissions one and two (5A) and submissions two and three (5B).

What happened when students decided to retake a unit test and revisit that unit's contents? Was the additional practice associated with learning? As Figure 5 shows, studying relevant content between test attempts was associated with improved performance; each additional 100 practice problems completed predicted a 5.1 point improvement in test scores between attempts one and two, F (1, 965) = 45.77, p = .001 (Figure 5A), and a 4.9 point improvement between attempts two and three, F (1, 321) = 5.01, p = .025 (5B). Notably, this analysis suggested that students were unlikely to show improvement in test scores unless they studied between attempts."
"Yu,_Heng,_Arden-Gard-with-image-refs","/home/mlazar/projects/thesis/tbp_articles/life_articles/processed_articles/Yu,_Heng,_Arden-Gard-with-image-refs.md","/home/mlazar/projects/thesis/tbp_articles/life_articles/processed_articles/Yu,_Heng,_Arden-Gard-with-image-refs_artifacts/image_000003_97c4e4769b9d432d26613fc4b5c432f31defbf4903a118ce7cc0b09d525993f4.png","## Experiment 2 Criterial Test Results

Table 2 Experiment 2 Pairwise Comparisons of Learning Condition and Its Interaction with

Question Type on the Criterial Test

| Analysis type                  | Pairwise comparisons           |     b |    SE | 95% CI         | p        |     d |
|--------------------------------|--------------------------------|-------|-------|----------------|----------|-------|
| Main                           | Feedback vs. Control           | 0.058 | 0.045 | [-0.047, 0.16] | .396     | 0.2   |
| effects                        | Feedback vs. Non-feedback      | 0.043 | 0.045 | [-0.063, 0.15] | .606     | 0.15  |
|                                | Non-feedback vs. Control       | 0.016 | 0.045 | [-0.090, 0.12] | .936     | 0.053 |
|                                | Tested vs. Untested questions  | 0.028 | 0.037 | [-0.044, 0.10] | .437     | 0.097 |
| ModeratingFeedback vs. Control | ModeratingFeedback vs. Control | 0.36  | 0.089 | [0.15, 0.58]   | <.001*** | 1.24  |
| effects of                     | Feedback vs. Non-feedback      | 0.27  | 0.089 | [0.056, 0.49]  | .005**   | 0.93  |
| question type                  | Non-feedback vs. Control       | 0.093 | 0.089 | [-0.12, 0.31]  | .299     | 0.32  |

Note. Main effects refer to pairwise comparisons of learning conditions. Moderating effects of question type refer to pairwise comparisons of the interaction between learning condition and question type. * = p &lt; . 05; *** = p &lt; .001."
A_Human-centered_Con-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/social_articles/processed_articles/A_Human-centered_Con-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/social_articles/processed_articles/A_Human-centered_Con-with-image-refs_artifacts/image_000003_f98fe0c0fd41f25cb7f9adfefd58f4f30624d39915dee299cb92e27fe12c4333.png,"## Reimer et al.

owners indicated that their charging behavior is triggered more by a situation in which the state of charge falls to a certain level.

In their reported frequency of how often they charge their car at a public charging station, EV and PHEV owners show a remarkable overlap in their responses (see Figure 3 ). Overall, 29.8% of electric (EV) car owners charge their car less than once a month, while 40.4% charge it several times a month, 25.4% several times a week, and 4.4% daily. EV users appear to use public charging stations at a similar rate as PHEV users, with a 4% larger group of PHEV users reporting that they charge their car several times a week at a public charging station (29.4% PHEV vs 25.4% EV users).

How often do you use public electric charging stations to charge your vehicle?

Figure 3 Percentage of Users Reporting Different Frequencies of Charging Separately for Electric Car and Plug-in Hybrid Car Users

empty as possible. This percentage rose to 24.8% of hybrid car users, but was still very low compared to 1 drivers of cars with combustion engines in the Philipsen et al. ( 1 ) study. 2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

Charging frequency

Figure 4 Reported Frequencies of Critical Battery/Fuel Levels that Cause Discomfort among Drivers Compared to the Reported Frequencies in Philipsen et al. ( 1 )"
Effector-specificity-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/social_articles/processed_articles/Effector-specificity-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/social_articles/processed_articles/Effector-specificity-with-image-refs_artifacts/image_000000_cd4d25ddcc675cde345fa457fba2feaaa7e81889a478ae219f18d689c6ef6a5b.png,"## Research Gap and Hypotheses

The literature is currently mixed regarding the superiority of one effector over another in beat synchronization tasks. While some studies report that SMS to music is more natural with the lower limbs, whether this translates to improved performance over SMS with the more dexterous fingers has yet to be investigated directly. Therefore, this experiment directly compares the biomechanical (via the intercept) and cognitive (via the slope) performance of finger and foot synchronization to music of varying rhythmic complexity. We propose three competing hypotheses:

Naturalness hypothesis. Naturalness is associated with greater ability, so foot tapping should outperform finger tapping. This would be reflected in both a lower intercept, indicating lower baseline variability, and a flatter slope, indicating less variability at higher rhythmic complexity.

Dexterity hypothesis . Because humans have better motor control over their fingers than their feet, finger tapping should be less variable than foot tapping. This would manifest in lower intercepts and flatter slopes with finger tapping, suggesting fewer biomechanical and cognitive constraints.

Central pattern generator hypothesis. Sensorimotor synchronization is not effector-specific at all and is instead governed by a unified neural system that coordinates synchronization equally well with any limb, regardless of rhythmic complexity. Thus, both the intercepts and slopes should be equivalent for SMS with the foot or finger.

It is important to note that because we are assessing two different parameters of the linear model, the intercepts and the slopes, it is possible to observe some combination of the above hypotheses. For instance, foot synchronization may exhibit a higher intercept than finger synchronization due to the inherent variability in moving a larger effector, however, foot synchronization may still exhibit a flatter slope with rhythmic complexity owing to its more natural ability to synchronize to slower, beat-related frequencies. A schematic of all three hypotheses is depicted below in Figure 1.

Figure 1 . Schematic illustrations of the three hypotheses. The left column represents the pattern of results that would support the naturalness hypothesis, the middle column represents the pattern of results that would support the dexterity hypothesis, and the right column represents the pattern of results that would support the central pattern generator hypothesis. The top row illustrates intercept effects while the bottom row illustrate slope effects."
Generative_Artificia-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/social_articles/processed_articles/Generative_Artificia-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/social_articles/processed_articles/Generative_Artificia-with-image-refs_artifacts/image_000001_dc35201b83ef8c5abed17e7f72aebc59dc00780b6d0ff4c78a76697dbb5cf338.png,"## Sex Invariance

Measurement invariance across biological sex was examined using a stepwise multi-group CFA. The configural model, which imposes no equality constraints, demonstrated acceptable fit ( χ² (82) = 186.28, p &lt; .001, CFI = .97, TLI = .95, RMSEA = .08, SRMR = .04), indicating that the hypothesized factor structure was similar for males and females. Additionally, metric invariance was supported, as the model constraining factor loadings to equality showed only a minimal change in fit (ΔCFI = .004), with improved AIC and BIC values compared to the configural model. Scalar invariance, which further constrained item intercepts, was also supported, with an additional negligible change in fit (ΔCFI = .002) and further reduced AIC and BIC values. Finally, the residual invariance model (equal loadings, intercepts, and residuals) yielded a slightly larger decline in fit (ΔCFI = .014; Table 4) . Together, these results provide evidence for scalar, but not residual 1 invariance across sex.

Table 4 Summary of Model Fit Statistics for Sex Invariance

| Model                   |   CFI | ΔCFI   |     AIC |     BIC |
|-------------------------|-------|--------|---------|---------|
| Configural              | 0.966 |        | 9961.48 | 10250.6 |
| Metric (vs. Configural) | 0.962 | .004   | 9963.79 | 10212.8 |
| Scalar (vs. Metric)     | 0.96  | .002   | 9962.07 | 10183   |
| Residual (vs. Scalar)   | 0.946 | .014   | 9975.32 | 10152   |

Note. ΔCFI = change in CFI relative to the previous model. Lower AIC and BIC values indicate better model fit. All ΔCFI values are below the recommended cutoff of .01 (Cheung &amp; Rensvold, 2002).

1 Modification indices suggested that partial strict invariance could be attained by freeing the residual variances of 'I have trouble completing work or other responsibilities without generative AI' and 'I get frustrated or irritable when I am unable to use generative AI'.

Figure 2 CFA Model of the Generative AI Dependency Scale

Note. N = 410. All estimates refer to standardized estimates. All estimates were statistically significant with p s &lt; .001."
Identifying_Psycholo-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/social_articles/processed_articles/Identifying_Psycholo-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/social_articles/processed_articles/Identifying_Psycholo-with-image-refs_artifacts/image_000001_1c7f013d40d0e3ac2ab64a12936e677201f61773457112aa7ec3f2ecf045b911.png,"## CIBER plot for potential determinants of safe listening

Means and associations (d) with Safe Listening (R? = .4 | .57)

Figure 2. Confidence interval -based estimation of relevance (CIBER) plot with, on the left panel, psychological factors ' distributions and sample means (purple diamonds for participants listening ≤ 2.5 hours/week at a loud volume, and green diamonds for those listening &gt;2.5 hours/week at a loud volume) and, on the right panel, Cohen's d for associations of psychological factors with safe ear- and headphone listening."
Impact_of_cash-out_a-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/social_articles/processed_articles/Impact_of_cash-out_a-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/social_articles/processed_articles/Impact_of_cash-out_a-with-image-refs_artifacts/image_000007_693ab24a53bef8712e68f103c1d2f105c0d645858890177365c821fced2d758b.png,"## Moderation by PGSI

There was no interaction between continuous PGSI score and cash-out availability for either stake size ( β =-0.02, SE =0.02, p =0.15) or implied win probability of placed bets ( β&lt; 0.01, SE&lt; 0.01, p =0.44). Cash-out availability also did not significantly predict either outcome in the models adjusted for PGSI (stake size β =0.08, SE =0.08, p =0.32;  implied  win  probability β =-0.01, SE =0.01, p =0.35).  See  Table  S9  in  the supplement for the full models.

Fig  5. Subgroup  analyses  of  effects  of  cash-out  availability  among  low-  (&lt;  3)  and  high-PGSI  (&gt;=  3) participants. The low-PGSI subgroup showed a significant tendency to place larger bets when cash-out was available, which was not observed in the high-PGSI subgroup (A). Neither subgroup showed an effect of cash-out availability on bet riskiness (B). Note: Individual data points have been jittered by up to £0.01 in the stake-size plots and up to 0.002 in the riskiness plots for ease of viewing.

As the effects of gambling products on those most at risk of gambling harms are of particular importance, we also pre-registered re-running all the analyses for H2 and H3 for those participants at moderate-to-high risk of gambling harms (PGSI &gt;= 3). For comparison, we also provide exploratory analyses of the lowPGSI subgroup.

Figure 5 shows the result for stake size (Panel A) and riskiness (Panel B) split by PGSI subgroup. There was no significant effect of cash-out availability on stake size among the high-PGSI subgroup ( W =1699.5, p =0.89, n=117). In contrast, the low-PGSI subgroup showed mixed evidence: median stake was almost 50% greater when cash-out was available (no-cash-out £0.69, IQR: £0-£1.00; cash-out-available £1.00, IQR £0-£1.00), and this difference was significant (W=6909.5, p =0.0076, n=260). As with the full sample, there were no effects of cash-out in the mixed-effects regression for either subgroup (low-PGSI subgroup: (β=0.11,  SE=0.07, p =0.16;  high-PGSI  subgroup:  β=-0.18,  SE=0.12, p =0.16).  See  Table  S10  in  the supplement for the full results.

Similar to the results from the analysis of the full sample, there were no significant effects of cash-out availability on bet riskiness in either the high-PGSI ( W =985, p =0.55, n=90) or the low-PGSI subgroups ( W =5654.5, p =0.18, n=206). Neither subgroup showed any significant association in the linear mixedmodel analysis either (see Table S11 in the supplement)."
Instrumental_variabl-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/social_articles/processed_articles/Instrumental_variabl-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/social_articles/processed_articles/Instrumental_variabl-with-image-refs_artifacts/image_000002_1a7094d50a98c33d5d1dee5d30bfb19a81e16de5d3856b51477d484e1f7b9a08.png,"## 1 Instrumental Variables Regression with Latent Variables and Differential Item Functioning

Instrumental variables regression (IVR) is an approach to reducing bias in the estimate of a treatment effect when the treatment is endogenous-that is, not randomly assigned (Angrist &amp; Krueger, 2001; Angrist &amp; Pischke, 2009; Murnane &amp; Willett, 2010). Most treatments of IVR use the econometric two-stage least squares (2SLS) approach to estimation. However, IVR can also be represented within a structural equation modeling (SEM) framework by allowing for correlated error terms between the endogenous treatment variable and the outcome, effectively modeling it as a type of mediation (Maydeu-Olivares et al., 2019; Murnane &amp; Willett, 2010).

Figure 1 shows a path diagram illustrating this IVR-as-SEM approach with one instrument. When the model assumptions are met, this approach yields an unbiased estimate of the treatment effect and provides accurate standard errors, equivalent to those generated in 2SLS using a robust sandwich estimator (Maydeu-Olivares et al., 2019, p. 880).

Figure 1: Path diagram for instrumental variables regression conceived as a structural equation model

Notes: ε 0 is the error term for the outcome; ε T is the error term for the endogenous treatment.

Although Maydeu-Olivares et al. (2019) limited their analysis to observed variables, one of the strengths of SEM is the ability to model latent variables and estimate relations between them that are corrected for measurement error. This involves embedding a confirmatory factor analysis (CFA) model (J¨ oreskog, 1969) within a larger structural model, where the CFA defines the latent

variable as an unobserved common cause of observed variables known as its indicators. Figure 2 extends the SEM treatment of IVR to the case of a latent outcome variable. To make this more literal and preview the focal analysis of this paper, we have replaced 'instrument,' 'treatment,' and 'outcome' with actual variables from an analysis of the influence of time spent on homework on math achievement that we expand upon below (Gustafsson, 2013). This model aims to estimate the effect of time spent on homework on math achievement. However, the original study was motivated by a concern about 'reverse causation': time spent on homework may positively influence math achievement, while poor math achievement may also drive more time spent on homework. The presence of the instrument, teacher-assigned homework time, enables the separate estimation of the relation between exogenous homework time in the presence of reverse causation if the model's assumptions are met. Furthermore, modeling math achievement as a latent variable corrects for attenuation bias in the main effect induced by outcome measurement error. Without correction, measurement error would affect standardization of the model coefficients and reduce power to detect statistically significant relations (Gilbert, 2025; Kline, 2023; Shear &amp; Briggs, 2024).

Figure 2: Path diagram for instrumental variables regression conceived as a structural equation model with a latent outcome

Notes: ε 0 is the error term for the outcome; ε T is the error term for the endogenous treatment. I1-I5 are observed indicators of latent math achievement. Squares represent observed variables, circles represent latent variables.

IVR relies on several well-known assumptions, most notably, that the instrument affects the outcome only through its influence on the treatment (the exclusion restriction), and that the instrument

is strong and applied to a sufficiently large sample (Murnane &amp; Willett, 2010; Winship &amp; Morgan, 1999)-When the outcome is modeled as a latent variable, an additional assumption comes into play: that the measurement model, specifically, the confirmatory factor analysis (CFA) portion, is invariant across levels of the endogenous treatment. In the context of Figure 2, this means that student homework time should affect performance on individual items (I1-I5) via its influence on the underlying latent variable Math . In other words, the relationship between homework time and item responses must be fully mediated by the latent variable (Stoetzer et al., 2025). This assumption is referred to as factorial invariance (Meredith, 1964, 1993; Thissen, 2025), and as differential item functioning (DIF) in the Item Response Theory (IRT) tradition (Angoff, 1981; Angoff &amp; Ford, 1973; Holland &amp; Thayer, 1986). Both frameworks deal with the question of whether the given latent trait is being measured equivalently across different groups or conditions. Factorial invariance, i.e., a lack of DIF, is generally an assumption of causal models that estimate the effect of a treatment on a latent variable (P. Halpin &amp; Gilbert, 2024; Stoetzer et al., 2025; VanderWeele &amp; Vansteelandt, 2022).

A violation of the assumption described above implies that some test items are directly influenced by time spent on homework, beyond its mediated effect through the latent variable of math achievement. This situation, illustrated in 3, introduces item-specific effects into the model. When this occurs, it becomes necessary to distinguish between the overall treatment effect, referred to as impact, and deviations at the item level, known as DIF.

Importantly, DIF is not inherently problematic or informative. Its implications depend on the specific context of the analysis, particularly the groups being compared and whether the DIF is construct relevant. In some cases, DIF may signal measurement bias that undermines valid estimation of impact (P. Halpin &amp; Gilbert, 2024; P. F. Halpin, 2024; VanderWeele &amp; Vansteelandt, 2022). In others, it may reveal meaningful differences in how treatment effects unfold across items, offering insight into underlying mechanisms of impact (Gilbert, Himmelsbach, Soland, et al., 2025; Gilbert, Kim, &amp; Miratrix, 2024; Gilbert et al., 2023). This study introduces two analytic

approaches to handling potential DIF when applying IVR with latent variables, and demonstrates their application.

Figure 3: Path diagram for instrumental variables regression conceived as a structural equation model with potential DIF

Notes: ε 0 is the error term for the outcome; ε T is the error term for the endogenous treatment. I1-I5 are observed indicators of latent math achievement."
Interpersonal_Cardia-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/social_articles/processed_articles/Interpersonal_Cardia-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/social_articles/processed_articles/Interpersonal_Cardia-with-image-refs_artifacts/image_000000_d05c1ac83ff7eafab2043bbab59656ee59686490d5754f6824434cde0c9f65b8.png,"## 2.4 ECG analysis

We preprocessed ECG signals using Neurokit2 (Makowski et al., 2021) in Python. We trimmed the raw 130 Hz ECG data to retain only the 15-minute interaction phase, ensuring consistency across recordings. Of the 40 dyads recorded with ECG, 11 dyads were excluded due to technical issues such as missing video, audio problems, or sensor malfunctions, resulting in a final sample of  28  dyads  (56  participants)  for  ECG  analysis.  An  additional  dyad  was  then  excluded  due  to excessive ECG data loss (&gt;5% missing; (Heathers et al., 2014)). For the 28 remaining dyads, we applied cubic spline interpolation to ensure data continuity. To remove artifacts, we applied a 0.5 Hz high-pass  Butterworth  filter  to  correct  for  baseline  drift,  a  60  Hz  notch  filter  to  eliminate electrical noise, and a 20 Hz low-pass filter to remove high-frequency components. We detected

Rpeaks  using  Neurokit2's  ecg\_findpeaks  function,  with  manual  corrections  for  erroneous detections based on peak-to-peak interval discrepancies. We computed inter-beat intervals (IBIs) and  removed  IBIs  outside  the  600 -1000  ms  range  using  manual  thresholding.  Across  all participants, an average of 2.7% of IBIs were rejected during preprocessing due to falling outside the acceptable range. To ensure that data quality did not influence our main results, we examined whether the proportion of excluded IBIs was associated with key outcome variables. No significant associations were found, indicating that data quality did not drive the observed effects. We then calculated  HRV  using  root  mean  square  successive  RR  interval  differences  (RMSSD)  via Neurokit2's ecg\_inte rvalrelated function, a validated index of vagal tone (Appelhans &amp; Luecken, 2006; Shaffer et al., 2014).

Figure 1 IBI Timeseries Sample Plot and Cross-Correlation between 2 residualized IBI series

Note. This figure displays a sample plot of a part of an IBI time series for a support SP (blue) and SR (green), along with the corresponding cross-correlation plot. The left panel (A) shows the IBI fluctuations  over  time,  while  the  right  panel  (B)  illustrates  the  cross-correlation  values  across different time lags."
Introducing_the_koll-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/social_articles/processed_articles/Introducing_the_koll-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/social_articles/processed_articles/Introducing_the_koll-with-image-refs_artifacts/image_000004_2e73605534ef545a6d763fa20d9b75c5543f4f7dcb0a155f53e23fd8db8e76a9.png,"## Examining the impact of parameter settings on fixation classification

Visualizations are useful for selection of event classification and their algorithms. Figure 5 shows the fixations identified by the I-VT algorithm using three different threshold parameter values in a segment of data from participant 1. Data are plotted in a 2D space. The plot was generated by the kollaR function filt\_plot\_2d . Reassuringly, the three threshold values lead to consistent results in this data segment. If results look

similar for a range of different sub-sections of the data set, it suggests that the choice of algorithm is not affecting the results much.

Gaze position Y

Figure 5. Fixations of classified by the I-VT algorithm at different velocity threshold values. Data from participant 1. Gaze position is shown in pixels.

Figure 6 shows the fixations detected by two different algorithms during a period of data from participant 2. Although results ares similar, the Identification by 2 means clustering (I2MC) algorithm reports a fixation in the upper part of the plot which is not reported by the I-VT algorithm. Visualizations like these over a range of data segments can indicate

systematic differences between algorithms. This would merit further examination before selecting an algorithm. It would be important to examine whether the discrepancies between algorithms are more likely to be found in time segments characterized by noisy data. Researchers examining group differences (for example, in fixation time between autistic and non-autistic children) would be interested in whether data from both groups are equally sensitive to differences between algorithms. It would also be of interest to know which outcome measures these discrepancies could affect. For example, do the algorithms disagree on the number of fixations, but not on their total duration or location? This could indicate a problem for a study that examines the number of fixations, but not necessarily pose a threat to an analysis of total fixation duration.

Gaze position Y"
Learning_visual_appe-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/social_articles/processed_articles/Learning_visual_appe-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/social_articles/processed_articles/Learning_visual_appe-with-image-refs_artifacts/image_000002_c1dbb446a7a7cd54abd1453b153f78712316e5932f3cbc3718380c64a85ad46b.png,"## Blind and sighted people infer object colorfulness from intended function

Both  sighted  and  congenitally  blind  participants  judged Colorfulness-Intent ( Intent ) artifacts to have more colors than No-Colorfulness-Intent  ( No-Intent ) artifacts (subject-wise repeated measures ANOVA, 2 conditions ( Intent  No-Intent , ) x 2 groups  (sighted, blind): main  effect of condition, F (1,36) = 441.29, p &lt; .001; Figure 2).

All  groups  assigned  more  colors  to  artifacts  for  which colorfulness is intended to facilitate function (i.e., Colorfulness-Intent artifacts; all p s &lt; .001). This effect was highly  significant  in  each  group  and  statistically  identical across groups (group by condition interaction, F (1,36) = 0.03, p = .88; main effect of group, F (1,36) = 1, p = .32).

Figure 2: Average item-wise colorfulness judgments across groups. Average non-normalized judgments for all 60 artifacts are displayed for each group. Paired artifacts (e.g., 'fairy  tale  book,'  'instructional  manual')  are  connected  by lines.

In  both  groups,  colorfulness  judgments  were  highly  and equally correlated with ratings of how helpful colorfulness is to  function  collected  in  a  separate  online  study  (sighted: ρ  =  .84, p &lt; .001,  blind:  ρ  =  .82, p &lt; .001).  These  results support  the  hypothesis  that  people  born  blind  use  this information to predict the number of colors an object is likely to have.

Further evidence for the idea that blind people can infer colorfulness  from  intention  comes  from  analyses  of  itemwise variation. Since items within a pair were matched on shape, differences in the number of colors assigned within Intent No-Intent / pairs  offers  a  more  fine-grained  test  of whether the degree to which colorfulness facilitates function influences estimates of colorfulness. Difference scores were highly  correlated  across  the  sighted  samples  (ρ  =  .86, p &lt; .001) and were also correlated across sighted and congenitally blind people (ρ = .51, p = .004). Notably, the two sighted samples exhibited numerically higher agreement than sighted and blind participants, suggesting that sighted people may rely partially on visual memory when making colorfulness judgments, whereas people born blind may rely more on inferences about intention.

Finally,  we  observed  numerical  differences  in  the  color labels that blind and sighted people used across Intent and No-Intent artifacts, whereby brighter colors (e.g., blue, red, green)  were  produced  more  often  for Intent artifacts  and neutral colors (e.g., black, white) were produced more often for No-Intent artifacts (Figure 3). Differences in color label (ρ = .98, all p s &lt; .001).

comparison group (sighted reference, GPT-4), F (1,36) = 54.45, p &lt;  .001).  This  finding  suggests  that  although  GPT-4  can acquire information about artifact colorfulness from language, it does not fully capture human performance.

use  across  conditions  was  highly  consistent  across  groups                  4, and the human groups (item-wise repeated measures Like blind and sighted people, GPT-4 assigned more colors to  artifacts  for  which  colorfulness  is  intended  to  facilitate function. (Figure 2; Intent vs No-Intent artifacts t (58) = 4.56, p &lt; .001). The size of this effect was not different between GPTANOVA,  2  conditions  ( Intent , No-Intent ) x 3 groups (sighted,  blind,  GPT-4),  group  by  condition  interaction, F (2,116) =  0.27, p =  .76;  main  effect  of  group,  F (2,116) =  0, p = 1).

Together, these results suggest that people born blind infer colorfulness of artifacts by appealing to the intentions of the maker (e.g.,  fairy  tale  books  are  colorful  because  they  are intended to capture the reader's attention; Table 1).

Table 1: Example explanations from each group. For each object,  participants  were  asked,  'why  did  you  choose  that number of colors?'

|         | Fairy tale book                                                             | Instructional manual                                                           |
|---------|-----------------------------------------------------------------------------|--------------------------------------------------------------------------------|
| Sighted | The ones we own are colorful.                                               | They are usually not too colorful from my experience.                          |
| Blind   | They are usually geared toward children, and colors grab a kid's attention. | It would have to have one color to be easy to find among other booklets.       |
| GPT-4   | Most have colorful illustrations to make the stories more engaging.         | Most I've seen usually have a base color, a color for accents, and black text. |"
"Power,_Privilege,_an-with-image-refs","/home/mlazar/projects/thesis/tbp_articles/social_articles/processed_articles/Power,_Privilege,_an-with-image-refs.md","/home/mlazar/projects/thesis/tbp_articles/social_articles/processed_articles/Power,_Privilege,_an-with-image-refs_artifacts/image_000000_108c0a4901f0b099f12637570fad2df6b51ee033a6809034d28bbbee15a3982e.png","## Principle 2: Fostering an acculturated climate via consciousness-raising

An inclusive intellectual climate is one that is not merely diverse but acculturated-actively resisting the gravitational pull of dominant norms (i.e., enculturation) in favor of pluralistic values. Achieving such a climate requires critical reflection on common entrenched practices and unconscious psychological mechanisms that inculcate and sustain privilege. Consciousnessraising as a means towards acculturation is therefore an elemental component of equity and inclusion.

Cultivating an acculturated environment depends in part on the presence of acculturated leadership (Dupree &amp; Boykin, 2021). However, acculturated leadership by individuals from under-represented backgrounds can challenge dominant assumptions about what constitutes effective leadership. Prevailing norms continue to favor masculinized models of authority, reinforcing narrow expectations that can devalue alternative leadership styles shaped by diverse identities and experiences (Cheryan &amp; Markus, 2020; Gruber et al., 2020). When new leaders depart from these traditional norms, frequently as an expression of their identities and experiences, the reception of their leadership becomes an inflection point for either resistance or cultural transformation. In particular, a departure from traditional norms via diverse leadership representation can introduce role incongruity, wherein leaders whose identities depart from established prototypes must simultaneously meet expectations associated with both their perceived social identity and their leadership role. When these expectations conflict, leaders often encounter heightened challenges, including prejudice and bias (Eagly &amp; Karau, 2002). For

instance, Asian women leaders are reported to adopt a relational and collaborative leadership style that emphasizes empowering others, distributing power, and leveraging collective strengths to achieve shared goals (Kawahara et al., 2013). Yet, despite its collaborative orientation, this style is often perceived as less agentic and, therefore, less competent (Rosette et al., 2018). Leadership effectiveness in such cases is greater when the surrounding environment engages with, rather than resists, alternative leadership styles (Kawahara et al., 2013).

Efforts to diversify leadership must therefore be accompanied by an institutional reckoning with the unique challenges faced by leaders from historically excluded groups, challenges often invisible and unacknowledged by department citizens and central administration. Without deliberate cultural shifts and institutional support for diverse leaders, they may find themselves on a constant quest for legitimacy, undermining both their effectiveness and retention. For example, Asian female leaders often find themselves continually challenged on issues never broached with their more traditionally positioned predecessors or peers. Continual second-guessing, microaggressions, and other subtle but pervasive forms of questioning authority contribute to fatigue, frustration, and eventual burnout (Võ, 2012). This underscores the importance of cultivating an acculturated climate that not only elevates individuals from underrepresented backgrounds to positions of authority, but also equips them with the institutional support necessary to succeed and thrive in those roles.

A parallel dynamic can be observed in everyday interactions within departments that can sustain enculturated practices. For example, it remains common for those in positions of power to support and advocate for others that share their own background characteristics, interests, or traits, reflecting homophily bias (Stewart &amp; Valian, 2018). This tendency to afford opportunities to like-minded individuals fosters a homogeneous and enculturated community rather than a

diverse acculturated departmental community, where conformity to a dominant identity becomes the primary criterion for inclusion, often at the expense of merit (van den Brink &amp; Benschop, 2014). The establishment of heterophilic networks requires department citizens to recognize and intentionally disrupt homophilic tendencies. This demands from individuals a true commitment to restorative sacrifice, acts that may entail personal cost, but ultimately serve to advance the equity agenda of a department. In this regard, naming and normalizing restorative sacrifice can be a valuable step towards equity of opportunity.

As academic environments diversify, so too must their culture and climate. An inclusive academic climate necessitates a shift from enculturation, which reinforces dominant norms and pressures individuals to assimilate into the existing status qu o, toward acculturation, which embraces pluralism and values diverse cultural contributions. True inclusion requires broadening institutional norms and expectations to accommodate identities and experiences that may diverge from established precedents. The extent to which these departures are acknowledged and supported critically shapes the potential for genuine acculturation. Moreover, consciousnessraising via explicit recognition of implicit cognitive bias (e.g., homophily; social stereotyping; implicit associations) can illustrate how existing norms can favor sameness and limit diversification.

Principle 3. Establishing the preconditions for internal alignment and cultural change

While universities are engines of innovation and intellectual progress, the pace of institutional change within academia can be notoriously slow. As astutely noted by Brian Rosenberg, 'Look at any mission statement for any college or university, and you will probably find a word like transformational or transformative. And look at the work of any faculty member in any discipline, and they will tell you that they're trying to push the boundaries of their

discipline and change things. But when it comes to the way these institutions operate, there is, in fact, a powerful resistance to, reluctance to, opposition to change' (Anderson, 2023). Rosenberg provides examples of how particular structures, such as shared governance and tenure systems, can have the effect, if not the intent, of incrementalizing change. In doing so, these systems can limit transformative change (Rosenberg, 2023). As a result, gradualist approaches to change based on a strong commitment to the status quo can take precedence over the kind of transformative change needed for cultural change. Cultural change is inherently complex and often elusive, owing to the intangible nature of culture itself. Institutional cultures, like human cultures, are a product of shared values, norms, and belief systems that shape daily practices and interactions, often operating beneath the level of conscious awareness or observable behavior. As such, changing culture presents a particularly challenging endeavor.

A substantial body of psychological research has focused on developing theories of cultural change at an evolutionary scale to explain how cultural patterns emerge, persist, and adapt within human societies. These theories not only describe cultural changes, but seek to explain why behaviors emerge, how they are transmitted, and how they become entrenched over time, even when they are maladaptive. Theories of cultural evolution account for how behaviors are reinforced or extinguished through subtle ecological and social selection pressures (Boyd &amp; Richerson, 2005; Lumsden &amp; Wilson, 1981; Varnum &amp; Grossmann, 2017). Understanding cultural evolution thus requires careful attention to the origins, persistence, and adaptability of behavior over time. Analogously, understanding organizational culture entails a similar analysis of how institutional norms are historically produced, socially reproduced, and adaptable to contextual pressure. The following section outlines three preconditions necessary to support inclusive and equity-oriented cultural transformation within academic departments.

A first precondition is cultivating a department-wide commitment to change . As organizational theorists note, transformational change requires broad-based recognition of both the need for change and strategies to overcome barriers to achieving it (Kotter, 1996). Within academic departments, however, such recognition is rarely uniform; disunity around change is to be expected. Resistance is especially pronounced when proposed reforms threaten the power, privilege, or autonomy of dominant actors-those who possess the most institutional capital, enjoy structural protections, and exert significant influence over decision-making processes (Kezar, 2014; Rosenberg, 2023). In particular, long-standing departmental members who have historically benefited from unexamined privilege often emerge as the most persistent opponents of cultural change that threatens their personal or professional interests, though their resistance is frequently tacit, operating subtly beneath the surface.

Academic systems have long demonstrated that cultural change is often resisted by those who benefit from the existing structures of power and privilege. A quintessential example of this is the introduction of external peer review, a process now regarded as a basic standard, but in its modern form, is a relatively recent development. Peer review was first introduced in 1665 by the Royal Society in London, intended to facilitate communication among a small, select group of elite scientists through letters, commentaries, and statements. Over time, this evolved into a system in which members of these self-organized scientific communities, serving as journal editors, would evaluate each other's scientific contributions and determine their suitability for publication. For example, Watson and Crick's famous 'double-helix' paper was deliberately withheld from external peer review because the Editor considered its correctness to be 'selfevident,' despite the authors themselves acknowledging that their model 'must be regarded as unproved until it has been checked against more exact results' (Watson &amp; Crick, 1953, p. 737).

As conceded by a later Editor of Nature, sending the submission for external peer review would have revealed that the foundational data belonged to a female colleague, Rosalind Franklin, whose critical contributions were minimized in this highly celebrated work (Baldwin, 2015a).

This exclusive model remained largely intact until the mid-20th century, when journals began issuing open calls for submissions and sending papers to external experts for peer review prior to publication. Notably, when prestigious journals adopted this new model, they faced considerable opposition and criticism from prominent scientists which protracted the time required to institutionalize this change (Baldwin, 2015b). Detractors of external peer review (which included Albert Einstein, who withdrew his own submission to Physical Review , objecting to the Editor's decision to send it outside experts for evaluation) argued that the existing system, which openly privileged those with influential personal and professional networks, was effective and thus did not warrant reform.

In today's world, such open and flagrant resistance to inequitable practices would likely be regarded as socially and professionally unacceptable. However, while the outward expression of opposition has shifted, the underlying drive to maintain inequitable practices can persist in more socially palatable forms. Today, common defenses of selective privilege within academic departments include appeals to institutional precedent (e.g. 'This is how things have always been""), concerns about destabilizing departmental operations, assertions of individual exceptionalism to justify special treatment, strategic delays, or situationist arguments that frame a faculty member's personal or professional circumstances as unique. Akin to detractors from external peer review, resistors are typically institutionally and structurally empowered beneficiaries of selective privilege in the academy, both historically and contemporarily.

In considering how to address subtle defenses of selective privilege, it is helpful to delineate the different types of inequity that give rise to selective privilege as this delineation has direct consequences for the ease and expediency with which inequities can be corrected. At one extreme, sources of structural inequity are particularly complex to correct as they often emerge as unintended consequences of longstanding institutional priorities and practices that reproduce systemic unfairness. These inequities are cumulative, diffuse, and can be difficult to define, rendering them resistant to rapid correction (Haslanger, 2023). By contrast, procedural and distributive inequities are typically more visible and measurable (Hartman et al., 1999), making them easier to expediently correct. These may include disparities in teaching assignments, variation in service workloads, or inconsistencies in the application of tenure and promotion criteria. Recognizing such inequities without taking immediate corrective action undermines departmental and institutional commitments to basic fairness and accountability.

Determining a department's commitment to change requires open engagement amongst its citizens. This necessitates truly inclusive channels of communication where the viewpoints of all members can be represented. Academic departments, like many organizations, often grapple with participatory inequity as it relates to dominance, wherein influence is disproportionately afforded to the most structurally empowered individuals. A routine expression of this at the departmental level is participation behaviors in faculty meetings. A simple audit of who speaks and who remains silent during faculty meetings reveals participation inequities that are often noticed, but seldom addressed through systematic intervention (Edmondson, 1999; Morley, 2013). As noted earlier, members of hierarchical institutions internalize beliefs about when and how it is safe to speak up (Detert &amp; Edmondson, 2011). These beliefs are unevenly distributed: marginalized faculty are more likely to perceive risks in expressing dissenting views, particularly

when advocating for equity-related change, which can be mistakenly construed as being driven by self-interest rather than by principle. Thus, examining participatory behaviors around discussions of change and committing intentional efforts to broaden participatory dialogue must also address the structural conditions (e.g. status hierarchies; norms of deference) that limit full engagement.

A second precondition is structural reinforcement of desired change . Culture change cannot be sustained without an alignment of systems of rewards, accountability mechanisms, and measurement of outcomes. Misaligned reward structures can incentivize behaviors that directly contradict institutional goals (Kerr &amp; Slocum, 1987). Within academia, individuals in positions of power may be tacitly rewarded for maintaining the status quo , regardless of the equity implications of their conduct (Fazackerley, 2025). This misalignment perpetuates maladaptive behaviors and shields those with privilege from scrutiny and accountability. For change to be effective, reward structures must be calibrated to goals, and systems of accountability must be introduced to support these new reward structures (Bushardt et al. 2011).

Connected to structural reinforcement is the need for an appraisal of the cost of change, both tangible and intangible. Organizational change often demands significant time, cognitive effort, and academic labor from those involved. Even ostensibly simple changes, such as transitioning to new digital platforms or introducing new operational procedures, can impose burdens that contribute to change fatigue or disengagement. Culture change, which involves addressing deeply held behaviors as they connect to values, carries an even greater risk of change fatigue. This fatigue can be particularly pronounced among those who are already overextended, such as marginalized faculty who are frequently called upon to do diversity labor without institutional recognition (Dancy &amp; Hodari, 2023). Successful transformation also therefore

requires the allocation of adequate resources-whether material, informational, or relational-to support those tasked with leading and enacting change.

Finally , mechanisms for measurement of change are a crucial, yet frequently underdeveloped, precondition for institutional transformation (Kezar, 2014; Eckel &amp; Kezar, 2003). Establishing clear, measurable goals with systems of accountability provides a shared reference point for assessing progress towards desired change (Bess &amp; Dee, 2008). Academic strategic plans often impose rigid timelines for change that fail to account for the unpredictable and fluid nature of culture change for which a chronology is often genuinely hard to predict. While some changes can be implemented relatively quickly (e.g., telework policies; committee structures), others, such as shifting departmental mindsets or building trust among colleagues, unfold over longer periods and require an ongoing commitment (Kezar, 2014). As such, evaluation frameworks must strike a balance between ambition and pragmatism. They should include both process indicators (e.g., participation patterns, qualitative feedback) and outcome indicators (e.g., retention patterns, diversity in faculty representation), while allowing for adaptation in response to new insights or barriers that emerge along the way (Kotter, 1996). Culture change is not a linear endeavor, and its success hinges on a shared implementable vision of what inclusive department life can and should look like.

Good intentions, uncertain outcomes: Navigating the complex terrain of equity implementation The aforementioned orienting principles for change offer a conceptual foundation for a more inclusive Psychology department (see Figure 1 for a graphical summary). However, there are challenges and threats that complicate the implementation and realization of these principles.

Figure 1. A graphical map of orienting principles towards inclusive excellence.

As is evident from the statistics on faculty hiring and retention in Psychology, the pace of change has been troublingly slow. A lack of change is not necessarily a product of apathy or negative intent; rather, it can reflect the structural and cultural complexities of academic institutions. Shifting departmental norms and re-establishing institutional priorities is profoundly challenging work. One contributing factor to gradualist change in academia is the nature of tenure, which is designed to support long-term employment and academic freedom. As a result, academic careers within departments are often significantly longer than those within other sectors. Moreover, the agency that tenured positions grant to faculty cuts both ways. On one hand, it empowers faculty to champion institutional change. On the other, it enables them to disengage entirely from almost any institutional agenda. This means that institutions rely heavily

on generational turnover, rather than more frequent cycles of hiring and leadership renewal, to produce systemic change. Ironically, those best positioned to implement change are those in administrative leadership. These individuals are structurally empowered, but for this very same reason, they may also be furthest removed from the lived experiences (and identities) of those most affected by inequities. This positional distance can limit their capacity to fully recognize or respond to specific needs for change.

Considering these structural constraints, one of the most promising avenues for long-term transformation lies in training the next generation of graduate students. As future faculty, researchers, teachers, and institutional leaders, graduate students shape the norms and values that will define the academic enterprise in the years to come. Equipping them with an understanding of structural power, privilege, and positionality fosters critical consciousness around these potent forces, encouraging them to examine how their own social identities influence both their scholarship and their roles within institutional hierarchies. Approaching this with intentionality can disrupt the reproduction of inequity by providing tools and strategies to identify and address systemic and structural disparities. Ultimately, this approach not only diversifies the future leadership pipeline, but also ensures that those who rise to positions of influence are empowered and equipped to challenge exclusionary practices and create more inclusive academic environments.

In general, the implementation of equity initiatives is rarely straightforward. Addressing structural inequities requires engaging with a deeply complex and multi-dimensional problem, one that spans every facet of academic life. Change therefore demands not only precise diagnostics, but also the design of actionable, well-sequenced interventions that are sustainable over time. Even with these supports, the most well-intentioned initiatives can falter. Ultimately,

acknowledging the inherent difficulty and slow pace of institutional transformation does not excuse inaction; rather, it underscores the need for sustained, strategic, and well-supported approaches to equity-driven reform."
Strong_primary_cue_w-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/social_articles/processed_articles/Strong_primary_cue_w-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/social_articles/processed_articles/Strong_primary_cue_w-with-image-refs_artifacts/image_000010_09b1daea87105243f16895c9713d0841883c6b36ae792d925ae583304e0fc603.png,"## L1 English speakers

Figure  5. Scatterplots  displaying  the  relationship  between  weighting  of  the  primary  (left)  and secondary (right) cues to prosodic features, as compared with dimension-selective attention ability. Attention performance was collapsed across verbal and non-verbal subtests. L1 English speakers are displayed on the top of the graph, with L1 Mandarin speakers on the bottom. Points are colored red for primary and blue for secondary cues to match Figures 1 through 4.

One  interpretation  of  the  finding  that  performance  on  the  dimension-selective  attention  test correlates with primary cue weighting is that individuals who can consistently attend to the most useful cue  for  a  given  speech  feature  will  develop  perceptual  strategies  which  strongly  utilize  this  cue. However, an alternate interpretation is that the dimension-selective attention test draws on low-level auditory processing, which also facilitates development of optimal perceptual strategies. To attempt to distinguish between these two explanations, we used linear regression in R to determine the extent to which dimension-selective attention and auditory discrimination skill explain independent variance in primary cue weighting. We also included age and gender to ensure that these results hold even when accounting for demographic variables. Discrimination was averaged across all subtests (pitch and duration  discrimination).  The  regression  equation  for  both  groups  was  [primary\_weighting  ~ discrimination + dimension-selective attention + age + gender]. For the L1 English speakers, the model predicted 33.4% of the variance (F(4,49) = 6.16, p &lt; 0.001). Attention was the only significant predictor

(  = 0.64, F = 23.0, p &lt; 0.001): individuals with better dimension-selective attention showed greater weighting  of  the  primary  cue.  For  the  L1  Mandarin  speakers,  the  model  predicted  21.4%  of  the variance (F(4,52) = 3.53, p = 0.013). Attention was the only significant predictor (  = 0.39, F = 10.77, p =  0.002):  individuals  with  better  dimension-selective  attention  showed  greater  weighting  of  the primary cue.

To  illustrate  in  more  detail  how  categorization  differs  between  individuals  with  good  versus  poor dimension-selective attention, Figures 6 and 7 show how responses vary with pitch and duration level in  these two groups across the three speech perception tasks. Good and poor dimension-selective attention groups were defined as the top versus bottom terciles of performance. In both L1 English and Mandarin speakers, for word emphasis and stress categorization, the function relating responses to pitch level (the primary dimension for these features) is steeper in individuals with good dimensionselective attention. However, for phrase categorization, the function relating responses to duration level (the primary dimension for this feature) is steeper in individuals with good dimension-selective attention.

Figure  6 .  Categorization  responses  across  three  speech  features  in  L1  English  speakers.  Error  bars indicate one standard error of the mean. The good dimension-selective attention group consists of the third  of  the  participants  who  performed  best  across  all  attention  subtests,  while  the  poor  group consists of the bottom third.

L1 Mandarin speakers

Figure 7. Categorization responses across three speech features in L1 Mandarin speakers. Error bars indicate one standard error of the mean. The good dimension-selective attention group consists of the third  of  the  participants  who  performed  best  across  all  attention  subtests,  while  the  poor  group consists of the bottom third."
The_Humble_Self-Conc-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/social_articles/processed_articles/The_Humble_Self-Conc-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/social_articles/processed_articles/The_Humble_Self-Conc-with-image-refs_artifacts/image_000001_83c58463113c9306c3366a7bb548b48ce15dcf84d05a3c9e9ddac11d99251af0.png,"## HSCM: Resilient Self-Belief Framework

who argue it conflates moral obligation with practical feasibility. For instance, Sinnott-Armstrong posits that obligations can exist in moral dilemmas even if impossible to fulfill, while empirical studies suggest laypeople sometimes endorse ""ought"" without ""can."" To avoid confusion between philosophical debates and psychological applications, HSCM employs OIC probabilistically and pragmatically in psychological contexts, where it empirically aligns with harm mitigation by emphasizing that individuals only have access to their conscious experience, not the unconscious processes shaping it, and cannot actively enhance their agency without a felt sense of safety in their internal self-relationship. This safety requires an always-accessible intrinsic self-worth, deserved baseline esteem, and justification for self-care and compassion. Thus, understanding OIC in this framing means that, in any given moment, especially within a seemingly deterministic reality, past states and human limitations are to be accepted without self-harm, treating self-correcting painful feelings (e.g., guilt or shame) as informative data for growth rather than allowing them to perpetuate conditioned cycles of internal harm, such as negative self-talk. This adoption is soundly premised on its utility for fostering self-forgiveness and resilience, remaining provisional and open to revision based on counter-evidence from psychological research.

- Principle 3: Universal Attempt as Virtuous Engine - Locates worth in the continuous, imperfect effort to persist and seek good, creating resilience, truth-seeking, and harm mitigation.

From these principles, the hypothesis derives that every human possesses unconditional worth as a meta-ethical foundation, independent of performance. This moral claim, that worth is intrinsic and tied to the universal attempt to persist, does not automatically confer psychological benefits; rather, it hypothesizes that endorsing this view fosters resilience through reduced shame and enhanced adaptability. This psychological link requires empirical testing, such as via mediators like shame reduction in RCTs. The conclusion: 'Every human is unconditionally worthy by virtue of existing in a state of continuous, imperfect effort, deserving steady self-esteem and full self-compassion' (Gopoian, 2023). This refutes conditional systems as incoherent and promotes a virtuous cycle: resilience enables truth-seeking, which mitigates harm. Individuals have access

only to their conscious experience, not the full unconscious processes shaping it, and cannot meaningfully enhance their agency without a secure internal self-relationship. This security demands an ever-present sense of intrinsic worth, baseline esteem, and justified self-compassion. Thus, self-forgiveness for human limitations and ignorance requires grasping that ""Ought implies can"" implies acceptance of past states, particularly in a seemingly deterministic reality, without self-inflicted harm, viewing corrective emotional signals as informative data rather than perpetuating conditioned cycles of internal harm, such as negative self-talk.

Figure 2: Guiding Philosophical Hypothesis of Unconditional Worth illustrates the axiomatic structure, with the prime axiom as the root, branching into principles, rules, and concluding hypothesis.

Guiding Philosophical Hypothesis of Unconditional Worth

Prime

Axiom

Humility emerges as the rational stance: Embracing fallibility without self-denigration. This axiomatic approach conceptually immunizes HSCM against relativism or nihilism, offering a meta-ethical rationale that integrates with the 10 steps (e.g., Step 2 operationalizes the proof as a daily mantra)."
The_Myth_of_Publicat-with-image-refs,/home/mlazar/projects/thesis/tbp_articles/social_articles/processed_articles/The_Myth_of_Publicat-with-image-refs.md,/home/mlazar/projects/thesis/tbp_articles/social_articles/processed_articles/The_Myth_of_Publicat-with-image-refs_artifacts/image_000000_fb8e71eb5cbe2876ae94fecf1cd61289bdc680dea4b2e3eb0a0e6d25a1cca7b0.png,"## The File-Drawer Problem

Even a suggestively low publication bias does not prove the case for ESP, though it does mean the field of parapsychology is more open to reporting all outcomes. The file-drawer problem (explained in the Introduction) must therefore still be considered a key concern to the field . Critics may argue that sufficient numbers of non-significant studies may still remain unpublished, and if

they had been published and included in the meta-analyses, the significant overall results would shrink to non-significance. We argue that past research on this problem provides little evidence for the critic's argument:

96

Figure 1. Percentage of positive statistical outcomes, observed in non-preregistered experiments. In parenthesis the number of studies examined.

Figure 2. Percentage of positive statistical hypotheses observed in preregistered studies."
